{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Learning approach to the Titanic problem"
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "%env PYTHONHASHSEED=0\n",
                "\n",
                "import os\n",
                "DEVMODE = os.getenv(\"KAGGLE_MODE\") == \"DEV\"\n",
                "print(f\"DEV MODE: {DEVMODE}\")\n",
                "\n",
                "# Define seed to reprodicibility of random generation\n",
                "SEED = 42\n",
                "DEV_SPLIT=0.2"
            ],
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "env: PYTHONHASHSEED=0\nDEV MODE: True\n"
                }
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "COMET INFO: old comet version (3.1.9) detected. current: 3.1.12 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\nCOMET INFO: Experiment is live on comet.ml https://www.comet.ml/witalia008/titanic/d56364a7269d45bd86fdd71b091aaa61\n\n"
                }
            ],
            "source": [
                "import importlib\n",
                "if importlib.util.find_spec(\"comet_ml\"):\n",
                "    from comet_ml import Experiment\n",
                "    experiment = Experiment(project_name=\"titanic\")\n",
                "else:\n",
                "    experiment = None"
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# To display all the columns from left to right without breaking into next line.\n",
                "pd.set_option(\"display.width\", 1500)\n",
                "pd.plotting.register_matplotlib_converters()\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "import seaborn as sns\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "print(tf.__version__)"
            ],
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "2.2.0\n"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "import random as python_random\n",
                "\n",
                "# Make sure Keras produces reproducible results.\n",
                "\n",
                "np.random.seed(SEED)\n",
                "python_random.seed(SEED)\n",
                "tf.random.set_seed(SEED)"
            ],
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
                "print(physical_devices)\n",
                "for device in (physical_devices or []):\n",
                "    tf.config.experimental.set_memory_growth(device, True)"
            ],
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "[]\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "# Read data and extract usable features"
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\", index_col=\"PassengerId\")\n",
                "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\", index_col=\"PassengerId\")"
            ],
            "execution_count": 6,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "features = \"Pclass Sex SibSp Parch Fare Embarked Name Cabin Age\".split()\n",
                "\n",
                "X_train_init = train_data[features]\n",
                "Y_train_init = train_data.Survived\n",
                "\n",
                "print(X_train_init.shape)\n",
                "print(Y_train_init.shape)\n",
                "\n",
                "X_test_init = test_data[features]\n",
                "\n",
                "print(X_test_init.shape)"
            ],
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "(891, 9)\n(891,)\n(418, 9)\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "# Split into train/dev sets\n",
                "## Needs to be done before pre-processing to avoid test-train contamination"
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X_train_unproc, X_dev_unproc, Y_train_unproc, Y_dev_unproc = train_test_split(X_train_init, Y_train_init, test_size=DEV_SPLIT, random_state=SEED)\n",
                "\n",
                "print(X_train_unproc.shape, Y_train_unproc.shape)\n",
                "print(X_dev_unproc.shape, Y_dev_unproc.shape)"
            ],
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "(712, 9) (712,)\n(179, 9) (179,)\n"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "def prepare_data(X):\n",
                "    import re\n",
                "\n",
                "    important_titles = [\"Mr\", \"Mrs\", \"Miss\", \"Master\"]\n",
                "    \n",
                "    X = X.copy()\n",
                "    \n",
                "    # Remember missing values\n",
                "    for col in \"Age\".split():\n",
                "        X[f\"{col}_Missing\"] = np.where(X[col].isnull(), 1, 0)\n",
                "        \n",
                "    if \"Parch\" in X and \"SibSp\" in X:\n",
                "        X[\"Family\"] = X.Parch + X.SibSp\n",
                "\n",
                "    if \"Fare\" in X:\n",
                "        X.Fare = X.Fare.fillna(X.Fare.mean())\n",
                "        # TODO: See if can transform using 'log' or something like that\n",
                "        X.Fare = pd.cut(X.Fare, bins=[-1, 15, 30, 50, 70, 100, 600], labels=False) + 1\n",
                "\n",
                "    if \"Embarked\" in X:\n",
                "        X.Embarked = X.Embarked.fillna(X.Embarked.mode()[0])\n",
                "        X.Embarked = X.Embarked.astype(pd.api.types.CategoricalDtype(categories=\"C Q S\".split()))\n",
                "\n",
                "    if \"Name\" in X:\n",
                "        X[\"Title\"] = X.Name.apply(lambda name: re.search(\", ([\\w ]+).\", name).group(1))\n",
                "\n",
                "        X.Title = X.Title.apply(lambda title: title if title in important_titles else \"Others\")\n",
                "\n",
                "    if \"Age\" in X:\n",
                "        # TODO: Maybe mean should always be taken from the train set?\n",
                "        X[\"Age\"] = X.groupby(\"Pclass Sex\".split())[\"Age\"].transform(lambda x: x.fillna(x.mean()))\n",
                "        X.Age = pd.cut(X.Age, bins=[0, 16, 30, 50, 80], labels=False) + 1\n",
                "        \n",
                "    X = X.drop(columns=\"Name Cabin Parch SibSp\".split())\n",
                "        \n",
                "    X = pd.get_dummies(X, columns=\"Sex Embarked Title\".split())\n",
                "\n",
                "    # TODO: Standardize features for the neural network - maybe use StandardScaler\n",
                "    \n",
                "    # TODO: Select better features to use for the model.\n",
                "    \n",
                "    return X"
            ],
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "print(\"Train data:\")\n",
                "X_train = prepare_data(X_train_unproc)\n",
                "Y_train = Y_train_unproc\n",
                "print(X_train.head())\n",
                "\n",
                "print(\"Dev data:\")\n",
                "X_dev = prepare_data(X_dev_unproc)\n",
                "Y_dev = Y_dev_unproc\n",
                "print(X_dev.head())\n",
                "\n",
                "print(\"Test data:\")\n",
                "X_test = prepare_data(X_test_init)\n",
                "print(X_test.head())"
            ],
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Train data:\n             Pclass  Fare  Age  Age_Missing  Family  Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  Title_Mrs  Title_Others\nPassengerId                                                                                                                                                               \n332               1     2    3            0       0           0         1           0           0           1             0           0         1          0             0\n734               2     1    2            0       0           0         1           0           0           1             0           0         1          0             0\n383               3     1    3            0       0           0         1           0           0           1             0           0         1          0             0\n705               3     1    2            0       1           0         1           0           0           1             0           0         1          0             0\n814               3     3    1            0       6           1         0           0           0           1             0           1         0          0             0\nDev data:\n             Pclass  Fare  Age  Age_Missing  Family  Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  Title_Mrs  Title_Others\nPassengerId                                                                                                                                                               \n710               3     2    2            1       2           0         1           1           0           0             1           0         0          0             0\n440               2     1    3            0       0           0         1           0           0           1             0           0         1          0             0\n841               3     1    2            0       0           0         1           0           0           1             0           0         1          0             0\n721               2     3    1            0       1           1         0           0           0           1             0           1         0          0             0\n40                3     1    1            0       1           1         0           1           0           0             0           1         0          0             0\nTest data:\n             Pclass  Fare  Age  Age_Missing  Family  Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  Title_Mrs  Title_Others\nPassengerId                                                                                                                                                               \n892               3     1    3            0       0           0         1           0           1           0             0           0         1          0             0\n893               3     1    3            0       1           1         0           0           0           1             0           0         0          1             0\n894               2     1    4            0       0           0         1           0           1           0             0           0         1          0             0\n895               3     1    2            0       0           0         1           0           0           1             0           0         1          0             0\n896               3     1    2            0       2           1         0           0           0           1             0           0         0          1             0\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "# DL model using Keras"
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "METRICS = [\n",
                "      keras.metrics.TruePositives(name=\"tp\"),\n",
                "      keras.metrics.FalsePositives(name=\"fp\"),\n",
                "      keras.metrics.TrueNegatives(name=\"tn\"),\n",
                "      keras.metrics.FalseNegatives(name=\"fn\"),\n",
                "      keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
                "      keras.metrics.Precision(name=\"precision\"),\n",
                "      keras.metrics.Recall(name=\"recall\"),\n",
                "      keras.metrics.AUC(name=\"auc\"),\n",
                "]\n",
                "\n",
                "def get_model(input_size):\n",
                "    from tensorflow.keras.models import Sequential\n",
                "    from tensorflow.keras.layers import Dense, Dropout\n",
                "    from tensorflow.keras.regularizers import l2\n",
                "\n",
                "    model = Sequential([\n",
                "        Dense(128, input_shape=(input_size,), activation=\"relu\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), kernel_initializer=\"he_uniform\"),\n",
                "        Dropout(0.2),\n",
                "        Dense(32, activation=\"relu\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), kernel_initializer=\"he_uniform\"),\n",
                "        Dropout(0.2),\n",
                "        Dense(20, activation=\"relu\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), kernel_initializer=\"he_uniform\"),\n",
                "        Dropout(0.25),\n",
                "        Dense(6, activation=\"tanh\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), kernel_initializer=\"glorot_uniform\"),\n",
                "        Dropout(0.3),\n",
                "        Dense(1, activation=\"sigmoid\")\n",
                "    ])\n",
                "    \n",
                "    model.compile(optimizer=keras.optimizers.Adam(1e-3), metrics=METRICS, loss=\"binary_crossentropy\")\n",
                "    \n",
                "    return model"
            ],
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "died_cnt, survived_cnt = np.bincount(Y_train_init)\n",
                "total_cnt = died_cnt + survived_cnt\n",
                "\n",
                "weight_died = total_cnt / died_cnt / 2\n",
                "weight_survived = total_cnt / survived_cnt / 2\n",
                "\n",
                "class_weights = {0: weight_died, 1: weight_survived}"
            ],
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "# Create a new model each time before running training (otherwise new trainings would just be on already trained model)\n",
                "model = get_model(X_train.shape[1])\n",
                "\n",
                "# TODO: See how can make less verbose.\n",
                "history = model.fit(X_train, Y_train, epochs=500, batch_size=32, validation_data=(X_dev, Y_dev), class_weight=class_weights)"
            ],
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "COMET INFO: Ignoring automatic log_parameter('verbose') because 'keras:verbose' is in COMET_LOGGING_PARAMETERS_IGNORE\nEpoch 1/500\n23/23 [==============================] - 3s 113ms/step - loss: 4.2285 - tp: 47.0000 - fp: 87.0000 - tn: 357.0000 - fn: 221.0000 - accuracy: 0.5674 - precision: 0.3507 - recall: 0.1754 - auc: 0.4914 - val_loss: 3.8278 - val_tp: 34.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 40.0000 - val_accuracy: 0.7095 - val_precision: 0.7391 - val_recall: 0.4595 - val_auc: 0.7008\nEpoch 2/500\n23/23 [==============================] - 0s 16ms/step - loss: 3.7530 - tp: 139.0000 - fp: 183.0000 - tn: 261.0000 - fn: 129.0000 - accuracy: 0.5618 - precision: 0.4317 - recall: 0.5187 - auc: 0.5664 - val_loss: 3.5026 - val_tp: 63.0000 - val_fp: 45.0000 - val_tn: 60.0000 - val_fn: 11.0000 - val_accuracy: 0.6872 - val_precision: 0.5833 - val_recall: 0.8514 - val_auc: 0.7844\nEpoch 3/500\n23/23 [==============================] - 0s 19ms/step - loss: 3.4336 - tp: 140.0000 - fp: 164.0000 - tn: 280.0000 - fn: 128.0000 - accuracy: 0.5899 - precision: 0.4605 - recall: 0.5224 - auc: 0.6108 - val_loss: 3.1760 - val_tp: 46.0000 - val_fp: 30.0000 - val_tn: 75.0000 - val_fn: 28.0000 - val_accuracy: 0.6760 - val_precision: 0.6053 - val_recall: 0.6216 - val_auc: 0.7959\nEpoch 4/500\n23/23 [==============================] - 0s 19ms/step - loss: 3.1462 - tp: 150.0000 - fp: 142.0000 - tn: 302.0000 - fn: 118.0000 - accuracy: 0.6348 - precision: 0.5137 - recall: 0.5597 - auc: 0.6593 - val_loss: 2.9226 - val_tp: 60.0000 - val_fp: 37.0000 - val_tn: 68.0000 - val_fn: 14.0000 - val_accuracy: 0.7151 - val_precision: 0.6186 - val_recall: 0.8108 - val_auc: 0.8109\nEpoch 5/500\n23/23 [==============================] - 1s 22ms/step - loss: 2.8768 - tp: 159.0000 - fp: 144.0000 - tn: 300.0000 - fn: 109.0000 - accuracy: 0.6447 - precision: 0.5248 - recall: 0.5933 - auc: 0.7045 - val_loss: 2.6693 - val_tp: 51.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 23.0000 - val_accuracy: 0.7207 - val_precision: 0.6538 - val_recall: 0.6892 - val_auc: 0.8272\nEpoch 6/500\n23/23 [==============================] - 0s 21ms/step - loss: 2.6604 - tp: 174.0000 - fp: 141.0000 - tn: 303.0000 - fn: 94.0000 - accuracy: 0.6699 - precision: 0.5524 - recall: 0.6493 - auc: 0.7165 - val_loss: 2.4555 - val_tp: 60.0000 - val_fp: 33.0000 - val_tn: 72.0000 - val_fn: 14.0000 - val_accuracy: 0.7374 - val_precision: 0.6452 - val_recall: 0.8108 - val_auc: 0.8354\nEpoch 7/500\n23/23 [==============================] - 0s 18ms/step - loss: 2.4511 - tp: 175.0000 - fp: 123.0000 - tn: 321.0000 - fn: 93.0000 - accuracy: 0.6966 - precision: 0.5872 - recall: 0.6530 - auc: 0.7468 - val_loss: 2.2580 - val_tp: 59.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 15.0000 - val_accuracy: 0.7542 - val_precision: 0.6705 - val_recall: 0.7973 - val_auc: 0.8477\nEpoch 8/500\n23/23 [==============================] - 0s 20ms/step - loss: 2.2623 - tp: 197.0000 - fp: 145.0000 - tn: 299.0000 - fn: 71.0000 - accuracy: 0.6966 - precision: 0.5760 - recall: 0.7351 - auc: 0.7651 - val_loss: 2.0936 - val_tp: 60.0000 - val_fp: 32.0000 - val_tn: 73.0000 - val_fn: 14.0000 - val_accuracy: 0.7430 - val_precision: 0.6522 - val_recall: 0.8108 - val_auc: 0.8539\nEpoch 9/500\n23/23 [==============================] - 0s 17ms/step - loss: 2.1105 - tp: 187.0000 - fp: 116.0000 - tn: 328.0000 - fn: 81.0000 - accuracy: 0.7233 - precision: 0.6172 - recall: 0.6978 - auc: 0.7655 - val_loss: 1.9363 - val_tp: 58.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 16.0000 - val_accuracy: 0.7709 - val_precision: 0.6988 - val_recall: 0.7838 - val_auc: 0.8672\nEpoch 10/500\n23/23 [==============================] - 0s 20ms/step - loss: 1.9571 - tp: 205.0000 - fp: 134.0000 - tn: 310.0000 - fn: 63.0000 - accuracy: 0.7233 - precision: 0.6047 - recall: 0.7649 - auc: 0.7838 - val_loss: 1.7980 - val_tp: 57.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 17.0000 - val_accuracy: 0.7765 - val_precision: 0.7125 - val_recall: 0.7703 - val_auc: 0.8701\nEpoch 11/500\n23/23 [==============================] - 0s 15ms/step - loss: 1.8217 - tp: 185.0000 - fp: 92.0000 - tn: 352.0000 - fn: 83.0000 - accuracy: 0.7542 - precision: 0.6679 - recall: 0.6903 - auc: 0.8029 - val_loss: 1.6798 - val_tp: 56.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 18.0000 - val_accuracy: 0.7709 - val_precision: 0.7089 - val_recall: 0.7568 - val_auc: 0.8708\nEpoch 12/500\n23/23 [==============================] - 0s 19ms/step - loss: 1.7286 - tp: 191.0000 - fp: 115.0000 - tn: 329.0000 - fn: 77.0000 - accuracy: 0.7303 - precision: 0.6242 - recall: 0.7127 - auc: 0.7732 - val_loss: 1.5878 - val_tp: 59.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 15.0000 - val_accuracy: 0.7654 - val_precision: 0.6860 - val_recall: 0.7973 - val_auc: 0.8669\nEpoch 13/500\n23/23 [==============================] - 0s 19ms/step - loss: 1.6118 - tp: 195.0000 - fp: 114.0000 - tn: 330.0000 - fn: 73.0000 - accuracy: 0.7374 - precision: 0.6311 - recall: 0.7276 - auc: 0.7945 - val_loss: 1.4794 - val_tp: 57.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 17.0000 - val_accuracy: 0.7765 - val_precision: 0.7125 - val_recall: 0.7703 - val_auc: 0.8691\nEpoch 14/500\n23/23 [==============================] - 0s 21ms/step - loss: 1.4922 - tp: 196.0000 - fp: 90.0000 - tn: 354.0000 - fn: 72.0000 - accuracy: 0.7725 - precision: 0.6853 - recall: 0.7313 - auc: 0.8200 - val_loss: 1.3993 - val_tp: 60.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 14.0000 - val_accuracy: 0.7821 - val_precision: 0.7059 - val_recall: 0.8108 - val_auc: 0.8657\nEpoch 15/500\n23/23 [==============================] - 1s 22ms/step - loss: 1.4225 - tp: 196.0000 - fp: 103.0000 - tn: 341.0000 - fn: 72.0000 - accuracy: 0.7542 - precision: 0.6555 - recall: 0.7313 - auc: 0.8070 - val_loss: 1.3128 - val_tp: 60.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 14.0000 - val_accuracy: 0.7877 - val_precision: 0.7143 - val_recall: 0.8108 - val_auc: 0.8717\nEpoch 16/500\n23/23 [==============================] - 0s 18ms/step - loss: 1.3277 - tp: 207.0000 - fp: 106.0000 - tn: 338.0000 - fn: 61.0000 - accuracy: 0.7654 - precision: 0.6613 - recall: 0.7724 - auc: 0.8261 - val_loss: 1.2361 - val_tp: 57.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 17.0000 - val_accuracy: 0.7765 - val_precision: 0.7125 - val_recall: 0.7703 - val_auc: 0.8708\nEpoch 17/500\n23/23 [==============================] - 0s 20ms/step - loss: 1.2767 - tp: 185.0000 - fp: 75.0000 - tn: 369.0000 - fn: 83.0000 - accuracy: 0.7781 - precision: 0.7115 - recall: 0.6903 - auc: 0.8097 - val_loss: 1.1964 - val_tp: 60.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 14.0000 - val_accuracy: 0.7598 - val_precision: 0.6742 - val_recall: 0.8108 - val_auc: 0.8656\nEpoch 18/500\n23/23 [==============================] - 0s 18ms/step - loss: 1.2273 - tp: 199.0000 - fp: 105.0000 - tn: 339.0000 - fn: 69.0000 - accuracy: 0.7556 - precision: 0.6546 - recall: 0.7425 - auc: 0.7990 - val_loss: 1.1100 - val_tp: 55.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 19.0000 - val_accuracy: 0.7877 - val_precision: 0.7432 - val_recall: 0.7432 - val_auc: 0.8782\nEpoch 19/500\n23/23 [==============================] - 0s 21ms/step - loss: 1.1441 - tp: 196.0000 - fp: 79.0000 - tn: 365.0000 - fn: 72.0000 - accuracy: 0.7879 - precision: 0.7127 - recall: 0.7313 - auc: 0.8271 - val_loss: 1.0725 - val_tp: 58.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 16.0000 - val_accuracy: 0.7765 - val_precision: 0.7073 - val_recall: 0.7838 - val_auc: 0.8736\nEpoch 20/500\n23/23 [==============================] - 0s 20ms/step - loss: 1.0786 - tp: 201.0000 - fp: 88.0000 - tn: 356.0000 - fn: 67.0000 - accuracy: 0.7823 - precision: 0.6955 - recall: 0.7500 - auc: 0.8357 - val_loss: 1.0232 - val_tp: 58.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 16.0000 - val_accuracy: 0.7821 - val_precision: 0.7160 - val_recall: 0.7838 - val_auc: 0.8755\nEpoch 21/500\n23/23 [==============================] - 0s 19ms/step - loss: 1.0510 - tp: 195.0000 - fp: 83.0000 - tn: 361.0000 - fn: 73.0000 - accuracy: 0.7809 - precision: 0.7014 - recall: 0.7276 - auc: 0.8213 - val_loss: 0.9936 - val_tp: 61.0000 - val_fp: 30.0000 - val_tn: 75.0000 - val_fn: 13.0000 - val_accuracy: 0.7598 - val_precision: 0.6703 - val_recall: 0.8243 - val_auc: 0.8723\nEpoch 22/500\n23/23 [==============================] - 0s 18ms/step - loss: 1.0144 - tp: 210.0000 - fp: 106.0000 - tn: 338.0000 - fn: 58.0000 - accuracy: 0.7697 - precision: 0.6646 - recall: 0.7836 - auc: 0.8194 - val_loss: 0.9300 - val_tp: 54.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 20.0000 - val_accuracy: 0.7654 - val_precision: 0.7105 - val_recall: 0.7297 - val_auc: 0.8780\nEpoch 23/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.9494 - tp: 204.0000 - fp: 88.0000 - tn: 356.0000 - fn: 64.0000 - accuracy: 0.7865 - precision: 0.6986 - recall: 0.7612 - auc: 0.8407 - val_loss: 0.8964 - val_tp: 55.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 19.0000 - val_accuracy: 0.7709 - val_precision: 0.7143 - val_recall: 0.7432 - val_auc: 0.8802\nEpoch 24/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.9410 - tp: 195.0000 - fp: 87.0000 - tn: 357.0000 - fn: 73.0000 - accuracy: 0.7753 - precision: 0.6915 - recall: 0.7276 - auc: 0.8204 - val_loss: 0.8793 - val_tp: 61.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 13.0000 - val_accuracy: 0.7989 - val_precision: 0.7262 - val_recall: 0.8243 - val_auc: 0.8759\nEpoch 25/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.8915 - tp: 208.0000 - fp: 83.0000 - tn: 361.0000 - fn: 60.0000 - accuracy: 0.7992 - precision: 0.7148 - recall: 0.7761 - auc: 0.8372 - val_loss: 0.8287 - val_tp: 55.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 19.0000 - val_accuracy: 0.7933 - val_precision: 0.7534 - val_recall: 0.7432 - val_auc: 0.8838\nEpoch 26/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.8623 - tp: 203.0000 - fp: 80.0000 - tn: 364.0000 - fn: 65.0000 - accuracy: 0.7963 - precision: 0.7173 - recall: 0.7575 - auc: 0.8354 - val_loss: 0.8064 - val_tp: 56.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 18.0000 - val_accuracy: 0.7877 - val_precision: 0.7368 - val_recall: 0.7568 - val_auc: 0.8843\nEpoch 27/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.8473 - tp: 214.0000 - fp: 97.0000 - tn: 347.0000 - fn: 54.0000 - accuracy: 0.7879 - precision: 0.6881 - recall: 0.7985 - auc: 0.8308 - val_loss: 0.7829 - val_tp: 55.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 19.0000 - val_accuracy: 0.7821 - val_precision: 0.7333 - val_recall: 0.7432 - val_auc: 0.8876\nEpoch 28/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.8050 - tp: 205.0000 - fp: 73.0000 - tn: 371.0000 - fn: 63.0000 - accuracy: 0.8090 - precision: 0.7374 - recall: 0.7649 - auc: 0.8401 - val_loss: 0.7541 - val_tp: 56.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 18.0000 - val_accuracy: 0.7877 - val_precision: 0.7368 - val_recall: 0.7568 - val_auc: 0.8846\nEpoch 29/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.7986 - tp: 200.0000 - fp: 72.0000 - tn: 372.0000 - fn: 68.0000 - accuracy: 0.8034 - precision: 0.7353 - recall: 0.7463 - auc: 0.8311 - val_loss: 0.7493 - val_tp: 56.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 18.0000 - val_accuracy: 0.7821 - val_precision: 0.7273 - val_recall: 0.7568 - val_auc: 0.8840\nEpoch 30/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.7786 - tp: 212.0000 - fp: 99.0000 - tn: 345.0000 - fn: 56.0000 - accuracy: 0.7823 - precision: 0.6817 - recall: 0.7910 - auc: 0.8371 - val_loss: 0.7494 - val_tp: 61.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 13.0000 - val_accuracy: 0.7989 - val_precision: 0.7262 - val_recall: 0.8243 - val_auc: 0.8802\nEpoch 31/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.7612 - tp: 206.0000 - fp: 84.0000 - tn: 360.0000 - fn: 62.0000 - accuracy: 0.7949 - precision: 0.7103 - recall: 0.7687 - auc: 0.8371 - val_loss: 0.7100 - val_tp: 55.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 19.0000 - val_accuracy: 0.7821 - val_precision: 0.7333 - val_recall: 0.7432 - val_auc: 0.8852\nEpoch 32/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.7386 - tp: 205.0000 - fp: 82.0000 - tn: 362.0000 - fn: 63.0000 - accuracy: 0.7963 - precision: 0.7143 - recall: 0.7649 - auc: 0.8430 - val_loss: 0.7085 - val_tp: 61.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 13.0000 - val_accuracy: 0.7989 - val_precision: 0.7262 - val_recall: 0.8243 - val_auc: 0.8799\nEpoch 33/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.7413 - tp: 204.0000 - fp: 81.0000 - tn: 363.0000 - fn: 64.0000 - accuracy: 0.7963 - precision: 0.7158 - recall: 0.7612 - auc: 0.8282 - val_loss: 0.6797 - val_tp: 56.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 18.0000 - val_accuracy: 0.7933 - val_precision: 0.7467 - val_recall: 0.7568 - val_auc: 0.8850\nEpoch 34/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.7072 - tp: 209.0000 - fp: 83.0000 - tn: 361.0000 - fn: 59.0000 - accuracy: 0.8006 - precision: 0.7158 - recall: 0.7799 - auc: 0.8442 - val_loss: 0.6973 - val_tp: 65.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 9.0000 - val_accuracy: 0.7877 - val_precision: 0.6915 - val_recall: 0.8784 - val_auc: 0.8754\nEpoch 35/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.6994 - tp: 207.0000 - fp: 78.0000 - tn: 366.0000 - fn: 61.0000 - accuracy: 0.8048 - precision: 0.7263 - recall: 0.7724 - auc: 0.8477 - val_loss: 0.6541 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8844\nEpoch 36/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.6816 - tp: 206.0000 - fp: 79.0000 - tn: 365.0000 - fn: 62.0000 - accuracy: 0.8020 - precision: 0.7228 - recall: 0.7687 - auc: 0.8494 - val_loss: 0.6495 - val_tp: 59.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 15.0000 - val_accuracy: 0.7989 - val_precision: 0.7375 - val_recall: 0.7973 - val_auc: 0.8838\nEpoch 37/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.6806 - tp: 208.0000 - fp: 87.0000 - tn: 357.0000 - fn: 60.0000 - accuracy: 0.7935 - precision: 0.7051 - recall: 0.7761 - auc: 0.8442 - val_loss: 0.6303 - val_tp: 56.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 18.0000 - val_accuracy: 0.7933 - val_precision: 0.7467 - val_recall: 0.7568 - val_auc: 0.8869\nEpoch 38/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.6644 - tp: 213.0000 - fp: 83.0000 - tn: 361.0000 - fn: 55.0000 - accuracy: 0.8062 - precision: 0.7196 - recall: 0.7948 - auc: 0.8509 - val_loss: 0.6544 - val_tp: 63.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 11.0000 - val_accuracy: 0.7877 - val_precision: 0.7000 - val_recall: 0.8514 - val_auc: 0.8797\nEpoch 39/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.6427 - tp: 210.0000 - fp: 84.0000 - tn: 360.0000 - fn: 58.0000 - accuracy: 0.8006 - precision: 0.7143 - recall: 0.7836 - auc: 0.8605 - val_loss: 0.6130 - val_tp: 52.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 22.0000 - val_accuracy: 0.7821 - val_precision: 0.7536 - val_recall: 0.7027 - val_auc: 0.8802\nEpoch 40/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.6584 - tp: 198.0000 - fp: 66.0000 - tn: 378.0000 - fn: 70.0000 - accuracy: 0.8090 - precision: 0.7500 - recall: 0.7388 - auc: 0.8408 - val_loss: 0.6433 - val_tp: 64.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 10.0000 - val_accuracy: 0.7877 - val_precision: 0.6957 - val_recall: 0.8649 - val_auc: 0.8801\nEpoch 41/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.6545 - tp: 213.0000 - fp: 80.0000 - tn: 364.0000 - fn: 55.0000 - accuracy: 0.8104 - precision: 0.7270 - recall: 0.7948 - auc: 0.8413 - val_loss: 0.6092 - val_tp: 56.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 18.0000 - val_accuracy: 0.7933 - val_precision: 0.7467 - val_recall: 0.7568 - val_auc: 0.8856\nEpoch 42/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.6369 - tp: 203.0000 - fp: 65.0000 - tn: 379.0000 - fn: 65.0000 - accuracy: 0.8174 - precision: 0.7575 - recall: 0.7575 - auc: 0.8469 - val_loss: 0.6082 - val_tp: 63.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 11.0000 - val_accuracy: 0.8101 - val_precision: 0.7326 - val_recall: 0.8514 - val_auc: 0.8816\nEpoch 43/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.6296 - tp: 204.0000 - fp: 74.0000 - tn: 370.0000 - fn: 64.0000 - accuracy: 0.8062 - precision: 0.7338 - recall: 0.7612 - auc: 0.8448 - val_loss: 0.6089 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8830\nEpoch 44/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.6328 - tp: 202.0000 - fp: 73.0000 - tn: 371.0000 - fn: 66.0000 - accuracy: 0.8048 - precision: 0.7345 - recall: 0.7537 - auc: 0.8420 - val_loss: 0.5855 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8878\nEpoch 45/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.6171 - tp: 201.0000 - fp: 72.0000 - tn: 372.0000 - fn: 67.0000 - accuracy: 0.8048 - precision: 0.7363 - recall: 0.7500 - auc: 0.8501 - val_loss: 0.5962 - val_tp: 58.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 16.0000 - val_accuracy: 0.7989 - val_precision: 0.7436 - val_recall: 0.7838 - val_auc: 0.8828\nEpoch 46/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.6083 - tp: 204.0000 - fp: 72.0000 - tn: 372.0000 - fn: 64.0000 - accuracy: 0.8090 - precision: 0.7391 - recall: 0.7612 - auc: 0.8534 - val_loss: 0.5850 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8847\nEpoch 47/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.6184 - tp: 209.0000 - fp: 73.0000 - tn: 371.0000 - fn: 59.0000 - accuracy: 0.8146 - precision: 0.7411 - recall: 0.7799 - auc: 0.8397 - val_loss: 0.5869 - val_tp: 59.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 15.0000 - val_accuracy: 0.7989 - val_precision: 0.7375 - val_recall: 0.7973 - val_auc: 0.8857\nEpoch 48/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5994 - tp: 209.0000 - fp: 69.0000 - tn: 375.0000 - fn: 59.0000 - accuracy: 0.8202 - precision: 0.7518 - recall: 0.7799 - auc: 0.8539 - val_loss: 0.5732 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8874\nEpoch 49/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5786 - tp: 210.0000 - fp: 68.0000 - tn: 376.0000 - fn: 58.0000 - accuracy: 0.8230 - precision: 0.7554 - recall: 0.7836 - auc: 0.8691 - val_loss: 0.5575 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8862\nEpoch 50/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5867 - tp: 209.0000 - fp: 76.0000 - tn: 368.0000 - fn: 59.0000 - accuracy: 0.8104 - precision: 0.7333 - recall: 0.7799 - auc: 0.8588 - val_loss: 0.5557 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8860\nEpoch 51/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5968 - tp: 210.0000 - fp: 68.0000 - tn: 376.0000 - fn: 58.0000 - accuracy: 0.8230 - precision: 0.7554 - recall: 0.7836 - auc: 0.8433 - val_loss: 0.5919 - val_tp: 64.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 10.0000 - val_accuracy: 0.7877 - val_precision: 0.6957 - val_recall: 0.8649 - val_auc: 0.8814\nEpoch 52/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5963 - tp: 204.0000 - fp: 69.0000 - tn: 375.0000 - fn: 64.0000 - accuracy: 0.8132 - precision: 0.7473 - recall: 0.7612 - auc: 0.8434 - val_loss: 0.5583 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8849\nEpoch 53/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5790 - tp: 210.0000 - fp: 74.0000 - tn: 370.0000 - fn: 58.0000 - accuracy: 0.8146 - precision: 0.7394 - recall: 0.7836 - auc: 0.8538 - val_loss: 0.5569 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8808\nEpoch 54/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.5966 - tp: 205.0000 - fp: 73.0000 - tn: 371.0000 - fn: 63.0000 - accuracy: 0.8090 - precision: 0.7374 - recall: 0.7649 - auc: 0.8367 - val_loss: 0.5732 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8830\nEpoch 55/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5977 - tp: 193.0000 - fp: 61.0000 - tn: 383.0000 - fn: 75.0000 - accuracy: 0.8090 - precision: 0.7598 - recall: 0.7201 - auc: 0.8365 - val_loss: 0.5768 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8807\nEpoch 56/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5783 - tp: 209.0000 - fp: 75.0000 - tn: 369.0000 - fn: 59.0000 - accuracy: 0.8118 - precision: 0.7359 - recall: 0.7799 - auc: 0.8484 - val_loss: 0.5430 - val_tp: 55.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 19.0000 - val_accuracy: 0.8045 - val_precision: 0.7746 - val_recall: 0.7432 - val_auc: 0.8864\nEpoch 57/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.5841 - tp: 206.0000 - fp: 84.0000 - tn: 360.0000 - fn: 62.0000 - accuracy: 0.7949 - precision: 0.7103 - recall: 0.7687 - auc: 0.8449 - val_loss: 0.5480 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8856\nEpoch 58/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5603 - tp: 199.0000 - fp: 48.0000 - tn: 396.0000 - fn: 69.0000 - accuracy: 0.8357 - precision: 0.8057 - recall: 0.7425 - auc: 0.8614 - val_loss: 0.5695 - val_tp: 60.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 14.0000 - val_accuracy: 0.7933 - val_precision: 0.7229 - val_recall: 0.8108 - val_auc: 0.8804\nEpoch 59/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5830 - tp: 205.0000 - fp: 68.0000 - tn: 376.0000 - fn: 63.0000 - accuracy: 0.8160 - precision: 0.7509 - recall: 0.7649 - auc: 0.8425 - val_loss: 0.5338 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8887\nEpoch 60/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5734 - tp: 209.0000 - fp: 80.0000 - tn: 364.0000 - fn: 59.0000 - accuracy: 0.8048 - precision: 0.7232 - recall: 0.7799 - auc: 0.8494 - val_loss: 0.5423 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8863\nEpoch 61/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5754 - tp: 200.0000 - fp: 72.0000 - tn: 372.0000 - fn: 68.0000 - accuracy: 0.8034 - precision: 0.7353 - recall: 0.7463 - auc: 0.8474 - val_loss: 0.5399 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8860\nEpoch 62/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5633 - tp: 207.0000 - fp: 71.0000 - tn: 373.0000 - fn: 61.0000 - accuracy: 0.8146 - precision: 0.7446 - recall: 0.7724 - auc: 0.8572 - val_loss: 0.5665 - val_tp: 64.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 10.0000 - val_accuracy: 0.7933 - val_precision: 0.7033 - val_recall: 0.8649 - val_auc: 0.8841\nEpoch 63/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5620 - tp: 213.0000 - fp: 86.0000 - tn: 358.0000 - fn: 55.0000 - accuracy: 0.8020 - precision: 0.7124 - recall: 0.7948 - auc: 0.8565 - val_loss: 0.5267 - val_tp: 54.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 20.0000 - val_accuracy: 0.7933 - val_precision: 0.7606 - val_recall: 0.7297 - val_auc: 0.8875\nEpoch 64/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5602 - tp: 208.0000 - fp: 78.0000 - tn: 366.0000 - fn: 60.0000 - accuracy: 0.8062 - precision: 0.7273 - recall: 0.7761 - auc: 0.8556 - val_loss: 0.5514 - val_tp: 60.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 14.0000 - val_accuracy: 0.8101 - val_precision: 0.7500 - val_recall: 0.8108 - val_auc: 0.8848\nEpoch 65/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.5439 - tp: 205.0000 - fp: 70.0000 - tn: 374.0000 - fn: 63.0000 - accuracy: 0.8132 - precision: 0.7455 - recall: 0.7649 - auc: 0.8663 - val_loss: 0.5413 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8878\nEpoch 66/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.5668 - tp: 213.0000 - fp: 79.0000 - tn: 365.0000 - fn: 55.0000 - accuracy: 0.8118 - precision: 0.7295 - recall: 0.7948 - auc: 0.8494 - val_loss: 0.5266 - val_tp: 55.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 19.0000 - val_accuracy: 0.8045 - val_precision: 0.7746 - val_recall: 0.7432 - val_auc: 0.8915\nEpoch 67/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5469 - tp: 206.0000 - fp: 56.0000 - tn: 388.0000 - fn: 62.0000 - accuracy: 0.8343 - precision: 0.7863 - recall: 0.7687 - auc: 0.8646 - val_loss: 0.5573 - val_tp: 61.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 13.0000 - val_accuracy: 0.7821 - val_precision: 0.7011 - val_recall: 0.8243 - val_auc: 0.8826\nEpoch 68/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5631 - tp: 209.0000 - fp: 76.0000 - tn: 368.0000 - fn: 59.0000 - accuracy: 0.8104 - precision: 0.7333 - recall: 0.7799 - auc: 0.8511 - val_loss: 0.5249 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8875\nEpoch 69/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5352 - tp: 209.0000 - fp: 74.0000 - tn: 370.0000 - fn: 59.0000 - accuracy: 0.8132 - precision: 0.7385 - recall: 0.7799 - auc: 0.8701 - val_loss: 0.5453 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8813\nEpoch 70/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5585 - tp: 209.0000 - fp: 85.0000 - tn: 359.0000 - fn: 59.0000 - accuracy: 0.7978 - precision: 0.7109 - recall: 0.7799 - auc: 0.8505 - val_loss: 0.5436 - val_tp: 59.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 15.0000 - val_accuracy: 0.7877 - val_precision: 0.7195 - val_recall: 0.7973 - val_auc: 0.8833\nEpoch 71/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5578 - tp: 203.0000 - fp: 67.0000 - tn: 377.0000 - fn: 65.0000 - accuracy: 0.8146 - precision: 0.7519 - recall: 0.7575 - auc: 0.8540 - val_loss: 0.5442 - val_tp: 61.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 13.0000 - val_accuracy: 0.7989 - val_precision: 0.7262 - val_recall: 0.8243 - val_auc: 0.8820\nEpoch 72/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.5456 - tp: 205.0000 - fp: 82.0000 - tn: 362.0000 - fn: 63.0000 - accuracy: 0.7963 - precision: 0.7143 - recall: 0.7649 - auc: 0.8609 - val_loss: 0.5244 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8851\nEpoch 73/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5520 - tp: 200.0000 - fp: 66.0000 - tn: 378.0000 - fn: 68.0000 - accuracy: 0.8118 - precision: 0.7519 - recall: 0.7463 - auc: 0.8559 - val_loss: 0.5246 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8878\nEpoch 74/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.5554 - tp: 209.0000 - fp: 69.0000 - tn: 375.0000 - fn: 59.0000 - accuracy: 0.8202 - precision: 0.7518 - recall: 0.7799 - auc: 0.8499 - val_loss: 0.5387 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8861\nEpoch 75/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5530 - tp: 209.0000 - fp: 72.0000 - tn: 372.0000 - fn: 59.0000 - accuracy: 0.8160 - precision: 0.7438 - recall: 0.7799 - auc: 0.8519 - val_loss: 0.5268 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8849\nEpoch 76/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.5480 - tp: 213.0000 - fp: 84.0000 - tn: 360.0000 - fn: 55.0000 - accuracy: 0.8048 - precision: 0.7172 - recall: 0.7948 - auc: 0.8580 - val_loss: 0.5139 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8875\nEpoch 77/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5468 - tp: 212.0000 - fp: 74.0000 - tn: 370.0000 - fn: 56.0000 - accuracy: 0.8174 - precision: 0.7413 - recall: 0.7910 - auc: 0.8540 - val_loss: 0.5188 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8927\nEpoch 78/500\n23/23 [==============================] - 1s 29ms/step - loss: 0.5425 - tp: 208.0000 - fp: 69.0000 - tn: 375.0000 - fn: 60.0000 - accuracy: 0.8188 - precision: 0.7509 - recall: 0.7761 - auc: 0.8586 - val_loss: 0.5354 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8884\nEpoch 79/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5435 - tp: 205.0000 - fp: 63.0000 - tn: 381.0000 - fn: 63.0000 - accuracy: 0.8230 - precision: 0.7649 - recall: 0.7649 - auc: 0.8565 - val_loss: 0.5449 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8865\nEpoch 80/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5520 - tp: 211.0000 - fp: 76.0000 - tn: 368.0000 - fn: 57.0000 - accuracy: 0.8132 - precision: 0.7352 - recall: 0.7873 - auc: 0.8495 - val_loss: 0.5117 - val_tp: 55.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 19.0000 - val_accuracy: 0.8101 - val_precision: 0.7857 - val_recall: 0.7432 - val_auc: 0.8908\nEpoch 81/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5357 - tp: 201.0000 - fp: 62.0000 - tn: 382.0000 - fn: 67.0000 - accuracy: 0.8188 - precision: 0.7643 - recall: 0.7500 - auc: 0.8620 - val_loss: 0.5391 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8873\nEpoch 82/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5341 - tp: 207.0000 - fp: 70.0000 - tn: 374.0000 - fn: 61.0000 - accuracy: 0.8160 - precision: 0.7473 - recall: 0.7724 - auc: 0.8611 - val_loss: 0.5190 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8878\nEpoch 83/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5306 - tp: 202.0000 - fp: 58.0000 - tn: 386.0000 - fn: 66.0000 - accuracy: 0.8258 - precision: 0.7769 - recall: 0.7537 - auc: 0.8678 - val_loss: 0.5315 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8846\nEpoch 84/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5282 - tp: 206.0000 - fp: 70.0000 - tn: 374.0000 - fn: 62.0000 - accuracy: 0.8146 - precision: 0.7464 - recall: 0.7687 - auc: 0.8691 - val_loss: 0.5253 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8842\nEpoch 85/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5250 - tp: 202.0000 - fp: 62.0000 - tn: 382.0000 - fn: 66.0000 - accuracy: 0.8202 - precision: 0.7652 - recall: 0.7537 - auc: 0.8655 - val_loss: 0.5308 - val_tp: 59.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 15.0000 - val_accuracy: 0.7821 - val_precision: 0.7108 - val_recall: 0.7973 - val_auc: 0.8825\nEpoch 86/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5394 - tp: 200.0000 - fp: 58.0000 - tn: 386.0000 - fn: 68.0000 - accuracy: 0.8230 - precision: 0.7752 - recall: 0.7463 - auc: 0.8586 - val_loss: 0.5180 - val_tp: 59.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 15.0000 - val_accuracy: 0.8101 - val_precision: 0.7564 - val_recall: 0.7973 - val_auc: 0.8875\nEpoch 87/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5279 - tp: 208.0000 - fp: 74.0000 - tn: 370.0000 - fn: 60.0000 - accuracy: 0.8118 - precision: 0.7376 - recall: 0.7761 - auc: 0.8673 - val_loss: 0.5126 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8880\nEpoch 88/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5502 - tp: 202.0000 - fp: 71.0000 - tn: 373.0000 - fn: 66.0000 - accuracy: 0.8076 - precision: 0.7399 - recall: 0.7537 - auc: 0.8451 - val_loss: 0.5131 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8894\nEpoch 89/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5456 - tp: 201.0000 - fp: 73.0000 - tn: 371.0000 - fn: 67.0000 - accuracy: 0.8034 - precision: 0.7336 - recall: 0.7500 - auc: 0.8535 - val_loss: 0.5303 - val_tp: 60.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 14.0000 - val_accuracy: 0.8045 - val_precision: 0.7407 - val_recall: 0.8108 - val_auc: 0.8846\nEpoch 90/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5354 - tp: 203.0000 - fp: 65.0000 - tn: 379.0000 - fn: 65.0000 - accuracy: 0.8174 - precision: 0.7575 - recall: 0.7575 - auc: 0.8546 - val_loss: 0.5205 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8869\nEpoch 91/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5368 - tp: 207.0000 - fp: 71.0000 - tn: 373.0000 - fn: 61.0000 - accuracy: 0.8146 - precision: 0.7446 - recall: 0.7724 - auc: 0.8530 - val_loss: 0.5206 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8876\nEpoch 92/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5230 - tp: 209.0000 - fp: 79.0000 - tn: 365.0000 - fn: 59.0000 - accuracy: 0.8062 - precision: 0.7257 - recall: 0.7799 - auc: 0.8677 - val_loss: 0.5090 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8882\nEpoch 93/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5364 - tp: 196.0000 - fp: 54.0000 - tn: 390.0000 - fn: 72.0000 - accuracy: 0.8230 - precision: 0.7840 - recall: 0.7313 - auc: 0.8593 - val_loss: 0.5297 - val_tp: 60.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 14.0000 - val_accuracy: 0.7877 - val_precision: 0.7143 - val_recall: 0.8108 - val_auc: 0.8828\nEpoch 94/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5251 - tp: 205.0000 - fp: 68.0000 - tn: 376.0000 - fn: 63.0000 - accuracy: 0.8160 - precision: 0.7509 - recall: 0.7649 - auc: 0.8635 - val_loss: 0.5265 - val_tp: 59.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 15.0000 - val_accuracy: 0.7989 - val_precision: 0.7375 - val_recall: 0.7973 - val_auc: 0.8851\nEpoch 95/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5237 - tp: 208.0000 - fp: 60.0000 - tn: 384.0000 - fn: 60.0000 - accuracy: 0.8315 - precision: 0.7761 - recall: 0.7761 - auc: 0.8624 - val_loss: 0.5126 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8895\nEpoch 96/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5277 - tp: 207.0000 - fp: 62.0000 - tn: 382.0000 - fn: 61.0000 - accuracy: 0.8272 - precision: 0.7695 - recall: 0.7724 - auc: 0.8618 - val_loss: 0.5171 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8891\nEpoch 97/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5369 - tp: 203.0000 - fp: 67.0000 - tn: 377.0000 - fn: 65.0000 - accuracy: 0.8146 - precision: 0.7519 - recall: 0.7575 - auc: 0.8524 - val_loss: 0.5065 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8919\nEpoch 98/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5328 - tp: 194.0000 - fp: 49.0000 - tn: 395.0000 - fn: 74.0000 - accuracy: 0.8272 - precision: 0.7984 - recall: 0.7239 - auc: 0.8589 - val_loss: 0.5114 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8914\nEpoch 99/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5302 - tp: 210.0000 - fp: 73.0000 - tn: 371.0000 - fn: 58.0000 - accuracy: 0.8160 - precision: 0.7420 - recall: 0.7836 - auc: 0.8581 - val_loss: 0.5204 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8865\nEpoch 100/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5271 - tp: 210.0000 - fp: 71.0000 - tn: 373.0000 - fn: 58.0000 - accuracy: 0.8188 - precision: 0.7473 - recall: 0.7836 - auc: 0.8585 - val_loss: 0.5021 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8902\nEpoch 101/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5267 - tp: 211.0000 - fp: 76.0000 - tn: 368.0000 - fn: 57.0000 - accuracy: 0.8132 - precision: 0.7352 - recall: 0.7873 - auc: 0.8614 - val_loss: 0.5007 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8912\nEpoch 102/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5363 - tp: 199.0000 - fp: 57.0000 - tn: 387.0000 - fn: 69.0000 - accuracy: 0.8230 - precision: 0.7773 - recall: 0.7425 - auc: 0.8509 - val_loss: 0.5403 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8841\nEpoch 103/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.5387 - tp: 202.0000 - fp: 67.0000 - tn: 377.0000 - fn: 66.0000 - accuracy: 0.8132 - precision: 0.7509 - recall: 0.7537 - auc: 0.8521 - val_loss: 0.5060 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8919\nEpoch 104/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5285 - tp: 204.0000 - fp: 63.0000 - tn: 381.0000 - fn: 64.0000 - accuracy: 0.8216 - precision: 0.7640 - recall: 0.7612 - auc: 0.8602 - val_loss: 0.5233 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8882\nEpoch 105/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5236 - tp: 200.0000 - fp: 62.0000 - tn: 382.0000 - fn: 68.0000 - accuracy: 0.8174 - precision: 0.7634 - recall: 0.7463 - auc: 0.8559 - val_loss: 0.5262 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8884\nEpoch 106/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5168 - tp: 208.0000 - fp: 68.0000 - tn: 376.0000 - fn: 60.0000 - accuracy: 0.8202 - precision: 0.7536 - recall: 0.7761 - auc: 0.8693 - val_loss: 0.5291 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8903\nEpoch 107/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5217 - tp: 202.0000 - fp: 64.0000 - tn: 380.0000 - fn: 66.0000 - accuracy: 0.8174 - precision: 0.7594 - recall: 0.7537 - auc: 0.8614 - val_loss: 0.5064 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8936\nEpoch 108/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.5354 - tp: 204.0000 - fp: 64.0000 - tn: 380.0000 - fn: 64.0000 - accuracy: 0.8202 - precision: 0.7612 - recall: 0.7612 - auc: 0.8532 - val_loss: 0.5092 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8890\nEpoch 109/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5214 - tp: 216.0000 - fp: 73.0000 - tn: 371.0000 - fn: 52.0000 - accuracy: 0.8244 - precision: 0.7474 - recall: 0.8060 - auc: 0.8609 - val_loss: 0.5010 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8897\nEpoch 110/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5344 - tp: 193.0000 - fp: 52.0000 - tn: 392.0000 - fn: 75.0000 - accuracy: 0.8216 - precision: 0.7878 - recall: 0.7201 - auc: 0.8462 - val_loss: 0.5450 - val_tp: 62.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 12.0000 - val_accuracy: 0.7877 - val_precision: 0.7045 - val_recall: 0.8378 - val_auc: 0.8856\nEpoch 111/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5190 - tp: 209.0000 - fp: 64.0000 - tn: 380.0000 - fn: 59.0000 - accuracy: 0.8272 - precision: 0.7656 - recall: 0.7799 - auc: 0.8623 - val_loss: 0.5016 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8939\nEpoch 112/500\n23/23 [==============================] - 1s 28ms/step - loss: 0.5381 - tp: 209.0000 - fp: 71.0000 - tn: 373.0000 - fn: 59.0000 - accuracy: 0.8174 - precision: 0.7464 - recall: 0.7799 - auc: 0.8513 - val_loss: 0.4966 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8923\nEpoch 113/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5325 - tp: 198.0000 - fp: 65.0000 - tn: 379.0000 - fn: 70.0000 - accuracy: 0.8104 - precision: 0.7529 - recall: 0.7388 - auc: 0.8521 - val_loss: 0.5046 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8948\nEpoch 114/500\n23/23 [==============================] - 1s 32ms/step - loss: 0.5212 - tp: 198.0000 - fp: 57.0000 - tn: 387.0000 - fn: 70.0000 - accuracy: 0.8216 - precision: 0.7765 - recall: 0.7388 - auc: 0.8612 - val_loss: 0.5203 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8870\nEpoch 115/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5155 - tp: 202.0000 - fp: 48.0000 - tn: 396.0000 - fn: 66.0000 - accuracy: 0.8399 - precision: 0.8080 - recall: 0.7537 - auc: 0.8610 - val_loss: 0.5079 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8915\nEpoch 116/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5171 - tp: 207.0000 - fp: 66.0000 - tn: 378.0000 - fn: 61.0000 - accuracy: 0.8216 - precision: 0.7582 - recall: 0.7724 - auc: 0.8633 - val_loss: 0.4973 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8927\nEpoch 117/500\n23/23 [==============================] - 1s 26ms/step - loss: 0.5135 - tp: 205.0000 - fp: 66.0000 - tn: 378.0000 - fn: 63.0000 - accuracy: 0.8188 - precision: 0.7565 - recall: 0.7649 - auc: 0.8687 - val_loss: 0.4947 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8906\nEpoch 118/500\n23/23 [==============================] - 1s 26ms/step - loss: 0.5212 - tp: 205.0000 - fp: 68.0000 - tn: 376.0000 - fn: 63.0000 - accuracy: 0.8160 - precision: 0.7509 - recall: 0.7649 - auc: 0.8616 - val_loss: 0.4928 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8925\nEpoch 119/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.5262 - tp: 203.0000 - fp: 65.0000 - tn: 379.0000 - fn: 65.0000 - accuracy: 0.8174 - precision: 0.7575 - recall: 0.7575 - auc: 0.8555 - val_loss: 0.5017 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8925\nEpoch 120/500\n23/23 [==============================] - 1s 25ms/step - loss: 0.5226 - tp: 197.0000 - fp: 56.0000 - tn: 388.0000 - fn: 71.0000 - accuracy: 0.8216 - precision: 0.7787 - recall: 0.7351 - auc: 0.8611 - val_loss: 0.5123 - val_tp: 58.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 16.0000 - val_accuracy: 0.7989 - val_precision: 0.7436 - val_recall: 0.7838 - val_auc: 0.8889\nEpoch 121/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5276 - tp: 203.0000 - fp: 68.0000 - tn: 376.0000 - fn: 65.0000 - accuracy: 0.8132 - precision: 0.7491 - recall: 0.7575 - auc: 0.8554 - val_loss: 0.5121 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8912\nEpoch 122/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5182 - tp: 204.0000 - fp: 60.0000 - tn: 384.0000 - fn: 64.0000 - accuracy: 0.8258 - precision: 0.7727 - recall: 0.7612 - auc: 0.8649 - val_loss: 0.5099 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8928\nEpoch 123/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5128 - tp: 209.0000 - fp: 68.0000 - tn: 376.0000 - fn: 59.0000 - accuracy: 0.8216 - precision: 0.7545 - recall: 0.7799 - auc: 0.8658 - val_loss: 0.5070 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8887\nEpoch 124/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5074 - tp: 203.0000 - fp: 52.0000 - tn: 392.0000 - fn: 65.0000 - accuracy: 0.8357 - precision: 0.7961 - recall: 0.7575 - auc: 0.8712 - val_loss: 0.5186 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8887\nEpoch 125/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5103 - tp: 209.0000 - fp: 64.0000 - tn: 380.0000 - fn: 59.0000 - accuracy: 0.8272 - precision: 0.7656 - recall: 0.7799 - auc: 0.8709 - val_loss: 0.5018 - val_tp: 56.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 18.0000 - val_accuracy: 0.7989 - val_precision: 0.7568 - val_recall: 0.7568 - val_auc: 0.8911\nEpoch 126/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5030 - tp: 204.0000 - fp: 61.0000 - tn: 383.0000 - fn: 64.0000 - accuracy: 0.8244 - precision: 0.7698 - recall: 0.7612 - auc: 0.8719 - val_loss: 0.5202 - val_tp: 61.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 13.0000 - val_accuracy: 0.8156 - val_precision: 0.7531 - val_recall: 0.8243 - val_auc: 0.8896\nEpoch 127/500\n23/23 [==============================] - 1s 25ms/step - loss: 0.5153 - tp: 208.0000 - fp: 66.0000 - tn: 378.0000 - fn: 60.0000 - accuracy: 0.8230 - precision: 0.7591 - recall: 0.7761 - auc: 0.8613 - val_loss: 0.5353 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8888\nEpoch 128/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5060 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8716 - val_loss: 0.5028 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8900\nEpoch 129/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5062 - tp: 207.0000 - fp: 70.0000 - tn: 374.0000 - fn: 61.0000 - accuracy: 0.8160 - precision: 0.7473 - recall: 0.7724 - auc: 0.8746 - val_loss: 0.4972 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8941\nEpoch 130/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5216 - tp: 203.0000 - fp: 66.0000 - tn: 378.0000 - fn: 65.0000 - accuracy: 0.8160 - precision: 0.7546 - recall: 0.7575 - auc: 0.8629 - val_loss: 0.5258 - val_tp: 62.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 12.0000 - val_accuracy: 0.8156 - val_precision: 0.7470 - val_recall: 0.8378 - val_auc: 0.8893\nEpoch 131/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5234 - tp: 206.0000 - fp: 56.0000 - tn: 388.0000 - fn: 62.0000 - accuracy: 0.8343 - precision: 0.7863 - recall: 0.7687 - auc: 0.8540 - val_loss: 0.5129 - val_tp: 62.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 12.0000 - val_accuracy: 0.8212 - val_precision: 0.7561 - val_recall: 0.8378 - val_auc: 0.8918\nEpoch 132/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5252 - tp: 205.0000 - fp: 71.0000 - tn: 373.0000 - fn: 63.0000 - accuracy: 0.8118 - precision: 0.7428 - recall: 0.7649 - auc: 0.8547 - val_loss: 0.5097 - val_tp: 60.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 14.0000 - val_accuracy: 0.7877 - val_precision: 0.7143 - val_recall: 0.8108 - val_auc: 0.8882\nEpoch 133/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5130 - tp: 207.0000 - fp: 59.0000 - tn: 385.0000 - fn: 61.0000 - accuracy: 0.8315 - precision: 0.7782 - recall: 0.7724 - auc: 0.8646 - val_loss: 0.5076 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8884\nEpoch 134/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5153 - tp: 204.0000 - fp: 62.0000 - tn: 382.0000 - fn: 64.0000 - accuracy: 0.8230 - precision: 0.7669 - recall: 0.7612 - auc: 0.8620 - val_loss: 0.4989 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8925\nEpoch 135/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5066 - tp: 209.0000 - fp: 64.0000 - tn: 380.0000 - fn: 59.0000 - accuracy: 0.8272 - precision: 0.7656 - recall: 0.7799 - auc: 0.8646 - val_loss: 0.4953 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8918\nEpoch 136/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5163 - tp: 200.0000 - fp: 55.0000 - tn: 389.0000 - fn: 68.0000 - accuracy: 0.8272 - precision: 0.7843 - recall: 0.7463 - auc: 0.8599 - val_loss: 0.5234 - val_tp: 59.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 15.0000 - val_accuracy: 0.8101 - val_precision: 0.7564 - val_recall: 0.7973 - val_auc: 0.8860\nEpoch 137/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5178 - tp: 203.0000 - fp: 67.0000 - tn: 377.0000 - fn: 65.0000 - accuracy: 0.8146 - precision: 0.7519 - recall: 0.7575 - auc: 0.8665 - val_loss: 0.4934 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8920\nEpoch 138/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5182 - tp: 203.0000 - fp: 57.0000 - tn: 387.0000 - fn: 65.0000 - accuracy: 0.8287 - precision: 0.7808 - recall: 0.7575 - auc: 0.8605 - val_loss: 0.5316 - val_tp: 63.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 11.0000 - val_accuracy: 0.7989 - val_precision: 0.7159 - val_recall: 0.8514 - val_auc: 0.8867\nEpoch 139/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5021 - tp: 208.0000 - fp: 54.0000 - tn: 390.0000 - fn: 60.0000 - accuracy: 0.8399 - precision: 0.7939 - recall: 0.7761 - auc: 0.8778 - val_loss: 0.4974 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8936\nEpoch 140/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5158 - tp: 201.0000 - fp: 49.0000 - tn: 395.0000 - fn: 67.0000 - accuracy: 0.8371 - precision: 0.8040 - recall: 0.7500 - auc: 0.8647 - val_loss: 0.5319 - val_tp: 63.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 11.0000 - val_accuracy: 0.7989 - val_precision: 0.7159 - val_recall: 0.8514 - val_auc: 0.8879\nEpoch 141/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5085 - tp: 217.0000 - fp: 89.0000 - tn: 355.0000 - fn: 51.0000 - accuracy: 0.8034 - precision: 0.7092 - recall: 0.8097 - auc: 0.8708 - val_loss: 0.4905 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8892\nEpoch 142/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5116 - tp: 198.0000 - fp: 49.0000 - tn: 395.0000 - fn: 70.0000 - accuracy: 0.8329 - precision: 0.8016 - recall: 0.7388 - auc: 0.8642 - val_loss: 0.5191 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8893\nEpoch 143/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5170 - tp: 204.0000 - fp: 62.0000 - tn: 382.0000 - fn: 64.0000 - accuracy: 0.8230 - precision: 0.7669 - recall: 0.7612 - auc: 0.8591 - val_loss: 0.5212 - val_tp: 61.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 13.0000 - val_accuracy: 0.7933 - val_precision: 0.7176 - val_recall: 0.8243 - val_auc: 0.8878\nEpoch 144/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5191 - tp: 201.0000 - fp: 65.0000 - tn: 379.0000 - fn: 67.0000 - accuracy: 0.8146 - precision: 0.7556 - recall: 0.7500 - auc: 0.8586 - val_loss: 0.5098 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8943\nEpoch 145/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5028 - tp: 203.0000 - fp: 67.0000 - tn: 377.0000 - fn: 65.0000 - accuracy: 0.8146 - precision: 0.7519 - recall: 0.7575 - auc: 0.8760 - val_loss: 0.4989 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8923\nEpoch 146/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5125 - tp: 210.0000 - fp: 75.0000 - tn: 369.0000 - fn: 58.0000 - accuracy: 0.8132 - precision: 0.7368 - recall: 0.7836 - auc: 0.8626 - val_loss: 0.4936 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8927\nEpoch 147/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5322 - tp: 194.0000 - fp: 51.0000 - tn: 393.0000 - fn: 74.0000 - accuracy: 0.8244 - precision: 0.7918 - recall: 0.7239 - auc: 0.8517 - val_loss: 0.5092 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8925\nEpoch 148/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5281 - tp: 205.0000 - fp: 66.0000 - tn: 378.0000 - fn: 63.0000 - accuracy: 0.8188 - precision: 0.7565 - recall: 0.7649 - auc: 0.8487 - val_loss: 0.5190 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8885\nEpoch 149/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5257 - tp: 214.0000 - fp: 86.0000 - tn: 358.0000 - fn: 54.0000 - accuracy: 0.8034 - precision: 0.7133 - recall: 0.7985 - auc: 0.8596 - val_loss: 0.4969 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8920\nEpoch 150/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5191 - tp: 202.0000 - fp: 62.0000 - tn: 382.0000 - fn: 66.0000 - accuracy: 0.8202 - precision: 0.7652 - recall: 0.7537 - auc: 0.8588 - val_loss: 0.5070 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8936\nEpoch 151/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5106 - tp: 203.0000 - fp: 51.0000 - tn: 393.0000 - fn: 65.0000 - accuracy: 0.8371 - precision: 0.7992 - recall: 0.7575 - auc: 0.8672 - val_loss: 0.5031 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8932\nEpoch 152/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5117 - tp: 201.0000 - fp: 51.0000 - tn: 393.0000 - fn: 67.0000 - accuracy: 0.8343 - precision: 0.7976 - recall: 0.7500 - auc: 0.8610 - val_loss: 0.5209 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8903\nEpoch 153/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5159 - tp: 200.0000 - fp: 51.0000 - tn: 393.0000 - fn: 68.0000 - accuracy: 0.8329 - precision: 0.7968 - recall: 0.7463 - auc: 0.8591 - val_loss: 0.5214 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8912\nEpoch 154/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5007 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8750 - val_loss: 0.5019 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8913\nEpoch 155/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5152 - tp: 197.0000 - fp: 56.0000 - tn: 388.0000 - fn: 71.0000 - accuracy: 0.8216 - precision: 0.7787 - recall: 0.7351 - auc: 0.8607 - val_loss: 0.5374 - val_tp: 64.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 10.0000 - val_accuracy: 0.7933 - val_precision: 0.7033 - val_recall: 0.8649 - val_auc: 0.8873\nEpoch 156/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5165 - tp: 212.0000 - fp: 70.0000 - tn: 374.0000 - fn: 56.0000 - accuracy: 0.8230 - precision: 0.7518 - recall: 0.7910 - auc: 0.8597 - val_loss: 0.5023 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8913\nEpoch 157/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5115 - tp: 205.0000 - fp: 63.0000 - tn: 381.0000 - fn: 63.0000 - accuracy: 0.8230 - precision: 0.7649 - recall: 0.7649 - auc: 0.8652 - val_loss: 0.4937 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8936\nEpoch 158/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4921 - tp: 196.0000 - fp: 43.0000 - tn: 401.0000 - fn: 72.0000 - accuracy: 0.8385 - precision: 0.8201 - recall: 0.7313 - auc: 0.8855 - val_loss: 0.5425 - val_tp: 63.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 11.0000 - val_accuracy: 0.7933 - val_precision: 0.7079 - val_recall: 0.8514 - val_auc: 0.8881\nEpoch 159/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5166 - tp: 205.0000 - fp: 66.0000 - tn: 378.0000 - fn: 63.0000 - accuracy: 0.8188 - precision: 0.7565 - recall: 0.7649 - auc: 0.8611 - val_loss: 0.5034 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8909\nEpoch 160/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5106 - tp: 200.0000 - fp: 51.0000 - tn: 393.0000 - fn: 68.0000 - accuracy: 0.8329 - precision: 0.7968 - recall: 0.7463 - auc: 0.8632 - val_loss: 0.5513 - val_tp: 69.0000 - val_fp: 33.0000 - val_tn: 72.0000 - val_fn: 5.0000 - val_accuracy: 0.7877 - val_precision: 0.6765 - val_recall: 0.9324 - val_auc: 0.8884\nEpoch 161/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5216 - tp: 208.0000 - fp: 70.0000 - tn: 374.0000 - fn: 60.0000 - accuracy: 0.8174 - precision: 0.7482 - recall: 0.7761 - auc: 0.8609 - val_loss: 0.4941 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8927\nEpoch 162/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5051 - tp: 201.0000 - fp: 58.0000 - tn: 386.0000 - fn: 67.0000 - accuracy: 0.8244 - precision: 0.7761 - recall: 0.7500 - auc: 0.8660 - val_loss: 0.5170 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8882\nEpoch 163/500\n23/23 [==============================] - 1s 27ms/step - loss: 0.5134 - tp: 203.0000 - fp: 60.0000 - tn: 384.0000 - fn: 65.0000 - accuracy: 0.8244 - precision: 0.7719 - recall: 0.7575 - auc: 0.8626 - val_loss: 0.5118 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8889\nEpoch 164/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4974 - tp: 209.0000 - fp: 71.0000 - tn: 373.0000 - fn: 59.0000 - accuracy: 0.8174 - precision: 0.7464 - recall: 0.7799 - auc: 0.8733 - val_loss: 0.4940 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8952\nEpoch 165/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5173 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8577 - val_loss: 0.5050 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8906\nEpoch 166/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5265 - tp: 206.0000 - fp: 69.0000 - tn: 375.0000 - fn: 62.0000 - accuracy: 0.8160 - precision: 0.7491 - recall: 0.7687 - auc: 0.8561 - val_loss: 0.5072 - val_tp: 59.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 15.0000 - val_accuracy: 0.7821 - val_precision: 0.7108 - val_recall: 0.7973 - val_auc: 0.8878\nEpoch 167/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5005 - tp: 202.0000 - fp: 67.0000 - tn: 377.0000 - fn: 66.0000 - accuracy: 0.8132 - precision: 0.7509 - recall: 0.7537 - auc: 0.8763 - val_loss: 0.5050 - val_tp: 56.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 18.0000 - val_accuracy: 0.7989 - val_precision: 0.7568 - val_recall: 0.7568 - val_auc: 0.8916\nEpoch 168/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.5095 - tp: 210.0000 - fp: 63.0000 - tn: 381.0000 - fn: 58.0000 - accuracy: 0.8301 - precision: 0.7692 - recall: 0.7836 - auc: 0.8690 - val_loss: 0.4843 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8953\nEpoch 169/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.5039 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8705 - val_loss: 0.5040 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8914\nEpoch 170/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5020 - tp: 205.0000 - fp: 59.0000 - tn: 385.0000 - fn: 63.0000 - accuracy: 0.8287 - precision: 0.7765 - recall: 0.7649 - auc: 0.8696 - val_loss: 0.5052 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8927\nEpoch 171/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5134 - tp: 204.0000 - fp: 54.0000 - tn: 390.0000 - fn: 64.0000 - accuracy: 0.8343 - precision: 0.7907 - recall: 0.7612 - auc: 0.8618 - val_loss: 0.4973 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8941\nEpoch 172/500\n23/23 [==============================] - 1s 26ms/step - loss: 0.5133 - tp: 205.0000 - fp: 58.0000 - tn: 386.0000 - fn: 63.0000 - accuracy: 0.8301 - precision: 0.7795 - recall: 0.7649 - auc: 0.8641 - val_loss: 0.5276 - val_tp: 66.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 8.0000 - val_accuracy: 0.8045 - val_precision: 0.7097 - val_recall: 0.8919 - val_auc: 0.8864\nEpoch 173/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.4960 - tp: 211.0000 - fp: 65.0000 - tn: 379.0000 - fn: 57.0000 - accuracy: 0.8287 - precision: 0.7645 - recall: 0.7873 - auc: 0.8756 - val_loss: 0.4946 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8925\nEpoch 174/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.5225 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8554 - val_loss: 0.5029 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8928\nEpoch 175/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4977 - tp: 201.0000 - fp: 65.0000 - tn: 379.0000 - fn: 67.0000 - accuracy: 0.8146 - precision: 0.7556 - recall: 0.7500 - auc: 0.8727 - val_loss: 0.4887 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8954\nEpoch 176/500\n20/23 [=========================>....] - ETA: 0s - loss: 0.5149 - tp: 175.0000 - fp: 53.0000 - tn: 352.0000 - fn: 60.0000 - accuracy: 0.8234 - precision: 0.7675 - recall: 0.7447 - auc: 0.85823/23 [==============================] - 0s 21ms/step - loss: 0.5089 - tp: 204.0000 - fp: 55.0000 - tn: 389.0000 - fn: 64.0000 - accuracy: 0.8329 - precision: 0.7876 - recall: 0.7612 - auc: 0.8646 - val_loss: 0.4862 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8950\nEpoch 177/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.5026 - tp: 208.0000 - fp: 63.0000 - tn: 381.0000 - fn: 60.0000 - accuracy: 0.8272 - precision: 0.7675 - recall: 0.7761 - auc: 0.8718 - val_loss: 0.5328 - val_tp: 63.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 11.0000 - val_accuracy: 0.7765 - val_precision: 0.6848 - val_recall: 0.8514 - val_auc: 0.8845\nEpoch 178/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.5149 - tp: 206.0000 - fp: 65.0000 - tn: 379.0000 - fn: 62.0000 - accuracy: 0.8216 - precision: 0.7601 - recall: 0.7687 - auc: 0.8518 - val_loss: 0.5082 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8928\nEpoch 179/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4908 - tp: 209.0000 - fp: 58.0000 - tn: 386.0000 - fn: 59.0000 - accuracy: 0.8357 - precision: 0.7828 - recall: 0.7799 - auc: 0.8721 - val_loss: 0.4939 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8956\nEpoch 180/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4962 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8761 - val_loss: 0.5033 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8929\nEpoch 181/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5058 - tp: 199.0000 - fp: 64.0000 - tn: 380.0000 - fn: 69.0000 - accuracy: 0.8132 - precision: 0.7567 - recall: 0.7425 - auc: 0.8662 - val_loss: 0.5109 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8925\nEpoch 182/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5232 - tp: 199.0000 - fp: 61.0000 - tn: 383.0000 - fn: 69.0000 - accuracy: 0.8174 - precision: 0.7654 - recall: 0.7425 - auc: 0.8534 - val_loss: 0.4963 - val_tp: 61.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 13.0000 - val_accuracy: 0.8156 - val_precision: 0.7531 - val_recall: 0.8243 - val_auc: 0.8939\nEpoch 183/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4958 - tp: 209.0000 - fp: 67.0000 - tn: 377.0000 - fn: 59.0000 - accuracy: 0.8230 - precision: 0.7572 - recall: 0.7799 - auc: 0.8776 - val_loss: 0.4920 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8966\nEpoch 184/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5032 - tp: 202.0000 - fp: 56.0000 - tn: 388.0000 - fn: 66.0000 - accuracy: 0.8287 - precision: 0.7829 - recall: 0.7537 - auc: 0.8716 - val_loss: 0.5079 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8925\nEpoch 185/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5047 - tp: 205.0000 - fp: 67.0000 - tn: 377.0000 - fn: 63.0000 - accuracy: 0.8174 - precision: 0.7537 - recall: 0.7649 - auc: 0.8629 - val_loss: 0.4907 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8947\nEpoch 186/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5154 - tp: 200.0000 - fp: 59.0000 - tn: 385.0000 - fn: 68.0000 - accuracy: 0.8216 - precision: 0.7722 - recall: 0.7463 - auc: 0.8549 - val_loss: 0.5216 - val_tp: 65.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 9.0000 - val_accuracy: 0.7989 - val_precision: 0.7065 - val_recall: 0.8784 - val_auc: 0.8875\nEpoch 187/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5230 - tp: 200.0000 - fp: 69.0000 - tn: 375.0000 - fn: 68.0000 - accuracy: 0.8076 - precision: 0.7435 - recall: 0.7463 - auc: 0.8525 - val_loss: 0.4862 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8956\nEpoch 188/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5043 - tp: 203.0000 - fp: 64.0000 - tn: 380.0000 - fn: 65.0000 - accuracy: 0.8188 - precision: 0.7603 - recall: 0.7575 - auc: 0.8651 - val_loss: 0.4946 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8938\nEpoch 189/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4881 - tp: 201.0000 - fp: 55.0000 - tn: 389.0000 - fn: 67.0000 - accuracy: 0.8287 - precision: 0.7852 - recall: 0.7500 - auc: 0.8812 - val_loss: 0.4955 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8952\nEpoch 190/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5059 - tp: 199.0000 - fp: 61.0000 - tn: 383.0000 - fn: 69.0000 - accuracy: 0.8174 - precision: 0.7654 - recall: 0.7425 - auc: 0.8651 - val_loss: 0.5110 - val_tp: 65.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 9.0000 - val_accuracy: 0.7989 - val_precision: 0.7065 - val_recall: 0.8784 - val_auc: 0.8889\nEpoch 191/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.5134 - tp: 214.0000 - fp: 83.0000 - tn: 361.0000 - fn: 54.0000 - accuracy: 0.8076 - precision: 0.7205 - recall: 0.7985 - auc: 0.8588 - val_loss: 0.4912 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8918\nEpoch 192/500\n19/23 [=======================>......] - ETA: 0s - loss: 0.4859 - tp: 176.0000 - fp: 48.0000 - tn: 331.0000 - fn: 53.0000 - accuracy: 0.8339 - precision: 0.7857 - recall: 0.7686 - auc: 0.87623/23 [==============================] - 0s 16ms/step - loss: 0.4954 - tp: 205.0000 - fp: 61.0000 - tn: 383.0000 - fn: 63.0000 - accuracy: 0.8258 - precision: 0.7707 - recall: 0.7649 - auc: 0.8703 - val_loss: 0.5020 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8914\nEpoch 193/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5054 - tp: 199.0000 - fp: 56.0000 - tn: 388.0000 - fn: 69.0000 - accuracy: 0.8244 - precision: 0.7804 - recall: 0.7425 - auc: 0.8695 - val_loss: 0.5107 - val_tp: 61.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 13.0000 - val_accuracy: 0.7989 - val_precision: 0.7262 - val_recall: 0.8243 - val_auc: 0.8901\nEpoch 194/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.4940 - tp: 201.0000 - fp: 58.0000 - tn: 386.0000 - fn: 67.0000 - accuracy: 0.8244 - precision: 0.7761 - recall: 0.7500 - auc: 0.8733 - val_loss: 0.4909 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8914\nEpoch 195/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5088 - tp: 204.0000 - fp: 58.0000 - tn: 386.0000 - fn: 64.0000 - accuracy: 0.8287 - precision: 0.7786 - recall: 0.7612 - auc: 0.8634 - val_loss: 0.5083 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8914\nEpoch 196/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5012 - tp: 200.0000 - fp: 56.0000 - tn: 388.0000 - fn: 68.0000 - accuracy: 0.8258 - precision: 0.7812 - recall: 0.7463 - auc: 0.8671 - val_loss: 0.4955 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8945\nEpoch 197/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5169 - tp: 198.0000 - fp: 63.0000 - tn: 381.0000 - fn: 70.0000 - accuracy: 0.8132 - precision: 0.7586 - recall: 0.7388 - auc: 0.8579 - val_loss: 0.4935 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8934\nEpoch 198/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4990 - tp: 201.0000 - fp: 50.0000 - tn: 394.0000 - fn: 67.0000 - accuracy: 0.8357 - precision: 0.8008 - recall: 0.7500 - auc: 0.8711 - val_loss: 0.4922 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8949\nEpoch 199/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5078 - tp: 199.0000 - fp: 51.0000 - tn: 393.0000 - fn: 69.0000 - accuracy: 0.8315 - precision: 0.7960 - recall: 0.7425 - auc: 0.8593 - val_loss: 0.5092 - val_tp: 62.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 12.0000 - val_accuracy: 0.8324 - val_precision: 0.7750 - val_recall: 0.8378 - val_auc: 0.8934\nEpoch 200/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5136 - tp: 204.0000 - fp: 57.0000 - tn: 387.0000 - fn: 64.0000 - accuracy: 0.8301 - precision: 0.7816 - recall: 0.7612 - auc: 0.8598 - val_loss: 0.5193 - val_tp: 63.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 11.0000 - val_accuracy: 0.8045 - val_precision: 0.7241 - val_recall: 0.8514 - val_auc: 0.8919\nEpoch 201/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4949 - tp: 206.0000 - fp: 64.0000 - tn: 380.0000 - fn: 62.0000 - accuracy: 0.8230 - precision: 0.7630 - recall: 0.7687 - auc: 0.8763 - val_loss: 0.4846 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8941\nEpoch 202/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5062 - tp: 202.0000 - fp: 55.0000 - tn: 389.0000 - fn: 66.0000 - accuracy: 0.8301 - precision: 0.7860 - recall: 0.7537 - auc: 0.8601 - val_loss: 0.5278 - val_tp: 59.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 15.0000 - val_accuracy: 0.8101 - val_precision: 0.7564 - val_recall: 0.7973 - val_auc: 0.8887\nEpoch 203/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5005 - tp: 202.0000 - fp: 59.0000 - tn: 385.0000 - fn: 66.0000 - accuracy: 0.8244 - precision: 0.7739 - recall: 0.7537 - auc: 0.8703 - val_loss: 0.4981 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8915\nEpoch 204/500\n23/23 [==============================] - ETA: 0s - loss: 0.5044 - tp: 207.0000 - fp: 57.0000 - tn: 387.0000 - fn: 61.0000 - accuracy: 0.8343 - precision: 0.7841 - recall: 0.7724 - auc: 0.86323/23 [==============================] - 0s 18ms/step - loss: 0.5044 - tp: 207.0000 - fp: 57.0000 - tn: 387.0000 - fn: 61.0000 - accuracy: 0.8343 - precision: 0.7841 - recall: 0.7724 - auc: 0.8638 - val_loss: 0.5059 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8951\nEpoch 205/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4973 - tp: 198.0000 - fp: 48.0000 - tn: 396.0000 - fn: 70.0000 - accuracy: 0.8343 - precision: 0.8049 - recall: 0.7388 - auc: 0.8690 - val_loss: 0.5264 - val_tp: 63.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 11.0000 - val_accuracy: 0.7933 - val_precision: 0.7079 - val_recall: 0.8514 - val_auc: 0.8853\nEpoch 206/500\n23/23 [==============================] - ETA: 0s - loss: 0.5091 - tp: 202.0000 - fp: 73.0000 - tn: 371.0000 - fn: 66.0000 - accuracy: 0.8048 - precision: 0.7345 - recall: 0.7537 - auc: 0.86823/23 [==============================] - 0s 13ms/step - loss: 0.5091 - tp: 202.0000 - fp: 73.0000 - tn: 371.0000 - fn: 66.0000 - accuracy: 0.8048 - precision: 0.7345 - recall: 0.7537 - auc: 0.8683 - val_loss: 0.4995 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8950\nEpoch 207/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4958 - tp: 210.0000 - fp: 65.0000 - tn: 379.0000 - fn: 58.0000 - accuracy: 0.8272 - precision: 0.7636 - recall: 0.7836 - auc: 0.8662 - val_loss: 0.4961 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8937\nEpoch 208/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5086 - tp: 203.0000 - fp: 45.0000 - tn: 399.0000 - fn: 65.0000 - accuracy: 0.8455 - precision: 0.8185 - recall: 0.7575 - auc: 0.8588 - val_loss: 0.5411 - val_tp: 63.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 11.0000 - val_accuracy: 0.7877 - val_precision: 0.7000 - val_recall: 0.8514 - val_auc: 0.8878\nEpoch 209/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5128 - tp: 206.0000 - fp: 68.0000 - tn: 376.0000 - fn: 62.0000 - accuracy: 0.8174 - precision: 0.7518 - recall: 0.7687 - auc: 0.8606 - val_loss: 0.4947 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8941\nEpoch 210/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4989 - tp: 200.0000 - fp: 53.0000 - tn: 391.0000 - fn: 68.0000 - accuracy: 0.8301 - precision: 0.7905 - recall: 0.7463 - auc: 0.8684 - val_loss: 0.5238 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8910\nEpoch 211/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5070 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8594 - val_loss: 0.5382 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8870\nEpoch 212/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5015 - tp: 199.0000 - fp: 57.0000 - tn: 387.0000 - fn: 69.0000 - accuracy: 0.8230 - precision: 0.7773 - recall: 0.7425 - auc: 0.8646 - val_loss: 0.4975 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8920\nEpoch 213/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5013 - tp: 206.0000 - fp: 53.0000 - tn: 391.0000 - fn: 62.0000 - accuracy: 0.8385 - precision: 0.7954 - recall: 0.7687 - auc: 0.8671 - val_loss: 0.5035 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8927\nEpoch 214/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4930 - tp: 205.0000 - fp: 56.0000 - tn: 388.0000 - fn: 63.0000 - accuracy: 0.8329 - precision: 0.7854 - recall: 0.7649 - auc: 0.8744 - val_loss: 0.4971 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8921\nEpoch 215/500\n23/23 [==============================] - 1s 25ms/step - loss: 0.5120 - tp: 204.0000 - fp: 63.0000 - tn: 381.0000 - fn: 64.0000 - accuracy: 0.8216 - precision: 0.7640 - recall: 0.7612 - auc: 0.8552 - val_loss: 0.4899 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8918\nEpoch 216/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4934 - tp: 202.0000 - fp: 55.0000 - tn: 389.0000 - fn: 66.0000 - accuracy: 0.8301 - precision: 0.7860 - recall: 0.7537 - auc: 0.8705 - val_loss: 0.4976 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8938\nEpoch 217/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4988 - tp: 202.0000 - fp: 56.0000 - tn: 388.0000 - fn: 66.0000 - accuracy: 0.8287 - precision: 0.7829 - recall: 0.7537 - auc: 0.8694 - val_loss: 0.5023 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8925\nEpoch 218/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5104 - tp: 198.0000 - fp: 55.0000 - tn: 389.0000 - fn: 70.0000 - accuracy: 0.8244 - precision: 0.7826 - recall: 0.7388 - auc: 0.8586 - val_loss: 0.5010 - val_tp: 56.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 18.0000 - val_accuracy: 0.7989 - val_precision: 0.7568 - val_recall: 0.7568 - val_auc: 0.8940\nEpoch 219/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4951 - tp: 203.0000 - fp: 54.0000 - tn: 390.0000 - fn: 65.0000 - accuracy: 0.8329 - precision: 0.7899 - recall: 0.7575 - auc: 0.8656 - val_loss: 0.5119 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8934\nEpoch 220/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5092 - tp: 211.0000 - fp: 63.0000 - tn: 381.0000 - fn: 57.0000 - accuracy: 0.8315 - precision: 0.7701 - recall: 0.7873 - auc: 0.8526 - val_loss: 0.5045 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8897\nEpoch 221/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5182 - tp: 199.0000 - fp: 58.0000 - tn: 386.0000 - fn: 69.0000 - accuracy: 0.8216 - precision: 0.7743 - recall: 0.7425 - auc: 0.8555 - val_loss: 0.5030 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8958\nEpoch 222/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4934 - tp: 207.0000 - fp: 56.0000 - tn: 388.0000 - fn: 61.0000 - accuracy: 0.8357 - precision: 0.7871 - recall: 0.7724 - auc: 0.8695 - val_loss: 0.4999 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8952\nEpoch 223/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4869 - tp: 204.0000 - fp: 52.0000 - tn: 392.0000 - fn: 64.0000 - accuracy: 0.8371 - precision: 0.7969 - recall: 0.7612 - auc: 0.8752 - val_loss: 0.5026 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8912\nEpoch 224/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4969 - tp: 200.0000 - fp: 52.0000 - tn: 392.0000 - fn: 68.0000 - accuracy: 0.8315 - precision: 0.7937 - recall: 0.7463 - auc: 0.8675 - val_loss: 0.5104 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8943\nEpoch 225/500\n21/23 [==========================>...] - ETA: 0s - loss: 0.4922 - tp: 189.0000 - fp: 52.0000 - tn: 369.0000 - fn: 62.0000 - accuracy: 0.8304 - precision: 0.7842 - recall: 0.7530 - auc: 0.86423/23 [==============================] - 0s 15ms/step - loss: 0.4897 - tp: 201.0000 - fp: 55.0000 - tn: 389.0000 - fn: 67.0000 - accuracy: 0.8287 - precision: 0.7852 - recall: 0.7500 - auc: 0.8691 - val_loss: 0.4949 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8958\nEpoch 226/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4871 - tp: 209.0000 - fp: 61.0000 - tn: 383.0000 - fn: 59.0000 - accuracy: 0.8315 - precision: 0.7741 - recall: 0.7799 - auc: 0.8737 - val_loss: 0.4907 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8937\nEpoch 227/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.4966 - tp: 194.0000 - fp: 47.0000 - tn: 397.0000 - fn: 74.0000 - accuracy: 0.8301 - precision: 0.8050 - recall: 0.7239 - auc: 0.8733 - val_loss: 0.4994 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8927\nEpoch 228/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5021 - tp: 197.0000 - fp: 52.0000 - tn: 392.0000 - fn: 71.0000 - accuracy: 0.8272 - precision: 0.7912 - recall: 0.7351 - auc: 0.8618 - val_loss: 0.5013 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8950\nEpoch 229/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.5087 - tp: 204.0000 - fp: 57.0000 - tn: 387.0000 - fn: 64.0000 - accuracy: 0.8301 - precision: 0.7816 - recall: 0.7612 - auc: 0.8589 - val_loss: 0.4904 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8988\nEpoch 230/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4954 - tp: 207.0000 - fp: 59.0000 - tn: 385.0000 - fn: 61.0000 - accuracy: 0.8315 - precision: 0.7782 - recall: 0.7724 - auc: 0.8719 - val_loss: 0.4815 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8963\nEpoch 231/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4958 - tp: 202.0000 - fp: 51.0000 - tn: 393.0000 - fn: 66.0000 - accuracy: 0.8357 - precision: 0.7984 - recall: 0.7537 - auc: 0.8704 - val_loss: 0.4884 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8950\nEpoch 232/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.5051 - tp: 202.0000 - fp: 53.0000 - tn: 391.0000 - fn: 66.0000 - accuracy: 0.8329 - precision: 0.7922 - recall: 0.7537 - auc: 0.8571 - val_loss: 0.4863 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8996\nEpoch 233/500\n23/23 [==============================] - 1s 27ms/step - loss: 0.4945 - tp: 201.0000 - fp: 50.0000 - tn: 394.0000 - fn: 67.0000 - accuracy: 0.8357 - precision: 0.8008 - recall: 0.7500 - auc: 0.8666 - val_loss: 0.5016 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8946\nEpoch 234/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4901 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8770 - val_loss: 0.5011 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8955\nEpoch 235/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4977 - tp: 208.0000 - fp: 69.0000 - tn: 375.0000 - fn: 60.0000 - accuracy: 0.8188 - precision: 0.7509 - recall: 0.7761 - auc: 0.8692 - val_loss: 0.4871 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8972\nEpoch 236/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5089 - tp: 200.0000 - fp: 62.0000 - tn: 382.0000 - fn: 68.0000 - accuracy: 0.8174 - precision: 0.7634 - recall: 0.7463 - auc: 0.8548 - val_loss: 0.5143 - val_tp: 62.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 12.0000 - val_accuracy: 0.8156 - val_precision: 0.7470 - val_recall: 0.8378 - val_auc: 0.8928\nEpoch 237/500\n23/23 [==============================] - 1s 28ms/step - loss: 0.5104 - tp: 198.0000 - fp: 52.0000 - tn: 392.0000 - fn: 70.0000 - accuracy: 0.8287 - precision: 0.7920 - recall: 0.7388 - auc: 0.8537 - val_loss: 0.5093 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8919\nEpoch 238/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.4985 - tp: 199.0000 - fp: 48.0000 - tn: 396.0000 - fn: 69.0000 - accuracy: 0.8357 - precision: 0.8057 - recall: 0.7425 - auc: 0.8703 - val_loss: 0.5055 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8895\nEpoch 239/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4876 - tp: 208.0000 - fp: 59.0000 - tn: 385.0000 - fn: 60.0000 - accuracy: 0.8329 - precision: 0.7790 - recall: 0.7761 - auc: 0.8702 - val_loss: 0.4923 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8965\nEpoch 240/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.4911 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8766 - val_loss: 0.4929 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8956\nEpoch 241/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4839 - tp: 209.0000 - fp: 62.0000 - tn: 382.0000 - fn: 59.0000 - accuracy: 0.8301 - precision: 0.7712 - recall: 0.7799 - auc: 0.8765 - val_loss: 0.4889 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8940\nEpoch 242/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4944 - tp: 201.0000 - fp: 49.0000 - tn: 395.0000 - fn: 67.0000 - accuracy: 0.8371 - precision: 0.8040 - recall: 0.7500 - auc: 0.8676 - val_loss: 0.4950 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8959\nEpoch 243/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4899 - tp: 201.0000 - fp: 56.0000 - tn: 388.0000 - fn: 67.0000 - accuracy: 0.8272 - precision: 0.7821 - recall: 0.7500 - auc: 0.8746 - val_loss: 0.5014 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8942\nEpoch 244/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5015 - tp: 205.0000 - fp: 54.0000 - tn: 390.0000 - fn: 63.0000 - accuracy: 0.8357 - precision: 0.7915 - recall: 0.7649 - auc: 0.8616 - val_loss: 0.5020 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8920\nEpoch 245/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5012 - tp: 206.0000 - fp: 55.0000 - tn: 389.0000 - fn: 62.0000 - accuracy: 0.8357 - precision: 0.7893 - recall: 0.7687 - auc: 0.8684 - val_loss: 0.4880 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8943\nEpoch 246/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5052 - tp: 192.0000 - fp: 51.0000 - tn: 393.0000 - fn: 76.0000 - accuracy: 0.8216 - precision: 0.7901 - recall: 0.7164 - auc: 0.8579 - val_loss: 0.5052 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8945\nEpoch 247/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.4982 - tp: 196.0000 - fp: 45.0000 - tn: 399.0000 - fn: 72.0000 - accuracy: 0.8357 - precision: 0.8133 - recall: 0.7313 - auc: 0.8715 - val_loss: 0.5194 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8932\nEpoch 248/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5053 - tp: 197.0000 - fp: 54.0000 - tn: 390.0000 - fn: 71.0000 - accuracy: 0.8244 - precision: 0.7849 - recall: 0.7351 - auc: 0.8594 - val_loss: 0.4990 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8965\nEpoch 249/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4908 - tp: 195.0000 - fp: 57.0000 - tn: 387.0000 - fn: 73.0000 - accuracy: 0.8174 - precision: 0.7738 - recall: 0.7276 - auc: 0.8695 - val_loss: 0.4985 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8931\nEpoch 250/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4956 - tp: 205.0000 - fp: 43.0000 - tn: 401.0000 - fn: 63.0000 - accuracy: 0.8511 - precision: 0.8266 - recall: 0.7649 - auc: 0.8639 - val_loss: 0.5063 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8970\nEpoch 251/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4951 - tp: 201.0000 - fp: 47.0000 - tn: 397.0000 - fn: 67.0000 - accuracy: 0.8399 - precision: 0.8105 - recall: 0.7500 - auc: 0.8715 - val_loss: 0.5363 - val_tp: 67.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 7.0000 - val_accuracy: 0.8156 - val_precision: 0.7204 - val_recall: 0.9054 - val_auc: 0.8927\nEpoch 252/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5005 - tp: 197.0000 - fp: 67.0000 - tn: 377.0000 - fn: 71.0000 - accuracy: 0.8062 - precision: 0.7462 - recall: 0.7351 - auc: 0.8615 - val_loss: 0.4949 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8963\nEpoch 253/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4821 - tp: 200.0000 - fp: 50.0000 - tn: 394.0000 - fn: 68.0000 - accuracy: 0.8343 - precision: 0.8000 - recall: 0.7463 - auc: 0.8793 - val_loss: 0.5019 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8904\nEpoch 254/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5031 - tp: 198.0000 - fp: 54.0000 - tn: 390.0000 - fn: 70.0000 - accuracy: 0.8258 - precision: 0.7857 - recall: 0.7388 - auc: 0.8656 - val_loss: 0.5208 - val_tp: 63.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 11.0000 - val_accuracy: 0.8045 - val_precision: 0.7241 - val_recall: 0.8514 - val_auc: 0.8898\nEpoch 255/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4947 - tp: 202.0000 - fp: 55.0000 - tn: 389.0000 - fn: 66.0000 - accuracy: 0.8301 - precision: 0.7860 - recall: 0.7537 - auc: 0.8717 - val_loss: 0.4980 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8962\nEpoch 256/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5053 - tp: 197.0000 - fp: 50.0000 - tn: 394.0000 - fn: 71.0000 - accuracy: 0.8301 - precision: 0.7976 - recall: 0.7351 - auc: 0.8630 - val_loss: 0.5061 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8904\nEpoch 257/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5076 - tp: 191.0000 - fp: 51.0000 - tn: 393.0000 - fn: 77.0000 - accuracy: 0.8202 - precision: 0.7893 - recall: 0.7127 - auc: 0.8688 - val_loss: 0.5122 - val_tp: 63.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 11.0000 - val_accuracy: 0.8156 - val_precision: 0.7412 - val_recall: 0.8514 - val_auc: 0.8908\nEpoch 258/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4920 - tp: 202.0000 - fp: 60.0000 - tn: 384.0000 - fn: 66.0000 - accuracy: 0.8230 - precision: 0.7710 - recall: 0.7537 - auc: 0.8719 - val_loss: 0.5115 - val_tp: 59.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.7933 - val_precision: 0.7284 - val_recall: 0.7973 - val_auc: 0.8903\nEpoch 259/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4891 - tp: 199.0000 - fp: 57.0000 - tn: 387.0000 - fn: 69.0000 - accuracy: 0.8230 - precision: 0.7773 - recall: 0.7425 - auc: 0.8760 - val_loss: 0.5205 - val_tp: 60.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 14.0000 - val_accuracy: 0.8045 - val_precision: 0.7407 - val_recall: 0.8108 - val_auc: 0.8893\nEpoch 260/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.4967 - tp: 202.0000 - fp: 53.0000 - tn: 391.0000 - fn: 66.0000 - accuracy: 0.8329 - precision: 0.7922 - recall: 0.7537 - auc: 0.8659 - val_loss: 0.5044 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8887\nEpoch 261/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4920 - tp: 200.0000 - fp: 49.0000 - tn: 395.0000 - fn: 68.0000 - accuracy: 0.8357 - precision: 0.8032 - recall: 0.7463 - auc: 0.8675 - val_loss: 0.5192 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8898\nEpoch 262/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4897 - tp: 206.0000 - fp: 65.0000 - tn: 379.0000 - fn: 62.0000 - accuracy: 0.8216 - precision: 0.7601 - recall: 0.7687 - auc: 0.8665 - val_loss: 0.4917 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8938\nEpoch 263/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4888 - tp: 197.0000 - fp: 45.0000 - tn: 399.0000 - fn: 71.0000 - accuracy: 0.8371 - precision: 0.8140 - recall: 0.7351 - auc: 0.8717 - val_loss: 0.4904 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8988\nEpoch 264/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4952 - tp: 207.0000 - fp: 60.0000 - tn: 384.0000 - fn: 61.0000 - accuracy: 0.8301 - precision: 0.7753 - recall: 0.7724 - auc: 0.8669 - val_loss: 0.4827 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8947\nEpoch 265/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4945 - tp: 196.0000 - fp: 48.0000 - tn: 396.0000 - fn: 72.0000 - accuracy: 0.8315 - precision: 0.8033 - recall: 0.7313 - auc: 0.8725 - val_loss: 0.4919 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8986\nEpoch 266/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4917 - tp: 198.0000 - fp: 52.0000 - tn: 392.0000 - fn: 70.0000 - accuracy: 0.8287 - precision: 0.7920 - recall: 0.7388 - auc: 0.8638 - val_loss: 0.4956 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8947\nEpoch 267/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4917 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8719 - val_loss: 0.5092 - val_tp: 60.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 14.0000 - val_accuracy: 0.8045 - val_precision: 0.7407 - val_recall: 0.8108 - val_auc: 0.8931\nEpoch 268/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4931 - tp: 202.0000 - fp: 63.0000 - tn: 381.0000 - fn: 66.0000 - accuracy: 0.8188 - precision: 0.7623 - recall: 0.7537 - auc: 0.8724 - val_loss: 0.4808 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.9014\nEpoch 269/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4883 - tp: 197.0000 - fp: 48.0000 - tn: 396.0000 - fn: 71.0000 - accuracy: 0.8329 - precision: 0.8041 - recall: 0.7351 - auc: 0.8723 - val_loss: 0.4860 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8994\nEpoch 270/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4997 - tp: 200.0000 - fp: 47.0000 - tn: 397.0000 - fn: 68.0000 - accuracy: 0.8385 - precision: 0.8097 - recall: 0.7463 - auc: 0.8577 - val_loss: 0.5020 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8947\nEpoch 271/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.4987 - tp: 198.0000 - fp: 51.0000 - tn: 393.0000 - fn: 70.0000 - accuracy: 0.8301 - precision: 0.7952 - recall: 0.7388 - auc: 0.8614 - val_loss: 0.5207 - val_tp: 59.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.7933 - val_precision: 0.7284 - val_recall: 0.7973 - val_auc: 0.8893\nEpoch 272/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5024 - tp: 199.0000 - fp: 51.0000 - tn: 393.0000 - fn: 69.0000 - accuracy: 0.8315 - precision: 0.7960 - recall: 0.7425 - auc: 0.8567 - val_loss: 0.4965 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8966\nEpoch 273/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4925 - tp: 203.0000 - fp: 57.0000 - tn: 387.0000 - fn: 65.0000 - accuracy: 0.8287 - precision: 0.7808 - recall: 0.7575 - auc: 0.8660 - val_loss: 0.4976 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8968\nEpoch 274/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5071 - tp: 205.0000 - fp: 61.0000 - tn: 383.0000 - fn: 63.0000 - accuracy: 0.8258 - precision: 0.7707 - recall: 0.7649 - auc: 0.8631 - val_loss: 0.4920 - val_tp: 60.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 14.0000 - val_accuracy: 0.8045 - val_precision: 0.7407 - val_recall: 0.8108 - val_auc: 0.8962\nEpoch 275/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4892 - tp: 203.0000 - fp: 58.0000 - tn: 386.0000 - fn: 65.0000 - accuracy: 0.8272 - precision: 0.7778 - recall: 0.7575 - auc: 0.8687 - val_loss: 0.5108 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8934\nEpoch 276/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4853 - tp: 200.0000 - fp: 49.0000 - tn: 395.0000 - fn: 68.0000 - accuracy: 0.8357 - precision: 0.8032 - recall: 0.7463 - auc: 0.8734 - val_loss: 0.4912 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8944\nEpoch 277/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4923 - tp: 203.0000 - fp: 60.0000 - tn: 384.0000 - fn: 65.0000 - accuracy: 0.8244 - precision: 0.7719 - recall: 0.7575 - auc: 0.8664 - val_loss: 0.4817 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8935\nEpoch 278/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4997 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8632 - val_loss: 0.4857 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8958\nEpoch 279/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4788 - tp: 200.0000 - fp: 45.0000 - tn: 399.0000 - fn: 68.0000 - accuracy: 0.8413 - precision: 0.8163 - recall: 0.7463 - auc: 0.8805 - val_loss: 0.4928 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8945\nEpoch 280/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.5113 - tp: 210.0000 - fp: 83.0000 - tn: 361.0000 - fn: 58.0000 - accuracy: 0.8020 - precision: 0.7167 - recall: 0.7836 - auc: 0.8630 - val_loss: 0.4834 - val_tp: 62.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 12.0000 - val_accuracy: 0.8045 - val_precision: 0.7294 - val_recall: 0.8378 - val_auc: 0.8959\nEpoch 281/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4858 - tp: 207.0000 - fp: 57.0000 - tn: 387.0000 - fn: 61.0000 - accuracy: 0.8343 - precision: 0.7841 - recall: 0.7724 - auc: 0.8771 - val_loss: 0.4975 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8943\nEpoch 282/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5008 - tp: 200.0000 - fp: 50.0000 - tn: 394.0000 - fn: 68.0000 - accuracy: 0.8343 - precision: 0.8000 - recall: 0.7463 - auc: 0.8628 - val_loss: 0.5046 - val_tp: 59.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 15.0000 - val_accuracy: 0.8101 - val_precision: 0.7564 - val_recall: 0.7973 - val_auc: 0.8882\nEpoch 283/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4848 - tp: 210.0000 - fp: 58.0000 - tn: 386.0000 - fn: 58.0000 - accuracy: 0.8371 - precision: 0.7836 - recall: 0.7836 - auc: 0.8727 - val_loss: 0.4783 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8939\nEpoch 284/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4891 - tp: 203.0000 - fp: 57.0000 - tn: 387.0000 - fn: 65.0000 - accuracy: 0.8287 - precision: 0.7808 - recall: 0.7575 - auc: 0.8703 - val_loss: 0.4891 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8931\nEpoch 285/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4848 - tp: 195.0000 - fp: 50.0000 - tn: 394.0000 - fn: 73.0000 - accuracy: 0.8272 - precision: 0.7959 - recall: 0.7276 - auc: 0.8723 - val_loss: 0.5004 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8945\nEpoch 286/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4928 - tp: 208.0000 - fp: 56.0000 - tn: 388.0000 - fn: 60.0000 - accuracy: 0.8371 - precision: 0.7879 - recall: 0.7761 - auc: 0.8651 - val_loss: 0.4994 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8944\nEpoch 287/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4884 - tp: 201.0000 - fp: 60.0000 - tn: 384.0000 - fn: 67.0000 - accuracy: 0.8216 - precision: 0.7701 - recall: 0.7500 - auc: 0.8750 - val_loss: 0.4954 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8973\nEpoch 288/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4799 - tp: 205.0000 - fp: 48.0000 - tn: 396.0000 - fn: 63.0000 - accuracy: 0.8441 - precision: 0.8103 - recall: 0.7649 - auc: 0.8719 - val_loss: 0.4889 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8983\nEpoch 289/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4985 - tp: 203.0000 - fp: 56.0000 - tn: 388.0000 - fn: 65.0000 - accuracy: 0.8301 - precision: 0.7838 - recall: 0.7575 - auc: 0.8703 - val_loss: 0.5015 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8916\nEpoch 290/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4966 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8654 - val_loss: 0.5049 - val_tp: 59.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.7933 - val_precision: 0.7284 - val_recall: 0.7973 - val_auc: 0.8932\nEpoch 291/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.4980 - tp: 214.0000 - fp: 65.0000 - tn: 379.0000 - fn: 54.0000 - accuracy: 0.8329 - precision: 0.7670 - recall: 0.7985 - auc: 0.8594 - val_loss: 0.4801 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8950\nEpoch 292/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4938 - tp: 208.0000 - fp: 61.0000 - tn: 383.0000 - fn: 60.0000 - accuracy: 0.8301 - precision: 0.7732 - recall: 0.7761 - auc: 0.8671 - val_loss: 0.5016 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8982\nEpoch 293/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4965 - tp: 197.0000 - fp: 56.0000 - tn: 388.0000 - fn: 71.0000 - accuracy: 0.8216 - precision: 0.7787 - recall: 0.7351 - auc: 0.8617 - val_loss: 0.4928 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8961\nEpoch 294/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4920 - tp: 200.0000 - fp: 56.0000 - tn: 388.0000 - fn: 68.0000 - accuracy: 0.8258 - precision: 0.7812 - recall: 0.7463 - auc: 0.8710 - val_loss: 0.4984 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8936\nEpoch 295/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4916 - tp: 202.0000 - fp: 56.0000 - tn: 388.0000 - fn: 66.0000 - accuracy: 0.8287 - precision: 0.7829 - recall: 0.7537 - auc: 0.8709 - val_loss: 0.4880 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8943\nEpoch 296/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.4952 - tp: 197.0000 - fp: 41.0000 - tn: 403.0000 - fn: 71.0000 - accuracy: 0.8427 - precision: 0.8277 - recall: 0.7351 - auc: 0.8655 - val_loss: 0.5123 - val_tp: 62.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 12.0000 - val_accuracy: 0.8380 - val_precision: 0.7848 - val_recall: 0.8378 - val_auc: 0.8923\nEpoch 297/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4948 - tp: 203.0000 - fp: 59.0000 - tn: 385.0000 - fn: 65.0000 - accuracy: 0.8258 - precision: 0.7748 - recall: 0.7575 - auc: 0.8654 - val_loss: 0.4911 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8960\nEpoch 298/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.4944 - tp: 201.0000 - fp: 65.0000 - tn: 379.0000 - fn: 67.0000 - accuracy: 0.8146 - precision: 0.7556 - recall: 0.7500 - auc: 0.8569 - val_loss: 0.4930 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8942\nEpoch 299/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4841 - tp: 210.0000 - fp: 56.0000 - tn: 388.0000 - fn: 58.0000 - accuracy: 0.8399 - precision: 0.7895 - recall: 0.7836 - auc: 0.8735 - val_loss: 0.4804 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8951\nEpoch 300/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4953 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8691 - val_loss: 0.4955 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8940\nEpoch 301/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4937 - tp: 199.0000 - fp: 52.0000 - tn: 392.0000 - fn: 69.0000 - accuracy: 0.8301 - precision: 0.7928 - recall: 0.7425 - auc: 0.8680 - val_loss: 0.4947 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8939\nEpoch 302/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4841 - tp: 204.0000 - fp: 60.0000 - tn: 384.0000 - fn: 64.0000 - accuracy: 0.8258 - precision: 0.7727 - recall: 0.7612 - auc: 0.8736 - val_loss: 0.4805 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8945\nEpoch 303/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4943 - tp: 206.0000 - fp: 59.0000 - tn: 385.0000 - fn: 62.0000 - accuracy: 0.8301 - precision: 0.7774 - recall: 0.7687 - auc: 0.8680 - val_loss: 0.4735 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8967\nEpoch 304/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4961 - tp: 195.0000 - fp: 47.0000 - tn: 397.0000 - fn: 73.0000 - accuracy: 0.8315 - precision: 0.8058 - recall: 0.7276 - auc: 0.8585 - val_loss: 0.4927 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8997\nEpoch 305/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4898 - tp: 205.0000 - fp: 53.0000 - tn: 391.0000 - fn: 63.0000 - accuracy: 0.8371 - precision: 0.7946 - recall: 0.7649 - auc: 0.8670 - val_loss: 0.4761 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8999\nEpoch 306/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4937 - tp: 206.0000 - fp: 62.0000 - tn: 382.0000 - fn: 62.0000 - accuracy: 0.8258 - precision: 0.7687 - recall: 0.7687 - auc: 0.8640 - val_loss: 0.4777 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8929\nEpoch 307/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5029 - tp: 195.0000 - fp: 46.0000 - tn: 398.0000 - fn: 73.0000 - accuracy: 0.8329 - precision: 0.8091 - recall: 0.7276 - auc: 0.8559 - val_loss: 0.4838 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8961\nEpoch 308/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.5029 - tp: 204.0000 - fp: 55.0000 - tn: 389.0000 - fn: 64.0000 - accuracy: 0.8329 - precision: 0.7876 - recall: 0.7612 - auc: 0.8608 - val_loss: 0.4902 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8906\nEpoch 309/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4812 - tp: 205.0000 - fp: 50.0000 - tn: 394.0000 - fn: 63.0000 - accuracy: 0.8413 - precision: 0.8039 - recall: 0.7649 - auc: 0.8756 - val_loss: 0.4952 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8900\nEpoch 310/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4918 - tp: 194.0000 - fp: 40.0000 - tn: 404.0000 - fn: 74.0000 - accuracy: 0.8399 - precision: 0.8291 - recall: 0.7239 - auc: 0.8697 - val_loss: 0.5053 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8914\nEpoch 311/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.5055 - tp: 204.0000 - fp: 57.0000 - tn: 387.0000 - fn: 64.0000 - accuracy: 0.8301 - precision: 0.7816 - recall: 0.7612 - auc: 0.8617 - val_loss: 0.4708 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8962\nEpoch 312/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4888 - tp: 208.0000 - fp: 51.0000 - tn: 393.0000 - fn: 60.0000 - accuracy: 0.8441 - precision: 0.8031 - recall: 0.7761 - auc: 0.8672 - val_loss: 0.4864 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8974\nEpoch 313/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4841 - tp: 201.0000 - fp: 51.0000 - tn: 393.0000 - fn: 67.0000 - accuracy: 0.8343 - precision: 0.7976 - recall: 0.7500 - auc: 0.8708 - val_loss: 0.4910 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8965\nEpoch 314/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4919 - tp: 197.0000 - fp: 43.0000 - tn: 401.0000 - fn: 71.0000 - accuracy: 0.8399 - precision: 0.8208 - recall: 0.7351 - auc: 0.8682 - val_loss: 0.4955 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8974\nEpoch 315/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4923 - tp: 203.0000 - fp: 56.0000 - tn: 388.0000 - fn: 65.0000 - accuracy: 0.8301 - precision: 0.7838 - recall: 0.7575 - auc: 0.8656 - val_loss: 0.5089 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8880\nEpoch 316/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4812 - tp: 206.0000 - fp: 56.0000 - tn: 388.0000 - fn: 62.0000 - accuracy: 0.8343 - precision: 0.7863 - recall: 0.7687 - auc: 0.8781 - val_loss: 0.4796 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8985\nEpoch 317/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4868 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8756 - val_loss: 0.4709 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8986\nEpoch 318/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4950 - tp: 200.0000 - fp: 62.0000 - tn: 382.0000 - fn: 68.0000 - accuracy: 0.8174 - precision: 0.7634 - recall: 0.7463 - auc: 0.8668 - val_loss: 0.4889 - val_tp: 57.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 17.0000 - val_accuracy: 0.7989 - val_precision: 0.7500 - val_recall: 0.7703 - val_auc: 0.8895\nEpoch 319/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5004 - tp: 197.0000 - fp: 50.0000 - tn: 394.0000 - fn: 71.0000 - accuracy: 0.8301 - precision: 0.7976 - recall: 0.7351 - auc: 0.8640 - val_loss: 0.5018 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8948\nEpoch 320/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5128 - tp: 195.0000 - fp: 51.0000 - tn: 393.0000 - fn: 73.0000 - accuracy: 0.8258 - precision: 0.7927 - recall: 0.7276 - auc: 0.8526 - val_loss: 0.4818 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8984\nEpoch 321/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4850 - tp: 210.0000 - fp: 66.0000 - tn: 378.0000 - fn: 58.0000 - accuracy: 0.8258 - precision: 0.7609 - recall: 0.7836 - auc: 0.8706 - val_loss: 0.4854 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8956\nEpoch 322/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4868 - tp: 201.0000 - fp: 55.0000 - tn: 389.0000 - fn: 67.0000 - accuracy: 0.8287 - precision: 0.7852 - recall: 0.7500 - auc: 0.8663 - val_loss: 0.4831 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8947\nEpoch 323/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4920 - tp: 196.0000 - fp: 49.0000 - tn: 395.0000 - fn: 72.0000 - accuracy: 0.8301 - precision: 0.8000 - recall: 0.7313 - auc: 0.8711 - val_loss: 0.4818 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8953\nEpoch 324/500\n23/23 [==============================] - 1s 25ms/step - loss: 0.4832 - tp: 208.0000 - fp: 64.0000 - tn: 380.0000 - fn: 60.0000 - accuracy: 0.8258 - precision: 0.7647 - recall: 0.7761 - auc: 0.8665 - val_loss: 0.4862 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8959\nEpoch 325/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4962 - tp: 199.0000 - fp: 53.0000 - tn: 391.0000 - fn: 69.0000 - accuracy: 0.8287 - precision: 0.7897 - recall: 0.7425 - auc: 0.8598 - val_loss: 0.4945 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.9006\nEpoch 326/500\n23/23 [==============================] - 1s 28ms/step - loss: 0.4843 - tp: 203.0000 - fp: 60.0000 - tn: 384.0000 - fn: 65.0000 - accuracy: 0.8244 - precision: 0.7719 - recall: 0.7575 - auc: 0.8680 - val_loss: 0.4809 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8945\nEpoch 327/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4926 - tp: 191.0000 - fp: 42.0000 - tn: 402.0000 - fn: 77.0000 - accuracy: 0.8329 - precision: 0.8197 - recall: 0.7127 - auc: 0.8678 - val_loss: 0.5237 - val_tp: 61.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 13.0000 - val_accuracy: 0.7989 - val_precision: 0.7262 - val_recall: 0.8243 - val_auc: 0.8928\nEpoch 328/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4854 - tp: 206.0000 - fp: 56.0000 - tn: 388.0000 - fn: 62.0000 - accuracy: 0.8343 - precision: 0.7863 - recall: 0.7687 - auc: 0.8748 - val_loss: 0.4745 - val_tp: 53.0000 - val_fp: 10.0000 - val_tn: 95.0000 - val_fn: 21.0000 - val_accuracy: 0.8268 - val_precision: 0.8413 - val_recall: 0.7162 - val_auc: 0.8983\nEpoch 329/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.4966 - tp: 197.0000 - fp: 53.0000 - tn: 391.0000 - fn: 71.0000 - accuracy: 0.8258 - precision: 0.7880 - recall: 0.7351 - auc: 0.8665 - val_loss: 0.4737 - val_tp: 56.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 18.0000 - val_accuracy: 0.8380 - val_precision: 0.8358 - val_recall: 0.7568 - val_auc: 0.9005\nEpoch 330/500\n23/23 [==============================] - 1s 25ms/step - loss: 0.5078 - tp: 190.0000 - fp: 50.0000 - tn: 394.0000 - fn: 78.0000 - accuracy: 0.8202 - precision: 0.7917 - recall: 0.7090 - auc: 0.8596 - val_loss: 0.4827 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8944\nEpoch 331/500\n23/23 [==============================] - 1s 30ms/step - loss: 0.4963 - tp: 207.0000 - fp: 64.0000 - tn: 380.0000 - fn: 61.0000 - accuracy: 0.8244 - precision: 0.7638 - recall: 0.7724 - auc: 0.8659 - val_loss: 0.4721 - val_tp: 55.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 19.0000 - val_accuracy: 0.8101 - val_precision: 0.7857 - val_recall: 0.7432 - val_auc: 0.8954\nEpoch 332/500\n23/23 [==============================] - 1s 35ms/step - loss: 0.5092 - tp: 192.0000 - fp: 51.0000 - tn: 393.0000 - fn: 76.0000 - accuracy: 0.8216 - precision: 0.7901 - recall: 0.7164 - auc: 0.8497 - val_loss: 0.4999 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8925\nEpoch 333/500\n23/23 [==============================] - 1s 26ms/step - loss: 0.4817 - tp: 204.0000 - fp: 56.0000 - tn: 388.0000 - fn: 64.0000 - accuracy: 0.8315 - precision: 0.7846 - recall: 0.7612 - auc: 0.8735 - val_loss: 0.5004 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8906\nEpoch 334/500\n23/23 [==============================] - 1s 29ms/step - loss: 0.5009 - tp: 195.0000 - fp: 52.0000 - tn: 392.0000 - fn: 73.0000 - accuracy: 0.8244 - precision: 0.7895 - recall: 0.7276 - auc: 0.8595 - val_loss: 0.5026 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8954\nEpoch 335/500\n23/23 [==============================] - 1s 25ms/step - loss: 0.4901 - tp: 198.0000 - fp: 51.0000 - tn: 393.0000 - fn: 70.0000 - accuracy: 0.8301 - precision: 0.7952 - recall: 0.7388 - auc: 0.8708 - val_loss: 0.4818 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8909\nEpoch 336/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4837 - tp: 210.0000 - fp: 57.0000 - tn: 387.0000 - fn: 58.0000 - accuracy: 0.8385 - precision: 0.7865 - recall: 0.7836 - auc: 0.8719 - val_loss: 0.4844 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8936\nEpoch 337/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4931 - tp: 199.0000 - fp: 47.0000 - tn: 397.0000 - fn: 69.0000 - accuracy: 0.8371 - precision: 0.8089 - recall: 0.7425 - auc: 0.8642 - val_loss: 0.5011 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8927\nEpoch 338/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4919 - tp: 202.0000 - fp: 54.0000 - tn: 390.0000 - fn: 66.0000 - accuracy: 0.8315 - precision: 0.7891 - recall: 0.7537 - auc: 0.8699 - val_loss: 0.4926 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8955\nEpoch 339/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.4897 - tp: 195.0000 - fp: 43.0000 - tn: 401.0000 - fn: 73.0000 - accuracy: 0.8371 - precision: 0.8193 - recall: 0.7276 - auc: 0.8679 - val_loss: 0.5007 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8952\nEpoch 340/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4810 - tp: 200.0000 - fp: 50.0000 - tn: 394.0000 - fn: 68.0000 - accuracy: 0.8343 - precision: 0.8000 - recall: 0.7463 - auc: 0.8764 - val_loss: 0.4916 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8950\nEpoch 341/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4888 - tp: 207.0000 - fp: 59.0000 - tn: 385.0000 - fn: 61.0000 - accuracy: 0.8315 - precision: 0.7782 - recall: 0.7724 - auc: 0.8731 - val_loss: 0.4910 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8976\nEpoch 342/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4824 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8736 - val_loss: 0.4758 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8996\nEpoch 343/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4849 - tp: 210.0000 - fp: 62.0000 - tn: 382.0000 - fn: 58.0000 - accuracy: 0.8315 - precision: 0.7721 - recall: 0.7836 - auc: 0.8788 - val_loss: 0.4818 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8975\nEpoch 344/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5027 - tp: 200.0000 - fp: 51.0000 - tn: 393.0000 - fn: 68.0000 - accuracy: 0.8329 - precision: 0.7968 - recall: 0.7463 - auc: 0.8577 - val_loss: 0.5002 - val_tp: 63.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 11.0000 - val_accuracy: 0.8156 - val_precision: 0.7412 - val_recall: 0.8514 - val_auc: 0.8979\nEpoch 345/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4796 - tp: 193.0000 - fp: 45.0000 - tn: 399.0000 - fn: 75.0000 - accuracy: 0.8315 - precision: 0.8109 - recall: 0.7201 - auc: 0.8861 - val_loss: 0.4938 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8976\nEpoch 346/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4790 - tp: 201.0000 - fp: 43.0000 - tn: 401.0000 - fn: 67.0000 - accuracy: 0.8455 - precision: 0.8238 - recall: 0.7500 - auc: 0.8774 - val_loss: 0.4781 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8967\nEpoch 347/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4907 - tp: 202.0000 - fp: 44.0000 - tn: 400.0000 - fn: 66.0000 - accuracy: 0.8455 - precision: 0.8211 - recall: 0.7537 - auc: 0.8621 - val_loss: 0.5054 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8924\nEpoch 348/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4954 - tp: 204.0000 - fp: 69.0000 - tn: 375.0000 - fn: 64.0000 - accuracy: 0.8132 - precision: 0.7473 - recall: 0.7612 - auc: 0.8723 - val_loss: 0.4782 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8949\nEpoch 349/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4884 - tp: 198.0000 - fp: 54.0000 - tn: 390.0000 - fn: 70.0000 - accuracy: 0.8258 - precision: 0.7857 - recall: 0.7388 - auc: 0.8730 - val_loss: 0.4837 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8945\nEpoch 350/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4964 - tp: 203.0000 - fp: 50.0000 - tn: 394.0000 - fn: 65.0000 - accuracy: 0.8385 - precision: 0.8024 - recall: 0.7575 - auc: 0.8627 - val_loss: 0.4903 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8954\nEpoch 351/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4899 - tp: 200.0000 - fp: 49.0000 - tn: 395.0000 - fn: 68.0000 - accuracy: 0.8357 - precision: 0.8032 - recall: 0.7463 - auc: 0.8740 - val_loss: 0.4958 - val_tp: 62.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 12.0000 - val_accuracy: 0.8380 - val_precision: 0.7848 - val_recall: 0.8378 - val_auc: 0.8981\nEpoch 352/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4881 - tp: 207.0000 - fp: 58.0000 - tn: 386.0000 - fn: 61.0000 - accuracy: 0.8329 - precision: 0.7811 - recall: 0.7724 - auc: 0.8653 - val_loss: 0.4795 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8992\nEpoch 353/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4890 - tp: 199.0000 - fp: 53.0000 - tn: 391.0000 - fn: 69.0000 - accuracy: 0.8287 - precision: 0.7897 - recall: 0.7425 - auc: 0.8698 - val_loss: 0.4961 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8961\nEpoch 354/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4996 - tp: 191.0000 - fp: 46.0000 - tn: 398.0000 - fn: 77.0000 - accuracy: 0.8272 - precision: 0.8059 - recall: 0.7127 - auc: 0.8629 - val_loss: 0.5353 - val_tp: 67.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 7.0000 - val_accuracy: 0.8101 - val_precision: 0.7128 - val_recall: 0.9054 - val_auc: 0.8920\nEpoch 355/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4980 - tp: 205.0000 - fp: 63.0000 - tn: 381.0000 - fn: 63.0000 - accuracy: 0.8230 - precision: 0.7649 - recall: 0.7649 - auc: 0.8565 - val_loss: 0.4878 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8955\nEpoch 356/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4919 - tp: 199.0000 - fp: 38.0000 - tn: 406.0000 - fn: 69.0000 - accuracy: 0.8497 - precision: 0.8397 - recall: 0.7425 - auc: 0.8609 - val_loss: 0.5204 - val_tp: 66.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 8.0000 - val_accuracy: 0.8045 - val_precision: 0.7097 - val_recall: 0.8919 - val_auc: 0.8936\nEpoch 357/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4840 - tp: 203.0000 - fp: 51.0000 - tn: 393.0000 - fn: 65.0000 - accuracy: 0.8371 - precision: 0.7992 - recall: 0.7575 - auc: 0.8773 - val_loss: 0.4940 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8957\nEpoch 358/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4891 - tp: 200.0000 - fp: 53.0000 - tn: 391.0000 - fn: 68.0000 - accuracy: 0.8301 - precision: 0.7905 - recall: 0.7463 - auc: 0.8661 - val_loss: 0.4957 - val_tp: 63.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 11.0000 - val_accuracy: 0.8212 - val_precision: 0.7500 - val_recall: 0.8514 - val_auc: 0.8970\nEpoch 359/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4875 - tp: 214.0000 - fp: 70.0000 - tn: 374.0000 - fn: 54.0000 - accuracy: 0.8258 - precision: 0.7535 - recall: 0.7985 - auc: 0.8641 - val_loss: 0.4746 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8985\nEpoch 360/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5076 - tp: 197.0000 - fp: 64.0000 - tn: 380.0000 - fn: 71.0000 - accuracy: 0.8104 - precision: 0.7548 - recall: 0.7351 - auc: 0.8635 - val_loss: 0.4952 - val_tp: 61.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 13.0000 - val_accuracy: 0.8156 - val_precision: 0.7531 - val_recall: 0.8243 - val_auc: 0.8947\nEpoch 361/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4964 - tp: 199.0000 - fp: 50.0000 - tn: 394.0000 - fn: 69.0000 - accuracy: 0.8329 - precision: 0.7992 - recall: 0.7425 - auc: 0.8671 - val_loss: 0.4840 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8968\nEpoch 362/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4929 - tp: 203.0000 - fp: 61.0000 - tn: 383.0000 - fn: 65.0000 - accuracy: 0.8230 - precision: 0.7689 - recall: 0.7575 - auc: 0.8673 - val_loss: 0.4730 - val_tp: 53.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 21.0000 - val_accuracy: 0.8101 - val_precision: 0.8030 - val_recall: 0.7162 - val_auc: 0.8959\nEpoch 363/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5008 - tp: 196.0000 - fp: 51.0000 - tn: 393.0000 - fn: 72.0000 - accuracy: 0.8272 - precision: 0.7935 - recall: 0.7313 - auc: 0.8621 - val_loss: 0.4821 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8935\nEpoch 364/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4870 - tp: 203.0000 - fp: 58.0000 - tn: 386.0000 - fn: 65.0000 - accuracy: 0.8272 - precision: 0.7778 - recall: 0.7575 - auc: 0.8693 - val_loss: 0.5023 - val_tp: 56.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 18.0000 - val_accuracy: 0.7877 - val_precision: 0.7368 - val_recall: 0.7568 - val_auc: 0.8912\nEpoch 365/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4863 - tp: 204.0000 - fp: 53.0000 - tn: 391.0000 - fn: 64.0000 - accuracy: 0.8357 - precision: 0.7938 - recall: 0.7612 - auc: 0.8668 - val_loss: 0.4848 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8977\nEpoch 366/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4912 - tp: 197.0000 - fp: 43.0000 - tn: 401.0000 - fn: 71.0000 - accuracy: 0.8399 - precision: 0.8208 - recall: 0.7351 - auc: 0.8673 - val_loss: 0.5138 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8924\nEpoch 367/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4956 - tp: 193.0000 - fp: 55.0000 - tn: 389.0000 - fn: 75.0000 - accuracy: 0.8174 - precision: 0.7782 - recall: 0.7201 - auc: 0.8692 - val_loss: 0.4868 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8961\nEpoch 368/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4892 - tp: 199.0000 - fp: 46.0000 - tn: 398.0000 - fn: 69.0000 - accuracy: 0.8385 - precision: 0.8122 - recall: 0.7425 - auc: 0.8693 - val_loss: 0.5183 - val_tp: 63.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 11.0000 - val_accuracy: 0.8156 - val_precision: 0.7412 - val_recall: 0.8514 - val_auc: 0.8905\nEpoch 369/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4679 - tp: 204.0000 - fp: 53.0000 - tn: 391.0000 - fn: 64.0000 - accuracy: 0.8357 - precision: 0.7938 - recall: 0.7612 - auc: 0.8825 - val_loss: 0.5131 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8929\nEpoch 370/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4871 - tp: 211.0000 - fp: 68.0000 - tn: 376.0000 - fn: 57.0000 - accuracy: 0.8244 - precision: 0.7563 - recall: 0.7873 - auc: 0.8726 - val_loss: 0.4800 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8945\nEpoch 371/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4873 - tp: 194.0000 - fp: 43.0000 - tn: 401.0000 - fn: 74.0000 - accuracy: 0.8357 - precision: 0.8186 - recall: 0.7239 - auc: 0.8698 - val_loss: 0.5160 - val_tp: 63.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 11.0000 - val_accuracy: 0.8101 - val_precision: 0.7326 - val_recall: 0.8514 - val_auc: 0.8925\nEpoch 372/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4953 - tp: 201.0000 - fp: 51.0000 - tn: 393.0000 - fn: 67.0000 - accuracy: 0.8343 - precision: 0.7976 - recall: 0.7500 - auc: 0.8613 - val_loss: 0.5008 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8957\nEpoch 373/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4831 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8721 - val_loss: 0.4741 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8994\nEpoch 374/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4779 - tp: 205.0000 - fp: 55.0000 - tn: 389.0000 - fn: 63.0000 - accuracy: 0.8343 - precision: 0.7885 - recall: 0.7649 - auc: 0.8745 - val_loss: 0.4845 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8963\nEpoch 375/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4855 - tp: 198.0000 - fp: 49.0000 - tn: 395.0000 - fn: 70.0000 - accuracy: 0.8329 - precision: 0.8016 - recall: 0.7388 - auc: 0.8725 - val_loss: 0.5156 - val_tp: 63.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 11.0000 - val_accuracy: 0.8156 - val_precision: 0.7412 - val_recall: 0.8514 - val_auc: 0.8949\nEpoch 376/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4876 - tp: 201.0000 - fp: 59.0000 - tn: 385.0000 - fn: 67.0000 - accuracy: 0.8230 - precision: 0.7731 - recall: 0.7500 - auc: 0.8744 - val_loss: 0.4888 - val_tp: 59.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.7933 - val_precision: 0.7284 - val_recall: 0.7973 - val_auc: 0.8959\nEpoch 377/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4997 - tp: 202.0000 - fp: 52.0000 - tn: 392.0000 - fn: 66.0000 - accuracy: 0.8343 - precision: 0.7953 - recall: 0.7537 - auc: 0.8596 - val_loss: 0.4872 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8972\nEpoch 378/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4901 - tp: 201.0000 - fp: 49.0000 - tn: 395.0000 - fn: 67.0000 - accuracy: 0.8371 - precision: 0.8040 - recall: 0.7500 - auc: 0.8684 - val_loss: 0.4778 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8940\nEpoch 379/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4861 - tp: 196.0000 - fp: 47.0000 - tn: 397.0000 - fn: 72.0000 - accuracy: 0.8329 - precision: 0.8066 - recall: 0.7313 - auc: 0.8700 - val_loss: 0.4908 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8921\nEpoch 380/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4844 - tp: 206.0000 - fp: 55.0000 - tn: 389.0000 - fn: 62.0000 - accuracy: 0.8357 - precision: 0.7893 - recall: 0.7687 - auc: 0.8741 - val_loss: 0.5015 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8907\nEpoch 381/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4751 - tp: 205.0000 - fp: 47.0000 - tn: 397.0000 - fn: 63.0000 - accuracy: 0.8455 - precision: 0.8135 - recall: 0.7649 - auc: 0.8711 - val_loss: 0.4988 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8984\nEpoch 382/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5022 - tp: 199.0000 - fp: 57.0000 - tn: 387.0000 - fn: 69.0000 - accuracy: 0.8230 - precision: 0.7773 - recall: 0.7425 - auc: 0.8640 - val_loss: 0.4675 - val_tp: 52.0000 - val_fp: 10.0000 - val_tn: 95.0000 - val_fn: 22.0000 - val_accuracy: 0.8212 - val_precision: 0.8387 - val_recall: 0.7027 - val_auc: 0.8972\nEpoch 383/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4936 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8633 - val_loss: 0.4872 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8916\nEpoch 384/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4848 - tp: 198.0000 - fp: 46.0000 - tn: 398.0000 - fn: 70.0000 - accuracy: 0.8371 - precision: 0.8115 - recall: 0.7388 - auc: 0.8699 - val_loss: 0.5243 - val_tp: 63.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 11.0000 - val_accuracy: 0.7877 - val_precision: 0.7000 - val_recall: 0.8514 - val_auc: 0.8878\nEpoch 385/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4971 - tp: 203.0000 - fp: 68.0000 - tn: 376.0000 - fn: 65.0000 - accuracy: 0.8132 - precision: 0.7491 - recall: 0.7575 - auc: 0.8632 - val_loss: 0.4888 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8943\nEpoch 386/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4961 - tp: 214.0000 - fp: 77.0000 - tn: 367.0000 - fn: 54.0000 - accuracy: 0.8160 - precision: 0.7354 - recall: 0.7985 - auc: 0.8673 - val_loss: 0.4803 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8965\nEpoch 387/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5061 - tp: 196.0000 - fp: 52.0000 - tn: 392.0000 - fn: 72.0000 - accuracy: 0.8258 - precision: 0.7903 - recall: 0.7313 - auc: 0.8604 - val_loss: 0.4823 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8956\nEpoch 388/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4816 - tp: 198.0000 - fp: 52.0000 - tn: 392.0000 - fn: 70.0000 - accuracy: 0.8287 - precision: 0.7920 - recall: 0.7388 - auc: 0.8774 - val_loss: 0.5052 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8923\nEpoch 389/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4893 - tp: 209.0000 - fp: 58.0000 - tn: 386.0000 - fn: 59.0000 - accuracy: 0.8357 - precision: 0.7828 - recall: 0.7799 - auc: 0.8708 - val_loss: 0.4863 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8963\nEpoch 390/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4879 - tp: 204.0000 - fp: 60.0000 - tn: 384.0000 - fn: 64.0000 - accuracy: 0.8258 - precision: 0.7727 - recall: 0.7612 - auc: 0.8695 - val_loss: 0.4729 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8929\nEpoch 391/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4768 - tp: 200.0000 - fp: 52.0000 - tn: 392.0000 - fn: 68.0000 - accuracy: 0.8315 - precision: 0.7937 - recall: 0.7463 - auc: 0.8770 - val_loss: 0.4765 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8986\nEpoch 392/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4986 - tp: 196.0000 - fp: 57.0000 - tn: 387.0000 - fn: 72.0000 - accuracy: 0.8188 - precision: 0.7747 - recall: 0.7313 - auc: 0.8633 - val_loss: 0.4687 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8985\nEpoch 393/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.4930 - tp: 201.0000 - fp: 56.0000 - tn: 388.0000 - fn: 67.0000 - accuracy: 0.8272 - precision: 0.7821 - recall: 0.7500 - auc: 0.8651 - val_loss: 0.4790 - val_tp: 55.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 19.0000 - val_accuracy: 0.8324 - val_precision: 0.8333 - val_recall: 0.7432 - val_auc: 0.8975\nEpoch 394/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4913 - tp: 190.0000 - fp: 45.0000 - tn: 399.0000 - fn: 78.0000 - accuracy: 0.8272 - precision: 0.8085 - recall: 0.7090 - auc: 0.8743 - val_loss: 0.4990 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8963\nEpoch 395/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4938 - tp: 195.0000 - fp: 51.0000 - tn: 393.0000 - fn: 73.0000 - accuracy: 0.8258 - precision: 0.7927 - recall: 0.7276 - auc: 0.8663 - val_loss: 0.4967 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8922\nEpoch 396/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4746 - tp: 202.0000 - fp: 44.0000 - tn: 400.0000 - fn: 66.0000 - accuracy: 0.8455 - precision: 0.8211 - recall: 0.7537 - auc: 0.8716 - val_loss: 0.4907 - val_tp: 58.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 16.0000 - val_accuracy: 0.7989 - val_precision: 0.7436 - val_recall: 0.7838 - val_auc: 0.8899\nEpoch 397/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4839 - tp: 209.0000 - fp: 56.0000 - tn: 388.0000 - fn: 59.0000 - accuracy: 0.8385 - precision: 0.7887 - recall: 0.7799 - auc: 0.8717 - val_loss: 0.4907 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8967\nEpoch 398/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4757 - tp: 206.0000 - fp: 52.0000 - tn: 392.0000 - fn: 62.0000 - accuracy: 0.8399 - precision: 0.7984 - recall: 0.7687 - auc: 0.8761 - val_loss: 0.4807 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8954\nEpoch 399/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4912 - tp: 195.0000 - fp: 47.0000 - tn: 397.0000 - fn: 73.0000 - accuracy: 0.8315 - precision: 0.8058 - recall: 0.7276 - auc: 0.8632 - val_loss: 0.4925 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8985\nEpoch 400/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4774 - tp: 202.0000 - fp: 57.0000 - tn: 387.0000 - fn: 66.0000 - accuracy: 0.8272 - precision: 0.7799 - recall: 0.7537 - auc: 0.8734 - val_loss: 0.4809 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8956\nEpoch 401/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4746 - tp: 202.0000 - fp: 54.0000 - tn: 390.0000 - fn: 66.0000 - accuracy: 0.8315 - precision: 0.7891 - recall: 0.7537 - auc: 0.8787 - val_loss: 0.4868 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8923\nEpoch 402/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4816 - tp: 193.0000 - fp: 42.0000 - tn: 402.0000 - fn: 75.0000 - accuracy: 0.8357 - precision: 0.8213 - recall: 0.7201 - auc: 0.8771 - val_loss: 0.4982 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8977\nEpoch 403/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4811 - tp: 204.0000 - fp: 48.0000 - tn: 396.0000 - fn: 64.0000 - accuracy: 0.8427 - precision: 0.8095 - recall: 0.7612 - auc: 0.8736 - val_loss: 0.4906 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8956\nEpoch 404/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4887 - tp: 199.0000 - fp: 53.0000 - tn: 391.0000 - fn: 69.0000 - accuracy: 0.8287 - precision: 0.7897 - recall: 0.7425 - auc: 0.8681 - val_loss: 0.5131 - val_tp: 63.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 11.0000 - val_accuracy: 0.7989 - val_precision: 0.7159 - val_recall: 0.8514 - val_auc: 0.8938\nEpoch 405/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4803 - tp: 209.0000 - fp: 62.0000 - tn: 382.0000 - fn: 59.0000 - accuracy: 0.8301 - precision: 0.7712 - recall: 0.7799 - auc: 0.8729 - val_loss: 0.4778 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8958\nEpoch 406/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4831 - tp: 201.0000 - fp: 55.0000 - tn: 389.0000 - fn: 67.0000 - accuracy: 0.8287 - precision: 0.7852 - recall: 0.7500 - auc: 0.8716 - val_loss: 0.4894 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8931\nEpoch 407/500\n23/23 [==============================] - 1s 28ms/step - loss: 0.4763 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8808 - val_loss: 0.4938 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8941\nEpoch 408/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4788 - tp: 205.0000 - fp: 44.0000 - tn: 400.0000 - fn: 63.0000 - accuracy: 0.8497 - precision: 0.8233 - recall: 0.7649 - auc: 0.8735 - val_loss: 0.4767 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8977\nEpoch 409/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4825 - tp: 199.0000 - fp: 48.0000 - tn: 396.0000 - fn: 69.0000 - accuracy: 0.8357 - precision: 0.8057 - recall: 0.7425 - auc: 0.8730 - val_loss: 0.4910 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8956\nEpoch 410/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4839 - tp: 201.0000 - fp: 58.0000 - tn: 386.0000 - fn: 67.0000 - accuracy: 0.8244 - precision: 0.7761 - recall: 0.7500 - auc: 0.8688 - val_loss: 0.4711 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8995\nEpoch 411/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4877 - tp: 205.0000 - fp: 61.0000 - tn: 383.0000 - fn: 63.0000 - accuracy: 0.8258 - precision: 0.7707 - recall: 0.7649 - auc: 0.8671 - val_loss: 0.4807 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8983\nEpoch 412/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4976 - tp: 192.0000 - fp: 47.0000 - tn: 397.0000 - fn: 76.0000 - accuracy: 0.8272 - precision: 0.8033 - recall: 0.7164 - auc: 0.8660 - val_loss: 0.4911 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8978\nEpoch 413/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.4869 - tp: 198.0000 - fp: 39.0000 - tn: 405.0000 - fn: 70.0000 - accuracy: 0.8469 - precision: 0.8354 - recall: 0.7388 - auc: 0.8666 - val_loss: 0.4958 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8974\nEpoch 414/500\n23/23 [==============================] - 1s 25ms/step - loss: 0.4921 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8564 - val_loss: 0.4868 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8946\nEpoch 415/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4869 - tp: 203.0000 - fp: 54.0000 - tn: 390.0000 - fn: 65.0000 - accuracy: 0.8329 - precision: 0.7899 - recall: 0.7575 - auc: 0.8714 - val_loss: 0.4807 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8967\nEpoch 416/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4896 - tp: 204.0000 - fp: 53.0000 - tn: 391.0000 - fn: 64.0000 - accuracy: 0.8357 - precision: 0.7938 - recall: 0.7612 - auc: 0.8744 - val_loss: 0.4741 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8995\nEpoch 417/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4856 - tp: 199.0000 - fp: 54.0000 - tn: 390.0000 - fn: 69.0000 - accuracy: 0.8272 - precision: 0.7866 - recall: 0.7425 - auc: 0.8721 - val_loss: 0.4741 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.9006\nEpoch 418/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4755 - tp: 203.0000 - fp: 50.0000 - tn: 394.0000 - fn: 65.0000 - accuracy: 0.8385 - precision: 0.8024 - recall: 0.7575 - auc: 0.8754 - val_loss: 0.4866 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8962\nEpoch 419/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5018 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8607 - val_loss: 0.4711 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8964\nEpoch 420/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4800 - tp: 194.0000 - fp: 42.0000 - tn: 402.0000 - fn: 74.0000 - accuracy: 0.8371 - precision: 0.8220 - recall: 0.7239 - auc: 0.8737 - val_loss: 0.5106 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8936\nEpoch 421/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4786 - tp: 202.0000 - fp: 43.0000 - tn: 401.0000 - fn: 66.0000 - accuracy: 0.8469 - precision: 0.8245 - recall: 0.7537 - auc: 0.8711 - val_loss: 0.5253 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8900\nEpoch 422/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4830 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8707 - val_loss: 0.4729 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8955\nEpoch 423/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4812 - tp: 201.0000 - fp: 59.0000 - tn: 385.0000 - fn: 67.0000 - accuracy: 0.8230 - precision: 0.7731 - recall: 0.7500 - auc: 0.8741 - val_loss: 0.4751 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8975\nEpoch 424/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4739 - tp: 196.0000 - fp: 40.0000 - tn: 404.0000 - fn: 72.0000 - accuracy: 0.8427 - precision: 0.8305 - recall: 0.7313 - auc: 0.8813 - val_loss: 0.4902 - val_tp: 56.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 18.0000 - val_accuracy: 0.8045 - val_precision: 0.7671 - val_recall: 0.7568 - val_auc: 0.8933\nEpoch 425/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4664 - tp: 203.0000 - fp: 46.0000 - tn: 398.0000 - fn: 65.0000 - accuracy: 0.8441 - precision: 0.8153 - recall: 0.7575 - auc: 0.8761 - val_loss: 0.4847 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8963\nEpoch 426/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4868 - tp: 203.0000 - fp: 57.0000 - tn: 387.0000 - fn: 65.0000 - accuracy: 0.8287 - precision: 0.7808 - recall: 0.7575 - auc: 0.8700 - val_loss: 0.4767 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8990\nEpoch 427/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4757 - tp: 212.0000 - fp: 67.0000 - tn: 377.0000 - fn: 56.0000 - accuracy: 0.8272 - precision: 0.7599 - recall: 0.7910 - auc: 0.8767 - val_loss: 0.4791 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8954\nEpoch 428/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.4898 - tp: 190.0000 - fp: 46.0000 - tn: 398.0000 - fn: 78.0000 - accuracy: 0.8258 - precision: 0.8051 - recall: 0.7090 - auc: 0.8714 - val_loss: 0.4861 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8972\nEpoch 429/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4695 - tp: 207.0000 - fp: 59.0000 - tn: 385.0000 - fn: 61.0000 - accuracy: 0.8315 - precision: 0.7782 - recall: 0.7724 - auc: 0.8804 - val_loss: 0.4679 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8981\nEpoch 430/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4917 - tp: 202.0000 - fp: 53.0000 - tn: 391.0000 - fn: 66.0000 - accuracy: 0.8329 - precision: 0.7922 - recall: 0.7537 - auc: 0.8664 - val_loss: 0.4834 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8956\nEpoch 431/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4936 - tp: 198.0000 - fp: 54.0000 - tn: 390.0000 - fn: 70.0000 - accuracy: 0.8258 - precision: 0.7857 - recall: 0.7388 - auc: 0.8609 - val_loss: 0.4801 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8959\nEpoch 432/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4657 - tp: 203.0000 - fp: 48.0000 - tn: 396.0000 - fn: 65.0000 - accuracy: 0.8413 - precision: 0.8088 - recall: 0.7575 - auc: 0.8852 - val_loss: 0.5153 - val_tp: 63.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 11.0000 - val_accuracy: 0.8156 - val_precision: 0.7412 - val_recall: 0.8514 - val_auc: 0.8924\nEpoch 433/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.4892 - tp: 201.0000 - fp: 49.0000 - tn: 395.0000 - fn: 67.0000 - accuracy: 0.8371 - precision: 0.8040 - recall: 0.7500 - auc: 0.8687 - val_loss: 0.4772 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8973\nEpoch 434/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.4915 - tp: 204.0000 - fp: 61.0000 - tn: 383.0000 - fn: 64.0000 - accuracy: 0.8244 - precision: 0.7698 - recall: 0.7612 - auc: 0.8657 - val_loss: 0.4792 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8950\nEpoch 435/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4866 - tp: 208.0000 - fp: 69.0000 - tn: 375.0000 - fn: 60.0000 - accuracy: 0.8188 - precision: 0.7509 - recall: 0.7761 - auc: 0.8709 - val_loss: 0.4855 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8920\nEpoch 436/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4894 - tp: 193.0000 - fp: 42.0000 - tn: 402.0000 - fn: 75.0000 - accuracy: 0.8357 - precision: 0.8213 - recall: 0.7201 - auc: 0.8658 - val_loss: 0.5000 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8962\nEpoch 437/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4819 - tp: 199.0000 - fp: 55.0000 - tn: 389.0000 - fn: 69.0000 - accuracy: 0.8258 - precision: 0.7835 - recall: 0.7425 - auc: 0.8692 - val_loss: 0.4819 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8970\nEpoch 438/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4956 - tp: 196.0000 - fp: 46.0000 - tn: 398.0000 - fn: 72.0000 - accuracy: 0.8343 - precision: 0.8099 - recall: 0.7313 - auc: 0.8628 - val_loss: 0.4866 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8934\nEpoch 439/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4929 - tp: 190.0000 - fp: 46.0000 - tn: 398.0000 - fn: 78.0000 - accuracy: 0.8258 - precision: 0.8051 - recall: 0.7090 - auc: 0.8687 - val_loss: 0.5163 - val_tp: 61.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 13.0000 - val_accuracy: 0.7821 - val_precision: 0.7011 - val_recall: 0.8243 - val_auc: 0.8896\nEpoch 440/500\n23/23 [==============================] - 1s 24ms/step - loss: 0.4935 - tp: 196.0000 - fp: 51.0000 - tn: 393.0000 - fn: 72.0000 - accuracy: 0.8272 - precision: 0.7935 - recall: 0.7313 - auc: 0.8641 - val_loss: 0.4881 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8963\nEpoch 441/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4809 - tp: 202.0000 - fp: 61.0000 - tn: 383.0000 - fn: 66.0000 - accuracy: 0.8216 - precision: 0.7681 - recall: 0.7537 - auc: 0.8752 - val_loss: 0.5045 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8949\nEpoch 442/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4785 - tp: 210.0000 - fp: 60.0000 - tn: 384.0000 - fn: 58.0000 - accuracy: 0.8343 - precision: 0.7778 - recall: 0.7836 - auc: 0.8718 - val_loss: 0.4711 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8943\nEpoch 443/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4804 - tp: 194.0000 - fp: 47.0000 - tn: 397.0000 - fn: 74.0000 - accuracy: 0.8301 - precision: 0.8050 - recall: 0.7239 - auc: 0.8675 - val_loss: 0.4765 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8939\nEpoch 444/500\n23/23 [==============================] - 1s 26ms/step - loss: 0.4812 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8779 - val_loss: 0.4850 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8943\nEpoch 445/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4810 - tp: 198.0000 - fp: 44.0000 - tn: 400.0000 - fn: 70.0000 - accuracy: 0.8399 - precision: 0.8182 - recall: 0.7388 - auc: 0.8698 - val_loss: 0.4930 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8941\nEpoch 446/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4875 - tp: 191.0000 - fp: 40.0000 - tn: 404.0000 - fn: 77.0000 - accuracy: 0.8357 - precision: 0.8268 - recall: 0.7127 - auc: 0.8678 - val_loss: 0.4886 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8983\nEpoch 447/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4736 - tp: 209.0000 - fp: 63.0000 - tn: 381.0000 - fn: 59.0000 - accuracy: 0.8287 - precision: 0.7684 - recall: 0.7799 - auc: 0.8749 - val_loss: 0.4894 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8967\nEpoch 448/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4660 - tp: 200.0000 - fp: 42.0000 - tn: 402.0000 - fn: 68.0000 - accuracy: 0.8455 - precision: 0.8264 - recall: 0.7463 - auc: 0.8859 - val_loss: 0.4937 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8967\nEpoch 449/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4766 - tp: 199.0000 - fp: 58.0000 - tn: 386.0000 - fn: 69.0000 - accuracy: 0.8216 - precision: 0.7743 - recall: 0.7425 - auc: 0.8778 - val_loss: 0.4900 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8966\nEpoch 450/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4760 - tp: 197.0000 - fp: 40.0000 - tn: 404.0000 - fn: 71.0000 - accuracy: 0.8441 - precision: 0.8312 - recall: 0.7351 - auc: 0.8742 - val_loss: 0.4893 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8918\nEpoch 451/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4879 - tp: 195.0000 - fp: 53.0000 - tn: 391.0000 - fn: 73.0000 - accuracy: 0.8230 - precision: 0.7863 - recall: 0.7276 - auc: 0.8676 - val_loss: 0.4886 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8970\nEpoch 452/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4723 - tp: 206.0000 - fp: 51.0000 - tn: 393.0000 - fn: 62.0000 - accuracy: 0.8413 - precision: 0.8016 - recall: 0.7687 - auc: 0.8802 - val_loss: 0.4981 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8964\nEpoch 453/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.4749 - tp: 202.0000 - fp: 47.0000 - tn: 397.0000 - fn: 66.0000 - accuracy: 0.8413 - precision: 0.8112 - recall: 0.7537 - auc: 0.8759 - val_loss: 0.4908 - val_tp: 58.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 16.0000 - val_accuracy: 0.7933 - val_precision: 0.7342 - val_recall: 0.7838 - val_auc: 0.8952\nEpoch 454/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4871 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8716 - val_loss: 0.4719 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8997\nEpoch 455/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4875 - tp: 202.0000 - fp: 60.0000 - tn: 384.0000 - fn: 66.0000 - accuracy: 0.8230 - precision: 0.7710 - recall: 0.7537 - auc: 0.8788 - val_loss: 0.4594 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9028\nEpoch 456/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4957 - tp: 191.0000 - fp: 39.0000 - tn: 405.0000 - fn: 77.0000 - accuracy: 0.8371 - precision: 0.8304 - recall: 0.7127 - auc: 0.8610 - val_loss: 0.5012 - val_tp: 59.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 15.0000 - val_accuracy: 0.7989 - val_precision: 0.7375 - val_recall: 0.7973 - val_auc: 0.8929\nEpoch 457/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4961 - tp: 198.0000 - fp: 49.0000 - tn: 395.0000 - fn: 70.0000 - accuracy: 0.8329 - precision: 0.8016 - recall: 0.7388 - auc: 0.8622 - val_loss: 0.5061 - val_tp: 59.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 15.0000 - val_accuracy: 0.7989 - val_precision: 0.7375 - val_recall: 0.7973 - val_auc: 0.8926\nEpoch 458/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4869 - tp: 200.0000 - fp: 51.0000 - tn: 393.0000 - fn: 68.0000 - accuracy: 0.8329 - precision: 0.7968 - recall: 0.7463 - auc: 0.8627 - val_loss: 0.4797 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.9006\nEpoch 459/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.4862 - tp: 208.0000 - fp: 52.0000 - tn: 392.0000 - fn: 60.0000 - accuracy: 0.8427 - precision: 0.8000 - recall: 0.7761 - auc: 0.8714 - val_loss: 0.4962 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8961\nEpoch 460/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4846 - tp: 199.0000 - fp: 54.0000 - tn: 390.0000 - fn: 69.0000 - accuracy: 0.8272 - precision: 0.7866 - recall: 0.7425 - auc: 0.8753 - val_loss: 0.4956 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.9010\nEpoch 461/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4929 - tp: 201.0000 - fp: 51.0000 - tn: 393.0000 - fn: 67.0000 - accuracy: 0.8343 - precision: 0.7976 - recall: 0.7500 - auc: 0.8655 - val_loss: 0.5112 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8939\nEpoch 462/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4968 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8634 - val_loss: 0.4799 - val_tp: 54.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 20.0000 - val_accuracy: 0.8268 - val_precision: 0.8308 - val_recall: 0.7297 - val_auc: 0.9014\nEpoch 463/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4875 - tp: 198.0000 - fp: 43.0000 - tn: 401.0000 - fn: 70.0000 - accuracy: 0.8413 - precision: 0.8216 - recall: 0.7388 - auc: 0.8711 - val_loss: 0.4894 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8974\nEpoch 464/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4710 - tp: 203.0000 - fp: 50.0000 - tn: 394.0000 - fn: 65.0000 - accuracy: 0.8385 - precision: 0.8024 - recall: 0.7575 - auc: 0.8791 - val_loss: 0.4867 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8974\nEpoch 465/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4901 - tp: 196.0000 - fp: 58.0000 - tn: 386.0000 - fn: 72.0000 - accuracy: 0.8174 - precision: 0.7717 - recall: 0.7313 - auc: 0.8729 - val_loss: 0.4705 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8970\nEpoch 466/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4834 - tp: 200.0000 - fp: 45.0000 - tn: 399.0000 - fn: 68.0000 - accuracy: 0.8413 - precision: 0.8163 - recall: 0.7463 - auc: 0.8693 - val_loss: 0.4795 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8985\nEpoch 467/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4806 - tp: 199.0000 - fp: 52.0000 - tn: 392.0000 - fn: 69.0000 - accuracy: 0.8301 - precision: 0.7928 - recall: 0.7425 - auc: 0.8760 - val_loss: 0.5213 - val_tp: 63.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 11.0000 - val_accuracy: 0.7933 - val_precision: 0.7079 - val_recall: 0.8514 - val_auc: 0.8925\nEpoch 468/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.4815 - tp: 204.0000 - fp: 53.0000 - tn: 391.0000 - fn: 64.0000 - accuracy: 0.8357 - precision: 0.7938 - recall: 0.7612 - auc: 0.8797 - val_loss: 0.4818 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.9006\nEpoch 469/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4765 - tp: 196.0000 - fp: 46.0000 - tn: 398.0000 - fn: 72.0000 - accuracy: 0.8343 - precision: 0.8099 - recall: 0.7313 - auc: 0.8747 - val_loss: 0.5153 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8961\nEpoch 470/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4918 - tp: 202.0000 - fp: 62.0000 - tn: 382.0000 - fn: 66.0000 - accuracy: 0.8202 - precision: 0.7652 - recall: 0.7537 - auc: 0.8613 - val_loss: 0.4796 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8962\nEpoch 471/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.4791 - tp: 202.0000 - fp: 54.0000 - tn: 390.0000 - fn: 66.0000 - accuracy: 0.8315 - precision: 0.7891 - recall: 0.7537 - auc: 0.8673 - val_loss: 0.4873 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8958\nEpoch 472/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4977 - tp: 199.0000 - fp: 57.0000 - tn: 387.0000 - fn: 69.0000 - accuracy: 0.8230 - precision: 0.7773 - recall: 0.7425 - auc: 0.8576 - val_loss: 0.5016 - val_tp: 59.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.7933 - val_precision: 0.7284 - val_recall: 0.7973 - val_auc: 0.8944\nEpoch 473/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4707 - tp: 208.0000 - fp: 51.0000 - tn: 393.0000 - fn: 60.0000 - accuracy: 0.8441 - precision: 0.8031 - recall: 0.7761 - auc: 0.8841 - val_loss: 0.4682 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8963\nEpoch 474/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4746 - tp: 188.0000 - fp: 42.0000 - tn: 402.0000 - fn: 80.0000 - accuracy: 0.8287 - precision: 0.8174 - recall: 0.7015 - auc: 0.8847 - val_loss: 0.4857 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8972\nEpoch 475/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4735 - tp: 207.0000 - fp: 53.0000 - tn: 391.0000 - fn: 61.0000 - accuracy: 0.8399 - precision: 0.7962 - recall: 0.7724 - auc: 0.8802 - val_loss: 0.4754 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8999\nEpoch 476/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4820 - tp: 195.0000 - fp: 42.0000 - tn: 402.0000 - fn: 73.0000 - accuracy: 0.8385 - precision: 0.8228 - recall: 0.7276 - auc: 0.8713 - val_loss: 0.5231 - val_tp: 63.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 11.0000 - val_accuracy: 0.7989 - val_precision: 0.7159 - val_recall: 0.8514 - val_auc: 0.8972\nEpoch 477/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4948 - tp: 199.0000 - fp: 58.0000 - tn: 386.0000 - fn: 69.0000 - accuracy: 0.8216 - precision: 0.7743 - recall: 0.7425 - auc: 0.8654 - val_loss: 0.4970 - val_tp: 57.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 17.0000 - val_accuracy: 0.7877 - val_precision: 0.7308 - val_recall: 0.7703 - val_auc: 0.8924\nEpoch 478/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4815 - tp: 205.0000 - fp: 54.0000 - tn: 390.0000 - fn: 63.0000 - accuracy: 0.8357 - precision: 0.7915 - recall: 0.7649 - auc: 0.8765 - val_loss: 0.4762 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8929\nEpoch 479/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4764 - tp: 198.0000 - fp: 52.0000 - tn: 392.0000 - fn: 70.0000 - accuracy: 0.8287 - precision: 0.7920 - recall: 0.7388 - auc: 0.8776 - val_loss: 0.4839 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8965\nEpoch 480/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4826 - tp: 195.0000 - fp: 54.0000 - tn: 390.0000 - fn: 73.0000 - accuracy: 0.8216 - precision: 0.7831 - recall: 0.7276 - auc: 0.8715 - val_loss: 0.5012 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8952\nEpoch 481/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4667 - tp: 209.0000 - fp: 54.0000 - tn: 390.0000 - fn: 59.0000 - accuracy: 0.8413 - precision: 0.7947 - recall: 0.7799 - auc: 0.8813 - val_loss: 0.4733 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8956\nEpoch 482/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4968 - tp: 198.0000 - fp: 54.0000 - tn: 390.0000 - fn: 70.0000 - accuracy: 0.8258 - precision: 0.7857 - recall: 0.7388 - auc: 0.8671 - val_loss: 0.4770 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8969\nEpoch 483/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4871 - tp: 206.0000 - fp: 58.0000 - tn: 386.0000 - fn: 62.0000 - accuracy: 0.8315 - precision: 0.7803 - recall: 0.7687 - auc: 0.8633 - val_loss: 0.4845 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8949\nEpoch 484/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4793 - tp: 197.0000 - fp: 51.0000 - tn: 393.0000 - fn: 71.0000 - accuracy: 0.8287 - precision: 0.7944 - recall: 0.7351 - auc: 0.8759 - val_loss: 0.4906 - val_tp: 62.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 12.0000 - val_accuracy: 0.8324 - val_precision: 0.7750 - val_recall: 0.8378 - val_auc: 0.8998\nEpoch 485/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4836 - tp: 196.0000 - fp: 45.0000 - tn: 399.0000 - fn: 72.0000 - accuracy: 0.8357 - precision: 0.8133 - recall: 0.7313 - auc: 0.8670 - val_loss: 0.4950 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8922\nEpoch 486/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4856 - tp: 201.0000 - fp: 56.0000 - tn: 388.0000 - fn: 67.0000 - accuracy: 0.8272 - precision: 0.7821 - recall: 0.7500 - auc: 0.8609 - val_loss: 0.4804 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8979\nEpoch 487/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4865 - tp: 197.0000 - fp: 65.0000 - tn: 379.0000 - fn: 71.0000 - accuracy: 0.8090 - precision: 0.7519 - recall: 0.7351 - auc: 0.8688 - val_loss: 0.4740 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8974\nEpoch 488/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.4889 - tp: 194.0000 - fp: 41.0000 - tn: 403.0000 - fn: 74.0000 - accuracy: 0.8385 - precision: 0.8255 - recall: 0.7239 - auc: 0.8644 - val_loss: 0.4913 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8965\nEpoch 489/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4737 - tp: 201.0000 - fp: 48.0000 - tn: 396.0000 - fn: 67.0000 - accuracy: 0.8385 - precision: 0.8072 - recall: 0.7500 - auc: 0.8778 - val_loss: 0.5018 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8903\nEpoch 490/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4845 - tp: 200.0000 - fp: 53.0000 - tn: 391.0000 - fn: 68.0000 - accuracy: 0.8301 - precision: 0.7905 - recall: 0.7463 - auc: 0.8699 - val_loss: 0.4888 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8957\nEpoch 491/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4739 - tp: 197.0000 - fp: 44.0000 - tn: 400.0000 - fn: 71.0000 - accuracy: 0.8385 - precision: 0.8174 - recall: 0.7351 - auc: 0.8773 - val_loss: 0.5027 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8891\nEpoch 492/500\n23/23 [==============================] - 1s 22ms/step - loss: 0.4825 - tp: 198.0000 - fp: 51.0000 - tn: 393.0000 - fn: 70.0000 - accuracy: 0.8301 - precision: 0.7952 - recall: 0.7388 - auc: 0.8764 - val_loss: 0.4738 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8976\nEpoch 493/500\n23/23 [==============================] - 0s 19ms/step - loss: 0.4841 - tp: 202.0000 - fp: 56.0000 - tn: 388.0000 - fn: 66.0000 - accuracy: 0.8287 - precision: 0.7829 - recall: 0.7537 - auc: 0.8748 - val_loss: 0.4765 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8981\nEpoch 494/500\n23/23 [==============================] - 1s 23ms/step - loss: 0.4832 - tp: 202.0000 - fp: 52.0000 - tn: 392.0000 - fn: 66.0000 - accuracy: 0.8343 - precision: 0.7953 - recall: 0.7537 - auc: 0.8704 - val_loss: 0.4848 - val_tp: 59.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 15.0000 - val_accuracy: 0.7989 - val_precision: 0.7375 - val_recall: 0.7973 - val_auc: 0.8950\nEpoch 495/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4667 - tp: 206.0000 - fp: 54.0000 - tn: 390.0000 - fn: 62.0000 - accuracy: 0.8371 - precision: 0.7923 - recall: 0.7687 - auc: 0.8836 - val_loss: 0.4676 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8997\nEpoch 496/500\n23/23 [==============================] - 0s 20ms/step - loss: 0.4935 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8635 - val_loss: 0.4702 - val_tp: 54.0000 - val_fp: 10.0000 - val_tn: 95.0000 - val_fn: 20.0000 - val_accuracy: 0.8324 - val_precision: 0.8438 - val_recall: 0.7297 - val_auc: 0.8992\nEpoch 497/500\n23/23 [==============================] - 0s 22ms/step - loss: 0.4883 - tp: 192.0000 - fp: 48.0000 - tn: 396.0000 - fn: 76.0000 - accuracy: 0.8258 - precision: 0.8000 - recall: 0.7164 - auc: 0.8651 - val_loss: 0.4823 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.9005\nEpoch 498/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4855 - tp: 203.0000 - fp: 54.0000 - tn: 390.0000 - fn: 65.0000 - accuracy: 0.8329 - precision: 0.7899 - recall: 0.7575 - auc: 0.8723 - val_loss: 0.4868 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8972\nEpoch 499/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.4702 - tp: 209.0000 - fp: 61.0000 - tn: 383.0000 - fn: 59.0000 - accuracy: 0.8315 - precision: 0.7741 - recall: 0.7799 - auc: 0.8803 - val_loss: 0.4868 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8949\nEpoch 500/500\n23/23 [==============================] - 0s 21ms/step - loss: 0.4818 - tp: 195.0000 - fp: 35.0000 - tn: 409.0000 - fn: 73.0000 - accuracy: 0.8483 - precision: 0.8478 - recall: 0.7276 - auc: 0.8681 - val_loss: 0.5088 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8929\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "# Results of the DL model"
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "def plot_model_history(history):\n",
                "    # list all data in history\n",
                "    print(history.history.keys())\n",
                "    # summarize history for accuracy\n",
                "    plt.plot(history.history[\"accuracy\"])\n",
                "    plt.plot(history.history[\"val_accuracy\"])\n",
                "    plt.title(\"model accuracy\")\n",
                "    plt.ylabel(\"accuracy\")\n",
                "    plt.xlabel(\"epoch\")\n",
                "    plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
                "    plt.show()"
            ],
            "execution_count": 14,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "plot_model_history(history)"
            ],
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "dict_keys(['loss', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', 'recall', 'auc', 'val_loss', 'val_tp', 'val_fp', 'val_tn', 'val_fn', 'val_accuracy', 'val_precision', 'val_recall', 'val_auc'])\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<Figure size 432x288 with 1 Axes>",
                        "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 392.14375 277.314375 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \nL 384.94375 22.318125 \nL 50.14375 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"macce083b29\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#macce083b29\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(62.180682 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.356649\" xlink:href=\"#macce083b29\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(116.812899 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.351365\" xlink:href=\"#macce083b29\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(177.807615 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.346082\" xlink:href=\"#macce083b29\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(238.802332 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.340799\" xlink:href=\"#macce083b29\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(299.797049 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"370.335515\" xlink:href=\"#macce083b29\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(360.791765 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(202.315625 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7fd52b6cad\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7fd52b6cad\" y=\"237.93492\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.55 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 241.734139)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7fd52b6cad\" y=\"203.774001\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(20.878125 207.57322)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7fd52b6cad\" y=\"169.613082\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.65 -->\n      <g transform=\"translate(20.878125 173.412301)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7fd52b6cad\" y=\"135.452163\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.70 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 139.251382)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7fd52b6cad\" y=\"101.291244\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.75 -->\n      <g transform=\"translate(20.878125 105.090463)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7fd52b6cad\" y=\"67.130325\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(20.878125 70.929544)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7fd52b6cad\" y=\"32.969406\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.85 -->\n      <g transform=\"translate(20.878125 36.768625)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- accuracy -->\n     <defs>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(14.798438 153.5975)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p9004750f1a)\" d=\"M 65.361932 226.036184 \nL 65.971879 229.874489 \nL 66.581826 210.682966 \nL 67.191773 179.976488 \nL 67.80172 173.259476 \nL 69.021615 137.755138 \nL 69.631562 137.755138 \nL 70.241509 119.523211 \nL 70.851456 119.523211 \nL 71.461403 98.412495 \nL 72.071351 114.72531 \nL 72.681298 109.92745 \nL 73.291245 85.938026 \nL 73.901192 98.412495 \nL 75.121086 82.099721 \nL 75.731034 97.452939 \nL 76.340981 75.382668 \nL 76.950928 79.220972 \nL 77.560875 80.180569 \nL 78.170822 87.857178 \nL 78.780769 76.342264 \nL 79.390717 84.018873 \nL 80.000664 67.706058 \nL 80.610611 69.625211 \nL 81.220558 75.382668 \nL 81.830505 60.989046 \nL 82.440452 64.82735 \nL 83.0504 79.220972 \nL 83.660347 70.584807 \nL 84.270294 69.625211 \nL 84.880241 69.625211 \nL 86.100135 63.867754 \nL 86.710083 65.786906 \nL 87.32003 71.544363 \nL 87.929977 62.908198 \nL 88.539924 66.746503 \nL 89.149871 60.989046 \nL 89.759819 60.029449 \nL 90.369766 55.231589 \nL 90.979713 62.908198 \nL 91.58966 63.867754 \nL 92.199607 63.867754 \nL 92.809554 60.989046 \nL 94.029449 53.312437 \nL 94.639396 51.393284 \nL 95.249343 60.029449 \nL 95.85929 51.393284 \nL 96.469237 58.110297 \nL 97.079185 57.150741 \nL 97.689132 60.989046 \nL 98.299079 60.989046 \nL 98.909026 59.069894 \nL 99.518973 70.584807 \nL 100.12892 42.757079 \nL 100.738868 56.191145 \nL 101.348815 63.867754 \nL 101.958762 64.82735 \nL 102.568709 57.150741 \nL 103.178656 65.786906 \nL 103.788603 62.908198 \nL 104.398551 58.110297 \nL 105.008498 59.069894 \nL 105.618445 43.716675 \nL 106.228392 60.029449 \nL 106.838339 58.110297 \nL 107.448286 68.665655 \nL 108.058234 57.150741 \nL 108.668181 69.625211 \nL 109.278128 59.069894 \nL 109.888075 53.312437 \nL 110.498022 56.191145 \nL 111.107969 63.867754 \nL 111.717917 55.231589 \nL 112.327864 54.271992 \nL 112.937811 51.393284 \nL 113.547758 58.110297 \nL 114.157705 54.271992 \nL 114.767652 56.191145 \nL 115.3776 49.474132 \nL 115.987547 57.150741 \nL 116.597494 53.312437 \nL 117.207441 51.393284 \nL 117.817388 59.069894 \nL 119.037283 64.82735 \nL 119.64723 55.231589 \nL 120.257177 57.150741 \nL 120.867124 62.908198 \nL 121.477071 51.393284 \nL 122.087018 56.191145 \nL 122.696966 45.635827 \nL 123.306913 48.514536 \nL 123.91686 57.150741 \nL 124.526807 48.514536 \nL 125.136754 56.191145 \nL 125.746701 54.271992 \nL 126.356649 58.110297 \nL 126.966596 51.393284 \nL 127.576543 58.110297 \nL 128.18649 52.35284 \nL 128.796437 55.231589 \nL 129.406384 53.312437 \nL 130.016332 55.231589 \nL 130.626279 53.312437 \nL 131.236226 50.433688 \nL 131.846173 52.35284 \nL 132.45612 48.514536 \nL 133.066067 55.231589 \nL 133.676015 60.029449 \nL 134.285962 52.35284 \nL 134.895909 39.878371 \nL 135.505856 52.35284 \nL 136.72575 56.191145 \nL 137.335698 55.231589 \nL 137.945645 52.35284 \nL 138.555592 58.110297 \nL 139.165539 49.474132 \nL 139.775486 52.35284 \nL 140.385433 42.757079 \nL 140.995381 48.514536 \nL 141.605328 50.433688 \nL 142.825222 52.35284 \nL 143.435169 56.191145 \nL 144.045116 56.191145 \nL 144.655064 43.716675 \nL 145.265011 59.069894 \nL 145.874958 45.635827 \nL 146.484905 51.393284 \nL 147.094852 48.514536 \nL 147.704799 48.514536 \nL 148.314747 57.150741 \nL 148.924694 47.55498 \nL 149.534641 39.878371 \nL 150.144588 41.797523 \nL 150.754535 64.82735 \nL 151.364482 44.676231 \nL 152.584377 57.150741 \nL 153.194324 57.150741 \nL 153.804271 58.110297 \nL 154.414218 50.433688 \nL 155.024165 54.271992 \nL 155.634113 64.82735 \nL 156.854007 41.797523 \nL 157.463954 43.716675 \nL 158.073901 44.676231 \nL 158.683848 52.35284 \nL 159.293796 52.35284 \nL 159.903743 51.393284 \nL 160.51369 51.393284 \nL 161.123637 40.837926 \nL 161.733584 54.271992 \nL 162.343531 44.676231 \nL 162.953479 55.231589 \nL 163.563426 50.433688 \nL 164.173373 50.433688 \nL 164.78332 55.231589 \nL 165.393267 52.35284 \nL 166.003214 56.191145 \nL 166.613162 58.110297 \nL 167.223109 46.595383 \nL 167.833056 52.35284 \nL 168.443003 47.55498 \nL 169.05295 43.716675 \nL 169.662897 46.595383 \nL 170.272845 47.55498 \nL 171.492739 57.150741 \nL 172.102686 44.676231 \nL 173.32258 52.35284 \nL 173.932528 42.757079 \nL 174.542475 41.797523 \nL 175.152422 58.110297 \nL 175.762369 55.231589 \nL 176.982263 47.55498 \nL 177.592211 55.231589 \nL 178.202158 52.35284 \nL 178.812105 61.948602 \nL 180.031999 47.55498 \nL 181.251894 61.948602 \nL 181.861841 49.474132 \nL 182.471788 50.433688 \nL 183.081735 50.433688 \nL 183.691682 47.55498 \nL 184.301629 49.474132 \nL 184.911577 58.110297 \nL 185.521524 42.757079 \nL 186.131471 45.635827 \nL 186.741418 46.595383 \nL 187.351365 51.393284 \nL 187.961312 46.595383 \nL 188.57126 50.433688 \nL 189.181207 43.716675 \nL 189.791154 43.716675 \nL 190.401101 63.867754 \nL 191.011048 48.514536 \nL 191.620995 36.040066 \nL 192.230943 55.231589 \nL 192.84089 46.595383 \nL 193.450837 50.433688 \nL 194.060784 51.393284 \nL 194.670731 40.837926 \nL 195.280678 44.676231 \nL 195.890626 52.35284 \nL 196.500573 46.595383 \nL 197.11052 47.55498 \nL 197.720467 50.433688 \nL 198.330414 44.676231 \nL 198.940361 45.635827 \nL 199.550309 52.35284 \nL 200.160256 42.757079 \nL 200.770203 41.797523 \nL 201.38015 45.635827 \nL 201.990097 47.55498 \nL 202.600044 45.635827 \nL 203.209992 46.595383 \nL 203.819939 48.514536 \nL 204.429886 46.595383 \nL 205.039833 45.635827 \nL 205.64978 42.757079 \nL 206.259727 44.676231 \nL 206.869675 42.757079 \nL 207.479622 49.474132 \nL 208.089569 54.271992 \nL 208.699516 55.231589 \nL 209.309463 47.55498 \nL 209.91941 42.757079 \nL 210.529358 44.676231 \nL 211.139305 41.797523 \nL 211.749252 46.595383 \nL 212.359199 41.797523 \nL 212.969146 48.514536 \nL 213.579093 42.757079 \nL 214.189041 42.757079 \nL 214.798988 52.35284 \nL 215.408935 42.757079 \nL 216.018882 50.433688 \nL 216.628829 55.231589 \nL 217.238776 32.201761 \nL 217.848724 39.878371 \nL 218.458671 62.908198 \nL 219.068618 43.716675 \nL 219.678565 49.474132 \nL 220.288512 46.595383 \nL 220.898459 46.595383 \nL 221.508407 53.312437 \nL 222.118354 51.393284 \nL 222.728301 51.393284 \nL 223.338248 44.676231 \nL 223.948195 42.757079 \nL 224.558142 52.35284 \nL 225.16809 41.797523 \nL 225.778037 46.595383 \nL 226.387984 45.635827 \nL 226.997931 47.55498 \nL 227.607878 41.797523 \nL 228.217825 54.271992 \nL 228.827773 44.676231 \nL 229.43772 40.837926 \nL 230.047667 46.595383 \nL 230.657614 45.635827 \nL 231.877508 49.474132 \nL 232.487456 48.514536 \nL 233.097403 42.757079 \nL 233.70735 50.433688 \nL 234.927244 38.918774 \nL 235.537191 65.786906 \nL 236.147139 43.716675 \nL 236.757086 43.716675 \nL 237.367033 41.797523 \nL 237.97698 47.55498 \nL 238.586927 48.514536 \nL 239.196874 41.797523 \nL 239.806822 52.35284 \nL 240.416769 36.999622 \nL 241.026716 46.595383 \nL 241.636663 44.676231 \nL 242.24661 44.676231 \nL 242.856557 46.595383 \nL 243.466505 52.35284 \nL 244.076452 49.474132 \nL 244.686399 47.55498 \nL 245.296346 37.959218 \nL 245.906293 49.474132 \nL 246.51624 57.150741 \nL 247.126188 39.878371 \nL 247.736135 50.433688 \nL 248.346082 46.595383 \nL 248.956029 49.474132 \nL 249.565976 46.595383 \nL 250.175923 45.635827 \nL 250.785871 41.797523 \nL 251.395818 49.474132 \nL 252.005765 44.676231 \nL 252.615712 44.676231 \nL 253.225659 38.918774 \nL 253.835606 39.878371 \nL 254.445554 46.595383 \nL 255.055501 36.999622 \nL 255.665448 43.716675 \nL 256.275395 39.878371 \nL 256.885342 46.595383 \nL 257.495289 43.716675 \nL 258.715184 55.231589 \nL 259.325131 46.595383 \nL 259.935078 49.474132 \nL 260.545025 49.474132 \nL 261.154972 47.55498 \nL 261.76492 46.595383 \nL 262.374867 49.474132 \nL 262.984814 47.55498 \nL 263.594761 50.433688 \nL 264.204708 44.676231 \nL 264.814655 43.716675 \nL 265.424603 49.474132 \nL 266.03455 53.312437 \nL 266.644497 50.433688 \nL 267.254444 52.35284 \nL 267.864391 45.635827 \nL 268.474338 50.433688 \nL 269.084286 46.595383 \nL 269.694233 40.837926 \nL 270.30418 41.797523 \nL 270.914127 45.635827 \nL 271.524074 41.797523 \nL 272.743969 45.635827 \nL 273.353916 44.676231 \nL 273.963863 45.635827 \nL 274.57381 44.676231 \nL 275.183757 45.635827 \nL 275.793704 36.040066 \nL 276.403652 36.040066 \nL 277.013599 58.110297 \nL 278.233493 40.837926 \nL 279.453387 44.676231 \nL 280.063335 47.55498 \nL 280.673282 48.514536 \nL 281.283229 51.393284 \nL 281.893176 33.161317 \nL 282.503123 41.797523 \nL 283.11307 46.595383 \nL 283.723018 49.474132 \nL 284.332965 60.029449 \nL 284.942912 44.676231 \nL 285.552859 51.393284 \nL 286.162806 48.514536 \nL 286.772753 48.514536 \nL 287.382701 42.757079 \nL 287.992648 39.878371 \nL 288.602595 55.231589 \nL 289.212542 40.837926 \nL 289.822489 42.757079 \nL 290.432436 50.433688 \nL 291.042384 42.757079 \nL 291.652331 43.716675 \nL 292.262278 49.474132 \nL 292.872225 43.716675 \nL 293.482172 44.676231 \nL 294.092119 51.393284 \nL 294.702067 43.716675 \nL 295.312014 41.797523 \nL 295.921961 44.676231 \nL 296.531908 42.757079 \nL 297.141855 36.040066 \nL 297.751802 51.393284 \nL 298.36175 50.433688 \nL 298.971697 41.797523 \nL 299.581644 58.110297 \nL 300.191591 56.191145 \nL 300.801538 49.474132 \nL 301.411485 47.55498 \nL 302.021433 42.757079 \nL 302.63138 49.474132 \nL 303.241327 45.635827 \nL 303.851274 54.271992 \nL 304.461221 48.514536 \nL 305.071168 48.514536 \nL 305.681116 49.474132 \nL 306.291063 36.040066 \nL 306.90101 40.837926 \nL 307.510957 39.878371 \nL 308.120904 45.635827 \nL 308.730851 48.514536 \nL 309.950746 42.757079 \nL 310.560693 37.959218 \nL 311.17064 47.55498 \nL 311.780587 46.595383 \nL 312.390534 47.55498 \nL 313.000482 45.635827 \nL 313.610429 33.161317 \nL 314.220376 42.757079 \nL 314.830323 50.433688 \nL 316.050217 48.514536 \nL 316.660165 35.080469 \nL 317.270112 44.676231 \nL 317.880059 44.676231 \nL 318.490006 42.757079 \nL 319.099953 48.514536 \nL 319.7099 40.837926 \nL 320.319848 50.433688 \nL 320.929795 41.797523 \nL 321.539742 35.080469 \nL 322.149689 45.635827 \nL 322.759636 51.393284 \nL 323.369583 37.959218 \nL 323.979531 36.999622 \nL 324.589478 47.55498 \nL 325.809372 49.474132 \nL 326.419319 45.635827 \nL 327.029266 44.676231 \nL 327.639214 49.474132 \nL 328.249161 38.918774 \nL 328.859108 41.797523 \nL 329.469055 50.433688 \nL 330.079002 54.271992 \nL 330.688949 42.757079 \nL 331.298897 49.474132 \nL 331.908844 43.716675 \nL 332.518791 49.474132 \nL 333.128738 48.514536 \nL 333.738685 52.35284 \nL 334.348632 43.716675 \nL 334.95858 46.595383 \nL 335.568527 43.716675 \nL 336.178474 39.878371 \nL 336.788421 42.757079 \nL 337.398368 47.55498 \nL 338.008315 36.040066 \nL 338.618263 52.35284 \nL 339.22821 36.999622 \nL 339.838157 51.393284 \nL 340.448104 38.918774 \nL 341.058051 38.918774 \nL 342.277946 51.393284 \nL 342.887893 41.797523 \nL 343.49784 44.676231 \nL 344.107787 44.676231 \nL 344.717734 37.959218 \nL 345.327681 48.514536 \nL 345.937629 43.716675 \nL 346.547576 44.676231 \nL 347.157523 38.918774 \nL 347.76747 40.837926 \nL 348.377417 55.231589 \nL 348.987365 38.918774 \nL 349.597312 46.595383 \nL 350.207259 42.757079 \nL 350.817206 43.716675 \nL 351.427153 53.312437 \nL 352.0371 45.635827 \nL 352.647048 51.393284 \nL 353.256995 36.999622 \nL 353.866942 47.55498 \nL 354.476889 39.878371 \nL 355.086836 40.837926 \nL 355.696783 52.35284 \nL 356.306731 42.757079 \nL 357.526625 52.35284 \nL 358.136572 38.918774 \nL 358.746519 49.474132 \nL 359.356466 45.635827 \nL 359.966414 47.55498 \nL 360.576361 42.757079 \nL 361.186308 48.514536 \nL 361.796255 60.989046 \nL 362.406202 40.837926 \nL 363.016149 40.837926 \nL 363.626097 46.595383 \nL 364.236044 40.837926 \nL 364.845991 46.595383 \nL 365.455938 47.55498 \nL 366.065885 43.716675 \nL 366.675832 41.797523 \nL 367.28578 49.474132 \nL 367.895727 49.474132 \nL 368.505674 44.676231 \nL 369.115621 45.635827 \nL 369.725568 34.120914 \nL 369.725568 34.120914 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p9004750f1a)\" d=\"M 65.361932 128.963492 \nL 65.971879 144.230948 \nL 66.581826 151.864676 \nL 67.191773 125.146649 \nL 67.80172 121.329764 \nL 69.021615 98.428581 \nL 69.631562 106.062309 \nL 70.241509 86.978009 \nL 70.851456 83.161166 \nL 72.071351 90.794853 \nL 72.681298 83.161166 \nL 73.901192 75.527438 \nL 74.511139 83.161166 \nL 75.121086 94.611737 \nL 75.731034 75.527438 \nL 76.340981 83.161166 \nL 76.950928 79.344281 \nL 77.560875 94.611737 \nL 78.780769 86.978009 \nL 79.390717 67.89371 \nL 81.220558 79.344281 \nL 81.830505 75.527438 \nL 82.440452 79.344281 \nL 83.0504 67.89371 \nL 83.660347 79.344281 \nL 84.270294 67.89371 \nL 85.490188 75.527438 \nL 86.100135 64.076826 \nL 88.539924 79.344281 \nL 89.759819 71.710554 \nL 90.369766 60.259982 \nL 90.979713 67.89371 \nL 91.58966 64.076826 \nL 92.199607 67.89371 \nL 92.809554 64.076826 \nL 93.419502 67.89371 \nL 94.029449 64.076826 \nL 95.249343 64.076826 \nL 95.85929 75.527438 \nL 96.469237 60.259982 \nL 97.079185 64.076826 \nL 97.689132 64.076826 \nL 98.299079 67.89371 \nL 98.909026 64.076826 \nL 99.518973 64.076826 \nL 100.12892 71.710554 \nL 100.738868 56.443098 \nL 101.348815 64.076826 \nL 101.958762 64.076826 \nL 102.568709 71.710554 \nL 103.178656 71.710554 \nL 103.788603 60.259982 \nL 104.398551 60.259982 \nL 105.008498 64.076826 \nL 105.618445 79.344281 \nL 106.228392 64.076826 \nL 106.838339 71.710554 \nL 107.448286 75.527438 \nL 108.668181 60.259982 \nL 109.278128 56.443098 \nL 110.498022 64.076826 \nL 111.107969 64.076826 \nL 111.717917 60.259982 \nL 112.327864 64.076826 \nL 112.937811 71.710554 \nL 113.547758 60.259982 \nL 114.157705 64.076826 \nL 114.767652 56.443098 \nL 115.3776 64.076826 \nL 115.987547 60.259982 \nL 116.597494 79.344281 \nL 117.207441 60.259982 \nL 117.817388 64.076826 \nL 119.037283 64.076826 \nL 120.257177 56.443098 \nL 120.867124 64.076826 \nL 121.477071 75.527438 \nL 122.696966 60.259982 \nL 123.306913 56.443098 \nL 123.91686 60.259982 \nL 124.526807 56.443098 \nL 125.136754 64.076826 \nL 125.746701 56.443098 \nL 126.356649 52.626254 \nL 126.966596 64.076826 \nL 127.576543 52.626254 \nL 128.18649 56.443098 \nL 128.796437 64.076826 \nL 129.406384 64.076826 \nL 130.016332 56.443098 \nL 130.626279 64.076826 \nL 131.236226 60.259982 \nL 131.846173 75.527438 \nL 132.45612 64.076826 \nL 133.066067 56.443098 \nL 134.285962 64.076826 \nL 134.895909 56.443098 \nL 135.505856 64.076826 \nL 136.115803 60.259982 \nL 136.72575 52.626254 \nL 137.335698 56.443098 \nL 137.945645 67.89371 \nL 139.165539 60.259982 \nL 139.775486 60.259982 \nL 140.995381 67.89371 \nL 141.605328 56.443098 \nL 142.215275 71.710554 \nL 142.825222 64.076826 \nL 143.435169 64.076826 \nL 144.045116 56.443098 \nL 144.655064 52.626254 \nL 145.265011 75.527438 \nL 145.874958 64.076826 \nL 146.484905 64.076826 \nL 147.094852 52.626254 \nL 147.704799 60.259982 \nL 148.314747 52.626254 \nL 148.924694 67.89371 \nL 149.534641 64.076826 \nL 150.144588 67.89371 \nL 150.754535 56.443098 \nL 151.364482 52.626254 \nL 151.97443 71.710554 \nL 152.584377 64.076826 \nL 153.194324 64.076826 \nL 153.804271 52.626254 \nL 154.414218 60.259982 \nL 155.024165 48.80937 \nL 156.24406 64.076826 \nL 156.854007 64.076826 \nL 157.463954 67.89371 \nL 158.073901 52.626254 \nL 158.683848 64.076826 \nL 159.293796 71.710554 \nL 159.903743 64.076826 \nL 160.51369 60.259982 \nL 161.123637 71.710554 \nL 161.733584 56.443098 \nL 162.343531 75.527438 \nL 162.953479 64.076826 \nL 163.563426 60.259982 \nL 164.173373 67.89371 \nL 164.78332 60.259982 \nL 165.393267 67.89371 \nL 166.003214 79.344281 \nL 166.613162 67.89371 \nL 167.223109 52.626254 \nL 167.833056 64.076826 \nL 168.443003 60.259982 \nL 169.05295 60.259982 \nL 169.662897 64.076826 \nL 170.882792 56.443098 \nL 171.492739 64.076826 \nL 172.102686 56.443098 \nL 172.712633 83.161166 \nL 173.32258 56.443098 \nL 174.542475 64.076826 \nL 175.152422 56.443098 \nL 176.372316 56.443098 \nL 176.982263 64.076826 \nL 177.592211 64.076826 \nL 178.202158 67.89371 \nL 178.812105 44.992526 \nL 179.422052 60.259982 \nL 180.031999 60.259982 \nL 180.641946 67.89371 \nL 181.861841 52.626254 \nL 182.471788 67.89371 \nL 183.081735 52.626254 \nL 184.301629 60.259982 \nL 184.911577 52.626254 \nL 185.521524 64.076826 \nL 186.131471 44.992526 \nL 186.741418 64.076826 \nL 187.351365 48.80937 \nL 187.961312 60.259982 \nL 188.57126 64.076826 \nL 189.181207 56.443098 \nL 189.791154 71.710554 \nL 190.401101 56.443098 \nL 191.011048 64.076826 \nL 191.620995 75.527438 \nL 192.230943 60.259982 \nL 192.84089 67.89371 \nL 193.450837 67.89371 \nL 194.060784 60.259982 \nL 194.670731 64.076826 \nL 195.280678 64.076826 \nL 195.890626 60.259982 \nL 196.500573 60.259982 \nL 197.11052 52.626254 \nL 197.720467 67.89371 \nL 198.330414 60.259982 \nL 198.940361 56.443098 \nL 199.550309 60.259982 \nL 200.160256 60.259982 \nL 200.770203 64.076826 \nL 201.38015 56.443098 \nL 201.990097 64.076826 \nL 202.600044 52.626254 \nL 203.209992 64.076826 \nL 203.819939 67.89371 \nL 205.039833 52.626254 \nL 205.64978 56.443098 \nL 206.869675 48.80937 \nL 207.479622 60.259982 \nL 208.089569 56.443098 \nL 208.699516 56.443098 \nL 209.309463 60.259982 \nL 209.91941 60.259982 \nL 210.529358 64.076826 \nL 211.749252 56.443098 \nL 212.359199 60.259982 \nL 212.969146 48.80937 \nL 213.579093 64.076826 \nL 214.189041 60.259982 \nL 214.798988 52.626254 \nL 215.408935 60.259982 \nL 216.018882 56.443098 \nL 216.628829 64.076826 \nL 217.238776 56.443098 \nL 217.848724 56.443098 \nL 218.458671 60.259982 \nL 219.068618 60.259982 \nL 219.678565 64.076826 \nL 220.288512 60.259982 \nL 220.898459 64.076826 \nL 221.508407 56.443098 \nL 222.118354 71.710554 \nL 222.728301 64.076826 \nL 223.338248 64.076826 \nL 223.948195 67.89371 \nL 224.558142 56.443098 \nL 225.16809 64.076826 \nL 225.778037 56.443098 \nL 226.997931 64.076826 \nL 227.607878 64.076826 \nL 228.217825 56.443098 \nL 228.827773 60.259982 \nL 229.43772 67.89371 \nL 230.047667 71.710554 \nL 230.657614 60.259982 \nL 231.267561 64.076826 \nL 231.877508 64.076826 \nL 232.487456 60.259982 \nL 233.097403 60.259982 \nL 233.70735 48.80937 \nL 234.317297 60.259982 \nL 234.927244 67.89371 \nL 235.537191 64.076826 \nL 236.147139 56.443098 \nL 236.757086 60.259982 \nL 237.367033 56.443098 \nL 238.586927 56.443098 \nL 239.196874 64.076826 \nL 239.806822 60.259982 \nL 241.026716 60.259982 \nL 241.636663 71.710554 \nL 242.24661 56.443098 \nL 242.856557 56.443098 \nL 243.466505 52.626254 \nL 244.076452 60.259982 \nL 244.686399 56.443098 \nL 245.296346 41.175642 \nL 245.906293 60.259982 \nL 246.51624 60.259982 \nL 247.126188 52.626254 \nL 247.736135 60.259982 \nL 248.346082 60.259982 \nL 248.956029 48.80937 \nL 249.565976 52.626254 \nL 250.175923 44.992526 \nL 250.785871 44.992526 \nL 251.395818 48.80937 \nL 252.005765 60.259982 \nL 252.615712 64.076826 \nL 253.225659 60.259982 \nL 253.835606 64.076826 \nL 254.445554 48.80937 \nL 255.055501 60.259982 \nL 255.665448 60.259982 \nL 256.275395 44.992526 \nL 256.885342 60.259982 \nL 258.105237 52.626254 \nL 258.715184 67.89371 \nL 259.325131 67.89371 \nL 259.935078 52.626254 \nL 260.545025 48.80937 \nL 261.76492 56.443098 \nL 262.374867 52.626254 \nL 262.984814 64.076826 \nL 263.594761 41.175642 \nL 264.204708 67.89371 \nL 264.814655 48.80937 \nL 265.424603 41.175642 \nL 266.03455 56.443098 \nL 267.254444 64.076826 \nL 267.864391 60.259982 \nL 268.474338 60.259982 \nL 269.694233 52.626254 \nL 270.30418 67.89371 \nL 270.914127 56.443098 \nL 271.524074 56.443098 \nL 272.134021 64.076826 \nL 273.353916 56.443098 \nL 274.57381 56.443098 \nL 275.183757 64.076826 \nL 275.793704 56.443098 \nL 276.403652 64.076826 \nL 277.013599 56.443098 \nL 277.623546 56.443098 \nL 278.233493 60.259982 \nL 278.84344 41.175642 \nL 279.453387 52.626254 \nL 280.063335 60.259982 \nL 280.673282 60.259982 \nL 281.283229 52.626254 \nL 281.893176 64.076826 \nL 282.503123 60.259982 \nL 283.11307 52.626254 \nL 283.723018 52.626254 \nL 284.332965 56.443098 \nL 284.942912 52.626254 \nL 285.552859 60.259982 \nL 286.162806 52.626254 \nL 286.772753 75.527438 \nL 287.382701 56.443098 \nL 287.992648 60.259982 \nL 288.602595 48.80937 \nL 289.212542 56.443098 \nL 289.822489 60.259982 \nL 290.432436 41.175642 \nL 291.042384 60.259982 \nL 291.652331 44.992526 \nL 292.262278 48.80937 \nL 292.872225 48.80937 \nL 293.482172 56.443098 \nL 294.092119 71.710554 \nL 294.702067 56.443098 \nL 295.312014 48.80937 \nL 295.921961 52.626254 \nL 296.531908 67.89371 \nL 297.141855 56.443098 \nL 297.751802 52.626254 \nL 298.36175 56.443098 \nL 298.971697 75.527438 \nL 299.581644 52.626254 \nL 300.191591 52.626254 \nL 300.801538 56.443098 \nL 301.411485 64.076826 \nL 302.63138 56.443098 \nL 303.241327 60.259982 \nL 304.461221 44.992526 \nL 305.071168 52.626254 \nL 305.681116 67.89371 \nL 306.291063 67.89371 \nL 306.90101 56.443098 \nL 307.510957 52.626254 \nL 308.120904 60.259982 \nL 308.730851 56.443098 \nL 309.340799 60.259982 \nL 309.950746 56.443098 \nL 310.560693 60.259982 \nL 311.17064 67.89371 \nL 311.780587 48.80937 \nL 312.390534 64.076826 \nL 313.000482 60.259982 \nL 313.610429 52.626254 \nL 314.220376 60.259982 \nL 314.830323 48.80937 \nL 315.44027 48.80937 \nL 316.050217 52.626254 \nL 316.660165 48.80937 \nL 317.880059 56.443098 \nL 318.490006 52.626254 \nL 319.099953 60.259982 \nL 319.7099 56.443098 \nL 320.319848 48.80937 \nL 320.929795 67.89371 \nL 322.149689 52.626254 \nL 322.759636 56.443098 \nL 323.369583 64.076826 \nL 323.979531 56.443098 \nL 324.589478 44.992526 \nL 325.199425 52.626254 \nL 325.809372 48.80937 \nL 326.419319 48.80937 \nL 327.029266 56.443098 \nL 330.079002 56.443098 \nL 330.688949 64.076826 \nL 331.298897 56.443098 \nL 331.908844 56.443098 \nL 332.518791 79.344281 \nL 333.128738 52.626254 \nL 333.738685 48.80937 \nL 334.348632 52.626254 \nL 335.568527 52.626254 \nL 336.178474 60.259982 \nL 336.788421 60.259982 \nL 337.398368 52.626254 \nL 338.008315 60.259982 \nL 338.618263 52.626254 \nL 339.22821 60.259982 \nL 340.448104 60.259982 \nL 341.058051 71.710554 \nL 341.667998 56.443098 \nL 342.277946 48.80937 \nL 342.887893 67.89371 \nL 343.49784 67.89371 \nL 344.107787 56.443098 \nL 344.717734 64.076826 \nL 345.327681 52.626254 \nL 345.937629 67.89371 \nL 346.547576 48.80937 \nL 347.157523 60.259982 \nL 347.76747 60.259982 \nL 348.377417 44.992526 \nL 348.987365 56.443098 \nL 349.597312 71.710554 \nL 350.207259 52.626254 \nL 350.817206 64.076826 \nL 351.427153 56.443098 \nL 352.0371 60.259982 \nL 352.647048 71.710554 \nL 353.256995 48.80937 \nL 353.866942 60.259982 \nL 354.476889 56.443098 \nL 355.086836 67.89371 \nL 355.696783 75.527438 \nL 356.306731 52.626254 \nL 356.916678 52.626254 \nL 357.526625 64.076826 \nL 358.136572 56.443098 \nL 358.746519 60.259982 \nL 359.966414 44.992526 \nL 360.576361 60.259982 \nL 361.186308 52.626254 \nL 361.796255 48.80937 \nL 362.406202 60.259982 \nL 363.016149 64.076826 \nL 363.626097 48.80937 \nL 364.236044 64.076826 \nL 364.845991 56.443098 \nL 365.455938 52.626254 \nL 366.065885 67.89371 \nL 366.675832 52.626254 \nL 367.28578 44.992526 \nL 367.895727 60.259982 \nL 368.505674 60.259982 \nL 369.115621 52.626254 \nL 369.725568 71.710554 \nL 369.725568 71.710554 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 239.758125 \nL 50.14375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 239.758125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 22.318125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- model accuracy -->\n    <defs>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path id=\"DejaVuSans-32\"/>\n    </defs>\n    <g transform=\"translate(169.882188 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"158.59375\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"222.070312\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"283.59375\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"311.376953\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"343.164062\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"404.443359\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"459.423828\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"514.404297\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"577.783203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"618.896484\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"680.175781\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"735.15625\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 57.14375 59.674375 \nL 112.41875 59.674375 \nQ 114.41875 59.674375 114.41875 57.674375 \nL 114.41875 29.318125 \nQ 114.41875 27.318125 112.41875 27.318125 \nL 57.14375 27.318125 \nQ 55.14375 27.318125 55.14375 29.318125 \nL 55.14375 57.674375 \nQ 55.14375 59.674375 57.14375 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_16\">\n     <path d=\"M 59.14375 35.416562 \nL 79.14375 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_17\"/>\n    <g id=\"text_17\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(87.14375 38.916562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 59.14375 50.094687 \nL 79.14375 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_18\">\n     <!-- test -->\n     <defs>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(87.14375 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9004750f1a\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
                        "image/png": "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\n"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "def draw_confusion_matrix(evaluation_result, label):\n",
                "    # TP FP\n",
                "    # FN TN\n",
                "    matrix = np.round(np.array([[evaluation_result[1], evaluation_result[2]], [evaluation_result[4], evaluation_result[3]]]))\n",
                "    \n",
                "    plt.figure(figsize=(4,3))\n",
                "    \n",
                "    ax = sns.heatmap(data=matrix, annot=True, fmt=\".0f\")\n",
                "    ax.invert_yaxis()\n",
                "    ax.invert_xaxis()\n",
                "    \n",
                "    plt.xlabel(\"True\")\n",
                "    plt.ylabel(\"Predicted\")\n",
                "    plt.title(f\"Confusion matrix for {label}\")"
            ],
            "execution_count": 16,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "print(\"Train evaluation:\")\n",
                "evaluation_train = model.evaluate(X_train, Y_train, verbose=2)\n",
                "draw_confusion_matrix(evaluation_train, \"train\")"
            ],
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Train evaluation:\n23/23 - 0s - loss: 0.4661 - tp: 216.0000 - fp: 76.0000 - tn: 368.0000 - fn: 52.0000 - accuracy: 0.8202 - precision: 0.7397 - recall: 0.8060 - auc: 0.8808\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<Figure size 288x216 with 2 Axes>",
                        "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"222.954375pt\" version=\"1.1\" viewBox=\"0 0 268.71775 222.954375\" width=\"268.71775pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 222.954375 \nL 268.71775 222.954375 \nL 268.71775 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 37.55625 185.398125 \nL 216.11625 185.398125 \nL 216.11625 22.318125 \nL 37.55625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"QuadMesh_1\">\n    <path clip-path=\"url(#p6d3206d6bb)\" d=\"M 216.11625 185.398125 \nL 126.83625 185.398125 \nL 126.83625 103.858125 \nL 216.11625 103.858125 \nL 216.11625 185.398125 \n\" style=\"fill:#d11f4c;\"/>\n    <path clip-path=\"url(#p6d3206d6bb)\" d=\"M 126.83625 185.398125 \nL 37.55625 185.398125 \nL 37.55625 103.858125 \nL 126.83625 103.858125 \nL 126.83625 185.398125 \n\" style=\"fill:#1d112c;\"/>\n    <path clip-path=\"url(#p6d3206d6bb)\" d=\"M 216.11625 103.858125 \nL 126.83625 103.858125 \nL 126.83625 22.318125 \nL 216.11625 22.318125 \nL 216.11625 103.858125 \n\" style=\"fill:#03051a;\"/>\n    <path clip-path=\"url(#p6d3206d6bb)\" d=\"M 126.83625 103.858125 \nL 37.55625 103.858125 \nL 37.55625 22.318125 \nL 126.83625 22.318125 \nL 126.83625 103.858125 \n\" style=\"fill:#faebdd;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m8203e95e77\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.47625\" xlink:href=\"#m8203e95e77\" y=\"185.398125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(168.295 199.996562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"82.19625\" xlink:href=\"#m8203e95e77\" y=\"185.398125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(79.015 199.996562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- True -->\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     </defs>\n     <g transform=\"translate(116.219063 213.674688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"150.826172\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m954b4aebe2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#m954b4aebe2\" y=\"144.628125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(28.476563 147.191406)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#m954b4aebe2\" y=\"63.088125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(28.476563 65.651406)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Predicted -->\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <g transform=\"translate(14.798438 127.328437)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"344.388672\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"405.912109\" xlink:href=\"#DejaVuSans-100\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_7\">\n    <!-- 216 -->\n    <defs>\n     <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n     <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(161.9325 147.3875)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-49\"/>\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-54\"/>\n    </g>\n   </g>\n   <g id=\"text_8\">\n    <!-- 76 -->\n    <defs>\n     <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(75.83375 147.3875)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-55\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\n    </g>\n   </g>\n   <g id=\"text_9\">\n    <!-- 52 -->\n    <defs>\n     <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(165.11375 65.8475)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-53\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n    </g>\n   </g>\n   <g id=\"text_10\">\n    <!-- 368 -->\n    <defs>\n     <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n     <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(72.6525 65.8475)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-51\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-56\"/>\n    </g>\n   </g>\n   <g id=\"text_11\">\n    <!-- Confusion matrix for train -->\n    <defs>\n     <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n    </defs>\n    <g transform=\"translate(49.498125 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"194.384766\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"229.589844\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"292.96875\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"345.068359\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"372.851562\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"434.033203\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"497.412109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"529.199219\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"626.611328\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"687.890625\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"727.099609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"768.212891\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"795.996094\" xlink:href=\"#DejaVuSans-120\"/>\n     <use x=\"855.175781\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"886.962891\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"922.167969\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"983.349609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1024.462891\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1056.25\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1095.458984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1136.572266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1197.851562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1225.634766\" xlink:href=\"#DejaVuSans-110\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pe76fdb516c)\" d=\"M 227.27625 185.398125 \nL 227.27625 184.761094 \nL 227.27625 22.955156 \nL 227.27625 22.318125 \nL 235.43025 22.318125 \nL 235.43025 22.955156 \nL 235.43025 184.761094 \nL 235.43025 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\n   </g>\n   <image height=\"163\" id=\"imagebc6bbdc8b7\" transform=\"scale(1 -1)translate(0 -163)\" width=\"8\" x=\"227\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAgAAACjCAYAAAC31F+mAAAABHNCSVQICAgIfAhkiAAAARhJREFUWIXV19ENwyAMhGFs3C7R/ddsugEfklPU5hXr9x0+AYnH83WNxVcRsVoflZEqAGGKwAKKnCmR4y9stgmyScINNkd7mgdsknCgxQ02leoTNtef89AnrD2MUcXIaR84izJh/e0Q6KLf4vuEi6EFgTY5TbdY3po7hBJhY6tR0Bd5hLCu8EbNC4SNwIhADSS0NQQ3KlDgFiSoIEnItzQs1zcIiWlZZCK1FWphmwhE5ZTNtsgNwnL9Dg0hQuKYs8gQAc+HDUKkbj0c56Xz3i3uIHRf5iToct4QOUEYnkWXYA1swbf9AZH6Kf4JmyLoHVSDoZ0i9FuYwFmI4GmKcKAF8zAfX9dgm8rkAZv9PLR3Mto2f0IDpvkB0+4oS7K28UwAAAAASUVORK5CYII=\" y=\"-21\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL 3.5 0 \n\" id=\"mc5461486c7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#mc5461486c7\" y=\"160.626479\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 100 -->\n      <g transform=\"translate(242.43025 164.425698)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#mc5461486c7\" y=\"134.822682\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 150 -->\n      <g transform=\"translate(242.43025 138.621901)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#mc5461486c7\" y=\"109.018884\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 200 -->\n      <g transform=\"translate(242.43025 112.818103)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#mc5461486c7\" y=\"83.215087\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 250 -->\n      <g transform=\"translate(242.43025 87.014306)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#mc5461486c7\" y=\"57.41129\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 300 -->\n      <g transform=\"translate(242.43025 61.210508)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#mc5461486c7\" y=\"31.607492\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 350 -->\n      <g transform=\"translate(242.43025 35.406711)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 227.27625 185.398125 \nL 227.27625 184.761094 \nL 227.27625 22.955156 \nL 227.27625 22.318125 \nL 235.43025 22.318125 \nL 235.43025 22.955156 \nL 235.43025 184.761094 \nL 235.43025 185.398125 \nz\n\" style=\"fill:none;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6d3206d6bb\">\n   <rect height=\"163.08\" width=\"178.56\" x=\"37.55625\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"pe76fdb516c\">\n   <rect height=\"163.08\" width=\"8.154\" x=\"227.27625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
                        "image/png": "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\n"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "print(\"Dev evaluation:\")\n",
                "evaluation_dev = model.evaluate(X_dev, Y_dev, verbose=2)\n",
                "draw_confusion_matrix(evaluation_dev, \"dev\")"
            ],
            "execution_count": 18,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Dev evaluation:\n6/6 - 0s - loss: 0.5088 - tp: 62.0000 - fp: 25.0000 - tn: 80.0000 - fn: 12.0000 - accuracy: 0.7933 - precision: 0.7126 - recall: 0.8378 - auc: 0.8929\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<Figure size 288x216 with 2 Axes>",
                        "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"222.954375pt\" version=\"1.1\" viewBox=\"0 0 262.35525 222.954375\" width=\"262.35525pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 222.954375 \nL 262.35525 222.954375 \nL 262.35525 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 37.55625 185.398125 \nL 216.11625 185.398125 \nL 216.11625 22.318125 \nL 37.55625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"QuadMesh_1\">\n    <path clip-path=\"url(#p2042a14229)\" d=\"M 216.11625 185.398125 \nL 126.83625 185.398125 \nL 126.83625 103.858125 \nL 216.11625 103.858125 \nL 216.11625 185.398125 \n\" style=\"fill:#f4815a;\"/>\n    <path clip-path=\"url(#p2042a14229)\" d=\"M 126.83625 185.398125 \nL 37.55625 185.398125 \nL 37.55625 103.858125 \nL 126.83625 103.858125 \nL 126.83625 185.398125 \n\" style=\"fill:#481c48;\"/>\n    <path clip-path=\"url(#p2042a14229)\" d=\"M 216.11625 103.858125 \nL 126.83625 103.858125 \nL 126.83625 22.318125 \nL 216.11625 22.318125 \nL 216.11625 103.858125 \n\" style=\"fill:#03051a;\"/>\n    <path clip-path=\"url(#p2042a14229)\" d=\"M 126.83625 103.858125 \nL 37.55625 103.858125 \nL 37.55625 22.318125 \nL 126.83625 22.318125 \nL 126.83625 103.858125 \n\" style=\"fill:#faebdd;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m09dac9bffd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.47625\" xlink:href=\"#m09dac9bffd\" y=\"185.398125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(168.295 199.996562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"82.19625\" xlink:href=\"#m09dac9bffd\" y=\"185.398125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(79.015 199.996562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- True -->\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     </defs>\n     <g transform=\"translate(116.219063 213.674688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"150.826172\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m2cf43fade2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#m2cf43fade2\" y=\"144.628125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(28.476563 147.191406)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#m2cf43fade2\" y=\"63.088125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(28.476563 65.651406)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Predicted -->\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <g transform=\"translate(14.798438 127.328437)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"344.388672\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"405.912109\" xlink:href=\"#DejaVuSans-100\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_7\">\n    <!-- 62 -->\n    <defs>\n     <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n     <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(165.11375 147.3875)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-54\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n    </g>\n   </g>\n   <g id=\"text_8\">\n    <!-- 25 -->\n    <defs>\n     <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(75.83375 147.3875)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n    </g>\n   </g>\n   <g id=\"text_9\">\n    <!-- 12 -->\n    <g style=\"fill:#ffffff;\" transform=\"translate(165.11375 65.8475)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-49\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n    </g>\n   </g>\n   <g id=\"text_10\">\n    <!-- 80 -->\n    <defs>\n     <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(75.83375 65.8475)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-56\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n   <g id=\"text_11\">\n    <!-- Confusion matrix for dev -->\n    <defs>\n     <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n    </defs>\n    <g transform=\"translate(52.410938 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"194.384766\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"229.589844\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"292.96875\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"345.068359\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"372.851562\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"434.033203\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"497.412109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"529.199219\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"626.611328\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"687.890625\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"727.099609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"768.212891\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"795.996094\" xlink:href=\"#DejaVuSans-120\"/>\n     <use x=\"855.175781\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"886.962891\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"922.167969\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"983.349609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1024.462891\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1056.25\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"1119.726562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1181.25\" xlink:href=\"#DejaVuSans-118\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p8866dced79)\" d=\"M 227.27625 185.398125 \nL 227.27625 184.761094 \nL 227.27625 22.955156 \nL 227.27625 22.318125 \nL 235.43025 22.318125 \nL 235.43025 22.955156 \nL 235.43025 184.761094 \nL 235.43025 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\n   </g>\n   <image height=\"163\" id=\"image230488f0a2\" transform=\"scale(1 -1)translate(0 -163)\" width=\"8\" x=\"227\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAgAAACjCAYAAAC31F+mAAAABHNCSVQICAgIfAhkiAAAARhJREFUWIXV19ENwyAMhGFs3C7R/ddsugEfklPU5hXr9x0+AYnH83WNxVcRsVoflZEqAGGKwAKKnCmR4y9stgmyScINNkd7mgdsknCgxQ02leoTNtef89AnrD2MUcXIaR84izJh/e0Q6KLf4vuEi6EFgTY5TbdY3po7hBJhY6tR0Bd5hLCu8EbNC4SNwIhADSS0NQQ3KlDgFiSoIEnItzQs1zcIiWlZZCK1FWphmwhE5ZTNtsgNwnL9Dg0hQuKYs8gQAc+HDUKkbj0c56Xz3i3uIHRf5iToct4QOUEYnkWXYA1swbf9AZH6Kf4JmyLoHVSDoZ0i9FuYwFmI4GmKcKAF8zAfX9dgm8rkAZv9PLR3Mto2f0IDpvkB0+4oS7K28UwAAAAASUVORK5CYII=\" y=\"-21\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL 3.5 0 \n\" id=\"m87b72ec6c9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m87b72ec6c9\" y=\"166.212243\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(242.43025 170.011461)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m87b72ec6c9\" y=\"142.22989\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(242.43025 146.029108)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m87b72ec6c9\" y=\"118.247537\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(242.43025 122.046756)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m87b72ec6c9\" y=\"94.265184\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 50 -->\n      <g transform=\"translate(242.43025 98.064403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m87b72ec6c9\" y=\"70.282831\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 60 -->\n      <g transform=\"translate(242.43025 74.08205)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m87b72ec6c9\" y=\"46.300478\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 70 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(242.43025 50.099697)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m87b72ec6c9\" y=\"22.318125\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 80 -->\n      <g transform=\"translate(242.43025 26.117344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 227.27625 185.398125 \nL 227.27625 184.761094 \nL 227.27625 22.955156 \nL 227.27625 22.318125 \nL 235.43025 22.318125 \nL 235.43025 22.955156 \nL 235.43025 184.761094 \nL 235.43025 185.398125 \nz\n\" style=\"fill:none;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p2042a14229\">\n   <rect height=\"163.08\" width=\"178.56\" x=\"37.55625\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p8866dced79\">\n   <rect height=\"163.08\" width=\"8.154\" x=\"227.27625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
                        "image/png": "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\n"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "# Predict with DL model"
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "def store_predictions(model, submission_name):\n",
                "    predictions = model.predict(X_test)\n",
                "\n",
                "    predictions = np.round(predictions).astype(np.uint8).reshape((-1))\n",
                "\n",
                "    print(f\"{submission_name}:\\n{predictions}\")\n",
                "    \n",
                "    output = pd.DataFrame({\"Survived\": predictions}, index=test_data.index)\n",
                "    output.to_csv(f\"/kaggle/working/{submission_name}.csv\", index=True)"
            ],
            "execution_count": 19,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "store_predictions(model, \"dl_submission\")"
            ],
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "dl_submission:\n[0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1\n 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1\n 1 1 0 1 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1\n 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0\n 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n 0 0 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0\n 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0\n 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0\n 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0\n 1 1 0 1 1 1 0 1 0 0 1]\n"
                }
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "COMET INFO: ---------------------------\nCOMET INFO: Comet.ml Experiment Summary\nCOMET INFO: ---------------------------\nCOMET INFO:   Data:\nCOMET INFO:     display_summary_level : 1\nCOMET INFO:     url                   : https://www.comet.ml/witalia008/titanic/d56364a7269d45bd86fdd71b091aaa61\nCOMET INFO:   Metrics [count] (min, max):\nCOMET INFO:     accuracy [500]                 : (0.5617977380752563, 0.851123571395874)\nCOMET INFO:     auc [500]                      : (0.4914490282535553, 0.8860595226287842)\nCOMET INFO:     batch_accuracy [1500]          : (0.4375, 1.0)\nCOMET INFO:     batch_auc [1500]               : (0.3481781482696533, 1.0)\nCOMET INFO:     batch_fn [1500]                : (0.0, 210.0)\nCOMET INFO:     batch_fp [1500]                : (0.0, 173.0)\nCOMET INFO:     batch_loss [1500]              : (0.2622811198234558, 4.647127628326416)\nCOMET INFO:     batch_precision [1500]         : (0.0, 1.0)\nCOMET INFO:     batch_recall [1500]            : (0.0, 1.0)\nCOMET INFO:     batch_tn [1500]                : (9.0, 391.0)\nCOMET INFO:     batch_tp [1500]                : (0.0, 207.0)\nCOMET INFO:     epoch_duration [500]           : (0.2831311980000919, 7.138746093999998)\nCOMET INFO:     fn [500]                       : (51.0, 221.0)\nCOMET INFO:     fp [500]                       : (35.0, 183.0)\nCOMET INFO:     loss [500]                     : (0.4657095670700073, 4.2284932136535645)\nCOMET INFO:     precision [500]                : (0.3507462739944458, 0.8478260636329651)\nCOMET INFO:     recall [500]                   : (0.1753731369972229, 0.8097015023231506)\nCOMET INFO:     tn [500]                       : (261.0, 409.0)\nCOMET INFO:     tp [500]                       : (47.0, 217.0)\nCOMET INFO:     val_accuracy [500]             : (0.6759776473045349, 0.8379888534545898)\nCOMET INFO:     val_auc [500]                  : (0.7007721662521362, 0.9028313755989075)\nCOMET INFO:     val_fn [500]                   : (5.0, 40.0)\nCOMET INFO:     val_fp [500]                   : (10.0, 45.0)\nCOMET INFO:     val_loss [500]                 : (0.4594171643257141, 3.8277840614318848)\nCOMET INFO:     val_precision [500]            : (0.5833333134651184, 0.84375)\nCOMET INFO:     val_recall [500]               : (0.45945945382118225, 0.9324324131011963)\nCOMET INFO:     val_tn [500]                   : (60.0, 95.0)\nCOMET INFO:     val_tp [500]                   : (34.0, 69.0)\nCOMET INFO:     validate_batch_accuracy [500]  : (0.5625, 0.875)\nCOMET INFO:     validate_batch_auc [500]       : (0.5277777910232544, 0.942460298538208)\nCOMET INFO:     validate_batch_fn [500]        : (1.0, 10.0)\nCOMET INFO:     validate_batch_fp [500]        : (0.0, 8.0)\nCOMET INFO:     validate_batch_loss [500]      : (0.41297879815101624, 3.8639068603515625)\nCOMET INFO:     validate_batch_precision [500] : (0.5, 1.0)\nCOMET INFO:     validate_batch_recall [500]    : (0.2857142984867096, 0.9285714030265808)\nCOMET INFO:     validate_batch_tn [500]        : (10.0, 18.0)\nCOMET INFO:     validate_batch_tp [500]        : (4.0, 13.0)\nCOMET INFO:   Others:\nCOMET INFO:     trainable_params : 6969\nCOMET INFO:   Parameters:\nCOMET INFO:     Adam_amsgrad       : 1\nCOMET INFO:     Adam_beta_1        : 0.9\nCOMET INFO:     Adam_beta_2        : 0.999\nCOMET INFO:     Adam_decay         : 1\nCOMET INFO:     Adam_epsilon       : 1e-07\nCOMET INFO:     Adam_learning_rate : 0.001\nCOMET INFO:     Adam_name          : Adam\nCOMET INFO:     Optimizer          : Adam\nCOMET INFO:     epochs             : 500\nCOMET INFO:     steps              : 23\nCOMET INFO:   Uploads:\nCOMET INFO:     code                     : 1 (7 KB)\nCOMET INFO:     environment details      : 1\nCOMET INFO:     filename                 : 1\nCOMET INFO:     git metadata             : 1\nCOMET INFO:     git-patch (uncompressed) : 1 (481 KB)\nCOMET INFO:     installed packages       : 1\nCOMET INFO:     model graph              : 1\nCOMET INFO:     notebook                 : 1\nCOMET INFO:     os packages              : 1\nCOMET INFO: ---------------------------\nCOMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
                }
            ],
            "source": [
                "if experiment:\n",
                "    experiment.end()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3.7.6 64-bit",
            "name": "python37664bit26036f9dd6fd4360b35d7bd99f8e1f11"
        },
        "language_info": {
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "version": "3.7.6-final",
            "file_extension": ".py",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "name": "python",
            "mimetype": "text/x-python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}