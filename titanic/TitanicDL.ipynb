{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Deep Learning approach to the Titanic problem"
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "%env PYTHONHASHSEED=0\n",
                "\n",
                "import os\n",
                "DEVMODE = os.getenv(\"KAGGLE_MODE\") == \"DEV\"\n",
                "print(f\"DEV MODE: {DEVMODE}\")\n",
                "\n",
                "# Define seed to reprodicibility of random generation\n",
                "SEED = 42\n",
                "DEV_SPLIT = 0.2"
            ],
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "env: PYTHONHASHSEED=0\nDEV MODE: True\n"
                }
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "COMET INFO: old comet version (3.1.9) detected. current: 3.1.13 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\nCOMET INFO: Experiment is live on comet.ml https://www.comet.ml/witalia008/titanic/f2deeaa19a724c2ebc2b0b8266342b82\n\n"
                }
            ],
            "source": [
                "import importlib\n",
                "if importlib.util.find_spec(\"comet_ml\"):\n",
                "    from comet_ml import Experiment\n",
                "    experiment = Experiment(project_name=\"titanic\")\n",
                "else:\n",
                "    experiment = None"
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# To display all the columns from left to right without breaking into next line.\n",
                "pd.set_option(\"display.width\", 1500)\n",
                "pd.plotting.register_matplotlib_converters()\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "import seaborn as sns\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "print(tf.__version__)"
            ],
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "2.2.0\n"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "import random as python_random\n",
                "\n",
                "# Make sure Keras produces reproducible results.\n",
                "\n",
                "np.random.seed(SEED)\n",
                "python_random.seed(SEED)\n",
                "tf.random.set_seed(SEED)"
            ],
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
                "print(physical_devices)\n",
                "for device in (physical_devices or []):\n",
                "    tf.config.experimental.set_memory_growth(device, True)"
            ],
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "[]\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "## Load data and split into train/dev sets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "from titanic.titanic_data import load_titanic_data, split_data, get_data_preprocessor\n",
                "\n",
                "X_train_full, y_train_full, X_pred = load_titanic_data()\n",
                "X_train, X_valid, y_train, y_valid = split_data(X_train_full, y_train_full, test_size=DEV_SPLIT, random_state=SEED)"
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "## Define pre-processing of the data"
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "preprocessor, preprocessed_column_names = get_data_preprocessor()"
            ],
            "execution_count": 7,
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = pd.DataFrame(preprocessor.fit_transform(X_train), index=X_train.index, columns=preprocessed_column_names)\n",
                "X_valid = pd.DataFrame(preprocessor.transform(X_valid), index=X_valid.index, columns=preprocessed_column_names)\n",
                "X_pred = pd.DataFrame(preprocessor.transform(X_pred), index=X_pred.index, columns=preprocessed_column_names)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": "               Pclass    Female      Male         C         Q         S    Master      Miss        Mr       Mrs    Others      Fare    Family  Age_Missing       Age\nPassengerId                                                                                                                                                         \n332         -1.614136 -0.724310  0.724310 -0.461462 -0.303355  0.592489 -0.220456 -0.501316  0.836232 -0.394771 -0.174329 -0.062785 -0.554666    -0.494727  0.862119\n734         -0.400551 -0.724310  0.724310 -0.461462 -0.303355  0.592489 -0.220456 -0.501316  0.836232 -0.394771 -0.174329 -0.720180 -0.554666    -0.494727 -0.435616\n383          0.813034 -0.724310  0.724310 -0.461462 -0.303355  0.592489 -0.220456 -0.501316  0.836232 -0.394771 -0.174329 -0.720180 -0.554666    -0.494727  0.862119\n705          0.813034 -0.724310  0.724310 -0.461462 -0.303355  0.592489 -0.220456 -0.501316  0.836232 -0.394771 -0.174329 -0.720180  0.040096    -0.494727 -0.435616\n814          0.813034  1.380624 -1.380624 -0.461462 -0.303355  0.592489 -0.220456  1.994748 -1.195840 -0.394771 -0.174329  0.594610  3.013909    -0.494727 -1.733351",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Female</th>\n      <th>Male</th>\n      <th>C</th>\n      <th>Q</th>\n      <th>S</th>\n      <th>Master</th>\n      <th>Miss</th>\n      <th>Mr</th>\n      <th>Mrs</th>\n      <th>Others</th>\n      <th>Fare</th>\n      <th>Family</th>\n      <th>Age_Missing</th>\n      <th>Age</th>\n    </tr>\n    <tr>\n      <th>PassengerId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>332</th>\n      <td>-1.614136</td>\n      <td>-0.724310</td>\n      <td>0.724310</td>\n      <td>-0.461462</td>\n      <td>-0.303355</td>\n      <td>0.592489</td>\n      <td>-0.220456</td>\n      <td>-0.501316</td>\n      <td>0.836232</td>\n      <td>-0.394771</td>\n      <td>-0.174329</td>\n      <td>-0.062785</td>\n      <td>-0.554666</td>\n      <td>-0.494727</td>\n      <td>0.862119</td>\n    </tr>\n    <tr>\n      <th>734</th>\n      <td>-0.400551</td>\n      <td>-0.724310</td>\n      <td>0.724310</td>\n      <td>-0.461462</td>\n      <td>-0.303355</td>\n      <td>0.592489</td>\n      <td>-0.220456</td>\n      <td>-0.501316</td>\n      <td>0.836232</td>\n      <td>-0.394771</td>\n      <td>-0.174329</td>\n      <td>-0.720180</td>\n      <td>-0.554666</td>\n      <td>-0.494727</td>\n      <td>-0.435616</td>\n    </tr>\n    <tr>\n      <th>383</th>\n      <td>0.813034</td>\n      <td>-0.724310</td>\n      <td>0.724310</td>\n      <td>-0.461462</td>\n      <td>-0.303355</td>\n      <td>0.592489</td>\n      <td>-0.220456</td>\n      <td>-0.501316</td>\n      <td>0.836232</td>\n      <td>-0.394771</td>\n      <td>-0.174329</td>\n      <td>-0.720180</td>\n      <td>-0.554666</td>\n      <td>-0.494727</td>\n      <td>0.862119</td>\n    </tr>\n    <tr>\n      <th>705</th>\n      <td>0.813034</td>\n      <td>-0.724310</td>\n      <td>0.724310</td>\n      <td>-0.461462</td>\n      <td>-0.303355</td>\n      <td>0.592489</td>\n      <td>-0.220456</td>\n      <td>-0.501316</td>\n      <td>0.836232</td>\n      <td>-0.394771</td>\n      <td>-0.174329</td>\n      <td>-0.720180</td>\n      <td>0.040096</td>\n      <td>-0.494727</td>\n      <td>-0.435616</td>\n    </tr>\n    <tr>\n      <th>814</th>\n      <td>0.813034</td>\n      <td>1.380624</td>\n      <td>-1.380624</td>\n      <td>-0.461462</td>\n      <td>-0.303355</td>\n      <td>0.592489</td>\n      <td>-0.220456</td>\n      <td>1.994748</td>\n      <td>-1.195840</td>\n      <td>-0.394771</td>\n      <td>-0.174329</td>\n      <td>0.594610</td>\n      <td>3.013909</td>\n      <td>-0.494727</td>\n      <td>-1.733351</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {},
                    "execution_count": 9
                }
            ],
            "source": [
                "X_train.head()"
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "## DL model using Keras"
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "METRICS = [\n",
                "      keras.metrics.TruePositives(name=\"tp\"),\n",
                "      keras.metrics.FalsePositives(name=\"fp\"),\n",
                "      keras.metrics.TrueNegatives(name=\"tn\"),\n",
                "      keras.metrics.FalseNegatives(name=\"fn\"),\n",
                "      keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
                "      keras.metrics.Precision(name=\"precision\"),\n",
                "      keras.metrics.Recall(name=\"recall\"),\n",
                "      keras.metrics.AUC(name=\"auc\"),\n",
                "]\n",
                "\n",
                "def get_model(input_size):\n",
                "    from tensorflow.keras.models import Sequential\n",
                "    from tensorflow.keras.layers import Dense, Dropout, Input\n",
                "    from tensorflow.keras.regularizers import l2\n",
                "\n",
                "    model = Sequential([\n",
                "        Input(shape=(input_size,)),\n",
                "        Dense(40, activation=\"relu\", kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001), kernel_initializer=\"he_uniform\"),\n",
                "        Dropout(0.2),\n",
                "        Dense(56, activation=\"tanh\", kernel_regularizer=l2(0.1), bias_regularizer=l2(0.1), kernel_initializer=\"glorot_uniform\"),\n",
                "        Dropout(0.3),\n",
                "        Dense(56, activation=\"relu\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), kernel_initializer=\"he_uniform\"),\n",
                "        Dropout(0.3),\n",
                "        Dense(16, activation=\"relu\", kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001), kernel_initializer=\"he_uniform\"),\n",
                "        Dropout(0.45),\n",
                "        Dense(16, activation=\"tanh\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), kernel_initializer=\"glorot_uniform\"),\n",
                "        Dropout(0.35),\n",
                "        Dense(8, activation=\"relu\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), kernel_initializer=\"he_uniform\"),\n",
                "        Dropout(0.4),\n",
                "        Dense(1, activation=\"sigmoid\")\n",
                "    ])\n",
                "    \n",
                "    model.compile(optimizer=keras.optimizers.Adam(1e-3), metrics=METRICS, loss=\"binary_crossentropy\")\n",
                "    \n",
                "    return model"
            ],
            "execution_count": 10,
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "COMET INFO: Ignoring automatic log_parameter('verbose') because 'keras:verbose' is in COMET_LOGGING_PARAMETERS_IGNORE\nEpoch 1/500\n23/23 [==============================] - 1s 55ms/step - loss: 6.3447 - tp: 113.0000 - fp: 176.0000 - tn: 268.0000 - fn: 155.0000 - accuracy: 0.5351 - precision: 0.3910 - recall: 0.4216 - auc: 0.5458 - val_loss: 5.7010 - val_tp: 18.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 56.0000 - val_accuracy: 0.5922 - val_precision: 0.5143 - val_recall: 0.2432 - val_auc: 0.6499\nEpoch 2/500\n23/23 [==============================] - 0s 9ms/step - loss: 5.2961 - tp: 140.0000 - fp: 196.0000 - tn: 248.0000 - fn: 128.0000 - accuracy: 0.5449 - precision: 0.4167 - recall: 0.5224 - auc: 0.5199 - val_loss: 4.7535 - val_tp: 54.0000 - val_fp: 30.0000 - val_tn: 75.0000 - val_fn: 20.0000 - val_accuracy: 0.7207 - val_precision: 0.6429 - val_recall: 0.7297 - val_auc: 0.7522\nEpoch 3/500\n23/23 [==============================] - 0s 9ms/step - loss: 4.4236 - tp: 128.0000 - fp: 218.0000 - tn: 226.0000 - fn: 140.0000 - accuracy: 0.4972 - precision: 0.3699 - recall: 0.4776 - auc: 0.5136 - val_loss: 3.9837 - val_tp: 63.0000 - val_fp: 51.0000 - val_tn: 54.0000 - val_fn: 11.0000 - val_accuracy: 0.6536 - val_precision: 0.5526 - val_recall: 0.8514 - val_auc: 0.7907\nEpoch 4/500\n23/23 [==============================] - 0s 7ms/step - loss: 3.6918 - tp: 176.0000 - fp: 209.0000 - tn: 235.0000 - fn: 92.0000 - accuracy: 0.5772 - precision: 0.4571 - recall: 0.6567 - auc: 0.6010 - val_loss: 3.3570 - val_tp: 66.0000 - val_fp: 56.0000 - val_tn: 49.0000 - val_fn: 8.0000 - val_accuracy: 0.6425 - val_precision: 0.5410 - val_recall: 0.8919 - val_auc: 0.8063\nEpoch 5/500\n23/23 [==============================] - 0s 7ms/step - loss: 3.1232 - tp: 164.0000 - fp: 211.0000 - tn: 233.0000 - fn: 104.0000 - accuracy: 0.5576 - precision: 0.4373 - recall: 0.6119 - auc: 0.5924 - val_loss: 2.8320 - val_tp: 64.0000 - val_fp: 40.0000 - val_tn: 65.0000 - val_fn: 10.0000 - val_accuracy: 0.7207 - val_precision: 0.6154 - val_recall: 0.8649 - val_auc: 0.8127\nEpoch 6/500\n23/23 [==============================] - 0s 7ms/step - loss: 2.6679 - tp: 151.0000 - fp: 178.0000 - tn: 266.0000 - fn: 117.0000 - accuracy: 0.5857 - precision: 0.4590 - recall: 0.5634 - auc: 0.6057 - val_loss: 2.4212 - val_tp: 60.0000 - val_fp: 30.0000 - val_tn: 75.0000 - val_fn: 14.0000 - val_accuracy: 0.7542 - val_precision: 0.6667 - val_recall: 0.8108 - val_auc: 0.8119\nEpoch 7/500\n23/23 [==============================] - 0s 9ms/step - loss: 2.2889 - tp: 175.0000 - fp: 168.0000 - tn: 276.0000 - fn: 93.0000 - accuracy: 0.6334 - precision: 0.5102 - recall: 0.6530 - auc: 0.6698 - val_loss: 2.0955 - val_tp: 58.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 16.0000 - val_accuracy: 0.7821 - val_precision: 0.7160 - val_recall: 0.7838 - val_auc: 0.8221\nEpoch 8/500\n23/23 [==============================] - 0s 7ms/step - loss: 2.0034 - tp: 174.0000 - fp: 168.0000 - tn: 276.0000 - fn: 94.0000 - accuracy: 0.6320 - precision: 0.5088 - recall: 0.6493 - auc: 0.6734 - val_loss: 1.8389 - val_tp: 58.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 16.0000 - val_accuracy: 0.7765 - val_precision: 0.7073 - val_recall: 0.7838 - val_auc: 0.8258\nEpoch 9/500\n23/23 [==============================] - 0s 6ms/step - loss: 1.7602 - tp: 177.0000 - fp: 147.0000 - tn: 297.0000 - fn: 91.0000 - accuracy: 0.6657 - precision: 0.5463 - recall: 0.6604 - auc: 0.7263 - val_loss: 1.6229 - val_tp: 57.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 17.0000 - val_accuracy: 0.7989 - val_precision: 0.7500 - val_recall: 0.7703 - val_auc: 0.8268\nEpoch 10/500\n23/23 [==============================] - 0s 6ms/step - loss: 1.5704 - tp: 182.0000 - fp: 140.0000 - tn: 304.0000 - fn: 86.0000 - accuracy: 0.6826 - precision: 0.5652 - recall: 0.6791 - auc: 0.7396 - val_loss: 1.4450 - val_tp: 52.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 22.0000 - val_accuracy: 0.7821 - val_precision: 0.7536 - val_recall: 0.7027 - val_auc: 0.8319\nEpoch 11/500\n23/23 [==============================] - 0s 6ms/step - loss: 1.4402 - tp: 189.0000 - fp: 147.0000 - tn: 297.0000 - fn: 79.0000 - accuracy: 0.6826 - precision: 0.5625 - recall: 0.7052 - auc: 0.7251 - val_loss: 1.3232 - val_tp: 56.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 18.0000 - val_accuracy: 0.7709 - val_precision: 0.7089 - val_recall: 0.7568 - val_auc: 0.8400\nEpoch 12/500\n23/23 [==============================] - 0s 6ms/step - loss: 1.3163 - tp: 191.0000 - fp: 141.0000 - tn: 303.0000 - fn: 77.0000 - accuracy: 0.6938 - precision: 0.5753 - recall: 0.7127 - auc: 0.7349 - val_loss: 1.2022 - val_tp: 53.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 21.0000 - val_accuracy: 0.7933 - val_precision: 0.7681 - val_recall: 0.7162 - val_auc: 0.8466\nEpoch 13/500\n23/23 [==============================] - 0s 7ms/step - loss: 1.2122 - tp: 195.0000 - fp: 150.0000 - tn: 294.0000 - fn: 73.0000 - accuracy: 0.6868 - precision: 0.5652 - recall: 0.7276 - auc: 0.7607 - val_loss: 1.1220 - val_tp: 57.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 17.0000 - val_accuracy: 0.7877 - val_precision: 0.7308 - val_recall: 0.7703 - val_auc: 0.8485\nEpoch 14/500\n23/23 [==============================] - 0s 9ms/step - loss: 1.1351 - tp: 204.0000 - fp: 145.0000 - tn: 299.0000 - fn: 64.0000 - accuracy: 0.7065 - precision: 0.5845 - recall: 0.7612 - auc: 0.7697 - val_loss: 1.0522 - val_tp: 58.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 16.0000 - val_accuracy: 0.7598 - val_precision: 0.6824 - val_recall: 0.7838 - val_auc: 0.8557\nEpoch 15/500\n23/23 [==============================] - 0s 6ms/step - loss: 1.0626 - tp: 214.0000 - fp: 165.0000 - tn: 279.0000 - fn: 54.0000 - accuracy: 0.6924 - precision: 0.5646 - recall: 0.7985 - auc: 0.7817 - val_loss: 0.9949 - val_tp: 60.0000 - val_fp: 32.0000 - val_tn: 73.0000 - val_fn: 14.0000 - val_accuracy: 0.7430 - val_precision: 0.6522 - val_recall: 0.8108 - val_auc: 0.8604\nEpoch 16/500\n23/23 [==============================] - 0s 6ms/step - loss: 1.0133 - tp: 210.0000 - fp: 156.0000 - tn: 288.0000 - fn: 58.0000 - accuracy: 0.6994 - precision: 0.5738 - recall: 0.7836 - auc: 0.7799 - val_loss: 0.9379 - val_tp: 61.0000 - val_fp: 30.0000 - val_tn: 75.0000 - val_fn: 13.0000 - val_accuracy: 0.7598 - val_precision: 0.6703 - val_recall: 0.8243 - val_auc: 0.8630\nEpoch 17/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.9778 - tp: 210.0000 - fp: 168.0000 - tn: 276.0000 - fn: 58.0000 - accuracy: 0.6826 - precision: 0.5556 - recall: 0.7836 - auc: 0.7595 - val_loss: 0.9149 - val_tp: 63.0000 - val_fp: 34.0000 - val_tn: 71.0000 - val_fn: 11.0000 - val_accuracy: 0.7486 - val_precision: 0.6495 - val_recall: 0.8514 - val_auc: 0.8608\nEpoch 18/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.9238 - tp: 202.0000 - fp: 149.0000 - tn: 295.0000 - fn: 66.0000 - accuracy: 0.6980 - precision: 0.5755 - recall: 0.7537 - auc: 0.7841 - val_loss: 0.8686 - val_tp: 62.0000 - val_fp: 31.0000 - val_tn: 74.0000 - val_fn: 12.0000 - val_accuracy: 0.7598 - val_precision: 0.6667 - val_recall: 0.8378 - val_auc: 0.8615\nEpoch 19/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.8865 - tp: 213.0000 - fp: 139.0000 - tn: 305.0000 - fn: 55.0000 - accuracy: 0.7275 - precision: 0.6051 - recall: 0.7948 - auc: 0.7913 - val_loss: 0.8129 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8618\nEpoch 20/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.8706 - tp: 202.0000 - fp: 126.0000 - tn: 318.0000 - fn: 66.0000 - accuracy: 0.7303 - precision: 0.6159 - recall: 0.7537 - auc: 0.7870 - val_loss: 0.8129 - val_tp: 58.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 16.0000 - val_accuracy: 0.7765 - val_precision: 0.7073 - val_recall: 0.7838 - val_auc: 0.8588\nEpoch 21/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.8330 - tp: 200.0000 - fp: 94.0000 - tn: 350.0000 - fn: 68.0000 - accuracy: 0.7725 - precision: 0.6803 - recall: 0.7463 - auc: 0.8088 - val_loss: 0.7853 - val_tp: 58.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 16.0000 - val_accuracy: 0.7709 - val_precision: 0.6988 - val_recall: 0.7838 - val_auc: 0.8586\nEpoch 22/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.8202 - tp: 210.0000 - fp: 120.0000 - tn: 324.0000 - fn: 58.0000 - accuracy: 0.7500 - precision: 0.6364 - recall: 0.7836 - auc: 0.8014 - val_loss: 0.7738 - val_tp: 62.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 12.0000 - val_accuracy: 0.7709 - val_precision: 0.6813 - val_recall: 0.8378 - val_auc: 0.8649\nEpoch 23/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.7875 - tp: 205.0000 - fp: 117.0000 - tn: 327.0000 - fn: 63.0000 - accuracy: 0.7472 - precision: 0.6366 - recall: 0.7649 - auc: 0.8069 - val_loss: 0.7441 - val_tp: 62.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 12.0000 - val_accuracy: 0.7877 - val_precision: 0.7045 - val_recall: 0.8378 - val_auc: 0.8727\nEpoch 24/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.7618 - tp: 211.0000 - fp: 97.0000 - tn: 347.0000 - fn: 57.0000 - accuracy: 0.7837 - precision: 0.6851 - recall: 0.7873 - auc: 0.8255 - val_loss: 0.7097 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8749\nEpoch 25/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.7521 - tp: 211.0000 - fp: 98.0000 - tn: 346.0000 - fn: 57.0000 - accuracy: 0.7823 - precision: 0.6828 - recall: 0.7873 - auc: 0.8264 - val_loss: 0.7020 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8761\nEpoch 26/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.7594 - tp: 198.0000 - fp: 88.0000 - tn: 356.0000 - fn: 70.0000 - accuracy: 0.7781 - precision: 0.6923 - recall: 0.7388 - auc: 0.7982 - val_loss: 0.7058 - val_tp: 61.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 13.0000 - val_accuracy: 0.7933 - val_precision: 0.7176 - val_recall: 0.8243 - val_auc: 0.8785\nEpoch 27/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.7353 - tp: 212.0000 - fp: 109.0000 - tn: 335.0000 - fn: 56.0000 - accuracy: 0.7683 - precision: 0.6604 - recall: 0.7910 - auc: 0.8181 - val_loss: 0.7008 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8772\nEpoch 28/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.7210 - tp: 198.0000 - fp: 76.0000 - tn: 368.0000 - fn: 70.0000 - accuracy: 0.7949 - precision: 0.7226 - recall: 0.7388 - auc: 0.8202 - val_loss: 0.6780 - val_tp: 58.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 16.0000 - val_accuracy: 0.7821 - val_precision: 0.7160 - val_recall: 0.7838 - val_auc: 0.8804\nEpoch 29/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6854 - tp: 195.0000 - fp: 66.0000 - tn: 378.0000 - fn: 73.0000 - accuracy: 0.8048 - precision: 0.7471 - recall: 0.7276 - auc: 0.8422 - val_loss: 0.6745 - val_tp: 58.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 16.0000 - val_accuracy: 0.7821 - val_precision: 0.7160 - val_recall: 0.7838 - val_auc: 0.8781\nEpoch 30/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.7027 - tp: 211.0000 - fp: 86.0000 - tn: 358.0000 - fn: 57.0000 - accuracy: 0.7992 - precision: 0.7104 - recall: 0.7873 - auc: 0.8311 - val_loss: 0.6716 - val_tp: 57.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 17.0000 - val_accuracy: 0.7654 - val_precision: 0.6951 - val_recall: 0.7703 - val_auc: 0.8775\nEpoch 31/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6788 - tp: 203.0000 - fp: 77.0000 - tn: 367.0000 - fn: 65.0000 - accuracy: 0.8006 - precision: 0.7250 - recall: 0.7575 - auc: 0.8329 - val_loss: 0.6517 - val_tp: 57.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 17.0000 - val_accuracy: 0.7933 - val_precision: 0.7403 - val_recall: 0.7703 - val_auc: 0.8809\nEpoch 32/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.6600 - tp: 198.0000 - fp: 60.0000 - tn: 384.0000 - fn: 70.0000 - accuracy: 0.8174 - precision: 0.7674 - recall: 0.7388 - auc: 0.8451 - val_loss: 0.6543 - val_tp: 62.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 12.0000 - val_accuracy: 0.8045 - val_precision: 0.7294 - val_recall: 0.8378 - val_auc: 0.8784\nEpoch 33/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6747 - tp: 209.0000 - fp: 72.0000 - tn: 372.0000 - fn: 59.0000 - accuracy: 0.8160 - precision: 0.7438 - recall: 0.7799 - auc: 0.8336 - val_loss: 0.6448 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8734\nEpoch 34/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.6744 - tp: 202.0000 - fp: 89.0000 - tn: 355.0000 - fn: 66.0000 - accuracy: 0.7823 - precision: 0.6942 - recall: 0.7537 - auc: 0.8233 - val_loss: 0.6259 - val_tp: 60.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 14.0000 - val_accuracy: 0.7877 - val_precision: 0.7143 - val_recall: 0.8108 - val_auc: 0.8868\nEpoch 35/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.6376 - tp: 198.0000 - fp: 57.0000 - tn: 387.0000 - fn: 70.0000 - accuracy: 0.8216 - precision: 0.7765 - recall: 0.7388 - auc: 0.8520 - val_loss: 0.6586 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8829\nEpoch 36/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.6642 - tp: 206.0000 - fp: 72.0000 - tn: 372.0000 - fn: 62.0000 - accuracy: 0.8118 - precision: 0.7410 - recall: 0.7687 - auc: 0.8285 - val_loss: 0.6159 - val_tp: 54.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 20.0000 - val_accuracy: 0.8101 - val_precision: 0.7941 - val_recall: 0.7297 - val_auc: 0.8856\nEpoch 37/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.6670 - tp: 204.0000 - fp: 82.0000 - tn: 362.0000 - fn: 64.0000 - accuracy: 0.7949 - precision: 0.7133 - recall: 0.7612 - auc: 0.8291 - val_loss: 0.6314 - val_tp: 59.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 15.0000 - val_accuracy: 0.7877 - val_precision: 0.7195 - val_recall: 0.7973 - val_auc: 0.8810\nEpoch 38/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.6373 - tp: 202.0000 - fp: 65.0000 - tn: 379.0000 - fn: 66.0000 - accuracy: 0.8160 - precision: 0.7566 - recall: 0.7537 - auc: 0.8491 - val_loss: 0.6524 - val_tp: 60.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 14.0000 - val_accuracy: 0.7709 - val_precision: 0.6897 - val_recall: 0.8108 - val_auc: 0.8741\nEpoch 39/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6489 - tp: 205.0000 - fp: 103.0000 - tn: 341.0000 - fn: 63.0000 - accuracy: 0.7669 - precision: 0.6656 - recall: 0.7649 - auc: 0.8341 - val_loss: 0.6201 - val_tp: 61.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 13.0000 - val_accuracy: 0.7933 - val_precision: 0.7176 - val_recall: 0.8243 - val_auc: 0.8856\nEpoch 40/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.6500 - tp: 197.0000 - fp: 73.0000 - tn: 371.0000 - fn: 71.0000 - accuracy: 0.7978 - precision: 0.7296 - recall: 0.7351 - auc: 0.8208 - val_loss: 0.6355 - val_tp: 61.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 13.0000 - val_accuracy: 0.7765 - val_precision: 0.6932 - val_recall: 0.8243 - val_auc: 0.8826\nEpoch 41/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.6529 - tp: 201.0000 - fp: 80.0000 - tn: 364.0000 - fn: 67.0000 - accuracy: 0.7935 - precision: 0.7153 - recall: 0.7500 - auc: 0.8215 - val_loss: 0.6083 - val_tp: 56.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 18.0000 - val_accuracy: 0.7933 - val_precision: 0.7467 - val_recall: 0.7568 - val_auc: 0.8864\nEpoch 42/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6476 - tp: 195.0000 - fp: 63.0000 - tn: 381.0000 - fn: 73.0000 - accuracy: 0.8090 - precision: 0.7558 - recall: 0.7276 - auc: 0.8267 - val_loss: 0.6144 - val_tp: 62.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 12.0000 - val_accuracy: 0.7877 - val_precision: 0.7045 - val_recall: 0.8378 - val_auc: 0.8934\nEpoch 43/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.6431 - tp: 200.0000 - fp: 71.0000 - tn: 373.0000 - fn: 68.0000 - accuracy: 0.8048 - precision: 0.7380 - recall: 0.7463 - auc: 0.8291 - val_loss: 0.5954 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8910\nEpoch 44/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6234 - tp: 198.0000 - fp: 71.0000 - tn: 373.0000 - fn: 70.0000 - accuracy: 0.8020 - precision: 0.7361 - recall: 0.7388 - auc: 0.8286 - val_loss: 0.5939 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8910\nEpoch 45/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6327 - tp: 194.0000 - fp: 67.0000 - tn: 377.0000 - fn: 74.0000 - accuracy: 0.8020 - precision: 0.7433 - recall: 0.7239 - auc: 0.8258 - val_loss: 0.5918 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8920\nEpoch 46/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.6165 - tp: 199.0000 - fp: 66.0000 - tn: 378.0000 - fn: 69.0000 - accuracy: 0.8104 - precision: 0.7509 - recall: 0.7425 - auc: 0.8452 - val_loss: 0.5913 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8881\nEpoch 47/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.6276 - tp: 190.0000 - fp: 69.0000 - tn: 375.0000 - fn: 78.0000 - accuracy: 0.7935 - precision: 0.7336 - recall: 0.7090 - auc: 0.8249 - val_loss: 0.5926 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8911\nEpoch 48/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6116 - tp: 198.0000 - fp: 59.0000 - tn: 385.0000 - fn: 70.0000 - accuracy: 0.8188 - precision: 0.7704 - recall: 0.7388 - auc: 0.8447 - val_loss: 0.5778 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8914\nEpoch 49/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6170 - tp: 201.0000 - fp: 74.0000 - tn: 370.0000 - fn: 67.0000 - accuracy: 0.8020 - precision: 0.7309 - recall: 0.7500 - auc: 0.8362 - val_loss: 0.5848 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8928\nEpoch 50/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.6182 - tp: 195.0000 - fp: 66.0000 - tn: 378.0000 - fn: 73.0000 - accuracy: 0.8048 - precision: 0.7471 - recall: 0.7276 - auc: 0.8255 - val_loss: 0.5808 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8940\nEpoch 51/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.5979 - tp: 201.0000 - fp: 58.0000 - tn: 386.0000 - fn: 67.0000 - accuracy: 0.8244 - precision: 0.7761 - recall: 0.7500 - auc: 0.8484 - val_loss: 0.5841 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8921\nEpoch 52/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.6100 - tp: 198.0000 - fp: 63.0000 - tn: 381.0000 - fn: 70.0000 - accuracy: 0.8132 - precision: 0.7586 - recall: 0.7388 - auc: 0.8410 - val_loss: 0.5678 - val_tp: 60.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 14.0000 - val_accuracy: 0.8436 - val_precision: 0.8108 - val_recall: 0.8108 - val_auc: 0.8957\nEpoch 53/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.6195 - tp: 198.0000 - fp: 62.0000 - tn: 382.0000 - fn: 70.0000 - accuracy: 0.8146 - precision: 0.7615 - recall: 0.7388 - auc: 0.8268 - val_loss: 0.5823 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8904\nEpoch 54/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.5965 - tp: 199.0000 - fp: 52.0000 - tn: 392.0000 - fn: 69.0000 - accuracy: 0.8301 - precision: 0.7928 - recall: 0.7425 - auc: 0.8498 - val_loss: 0.5846 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8936\nEpoch 55/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.6097 - tp: 195.0000 - fp: 63.0000 - tn: 381.0000 - fn: 73.0000 - accuracy: 0.8090 - precision: 0.7558 - recall: 0.7276 - auc: 0.8285 - val_loss: 0.6120 - val_tp: 63.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 11.0000 - val_accuracy: 0.7821 - val_precision: 0.6923 - val_recall: 0.8514 - val_auc: 0.8880\nEpoch 56/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.5910 - tp: 204.0000 - fp: 73.0000 - tn: 371.0000 - fn: 64.0000 - accuracy: 0.8076 - precision: 0.7365 - recall: 0.7612 - auc: 0.8480 - val_loss: 0.5740 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8900\nEpoch 57/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5941 - tp: 201.0000 - fp: 55.0000 - tn: 389.0000 - fn: 67.0000 - accuracy: 0.8287 - precision: 0.7852 - recall: 0.7500 - auc: 0.8394 - val_loss: 0.5687 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8931\nEpoch 58/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5891 - tp: 195.0000 - fp: 56.0000 - tn: 388.0000 - fn: 73.0000 - accuracy: 0.8188 - precision: 0.7769 - recall: 0.7276 - auc: 0.8405 - val_loss: 0.5782 - val_tp: 59.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 15.0000 - val_accuracy: 0.8101 - val_precision: 0.7564 - val_recall: 0.7973 - val_auc: 0.8928\nEpoch 59/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.5880 - tp: 200.0000 - fp: 70.0000 - tn: 374.0000 - fn: 68.0000 - accuracy: 0.8062 - precision: 0.7407 - recall: 0.7463 - auc: 0.8487 - val_loss: 0.5564 - val_tp: 54.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 20.0000 - val_accuracy: 0.8156 - val_precision: 0.8060 - val_recall: 0.7297 - val_auc: 0.8997\nEpoch 60/500\n23/23 [==============================] - 0s 6ms/step - loss: 0.5743 - tp: 194.0000 - fp: 43.0000 - tn: 401.0000 - fn: 74.0000 - accuracy: 0.8357 - precision: 0.8186 - recall: 0.7239 - auc: 0.8485 - val_loss: 0.5682 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8953\nEpoch 61/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5810 - tp: 193.0000 - fp: 52.0000 - tn: 392.0000 - fn: 75.0000 - accuracy: 0.8216 - precision: 0.7878 - recall: 0.7201 - auc: 0.8438 - val_loss: 0.5533 - val_tp: 54.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 20.0000 - val_accuracy: 0.8268 - val_precision: 0.8308 - val_recall: 0.7297 - val_auc: 0.9008\nEpoch 62/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.6053 - tp: 182.0000 - fp: 43.0000 - tn: 401.0000 - fn: 86.0000 - accuracy: 0.8188 - precision: 0.8089 - recall: 0.6791 - auc: 0.8276 - val_loss: 0.5712 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8935\nEpoch 63/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5836 - tp: 200.0000 - fp: 71.0000 - tn: 373.0000 - fn: 68.0000 - accuracy: 0.8048 - precision: 0.7380 - recall: 0.7463 - auc: 0.8485 - val_loss: 0.5552 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8959\nEpoch 64/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5731 - tp: 196.0000 - fp: 52.0000 - tn: 392.0000 - fn: 72.0000 - accuracy: 0.8258 - precision: 0.7903 - recall: 0.7313 - auc: 0.8554 - val_loss: 0.5702 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8947\nEpoch 65/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5748 - tp: 192.0000 - fp: 53.0000 - tn: 391.0000 - fn: 76.0000 - accuracy: 0.8188 - precision: 0.7837 - recall: 0.7164 - auc: 0.8541 - val_loss: 0.5478 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8977\nEpoch 66/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5828 - tp: 186.0000 - fp: 42.0000 - tn: 402.0000 - fn: 82.0000 - accuracy: 0.8258 - precision: 0.8158 - recall: 0.6940 - auc: 0.8460 - val_loss: 0.5484 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9017\nEpoch 67/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5952 - tp: 189.0000 - fp: 61.0000 - tn: 383.0000 - fn: 79.0000 - accuracy: 0.8034 - precision: 0.7560 - recall: 0.7052 - auc: 0.8316 - val_loss: 0.5466 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8977\nEpoch 68/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5651 - tp: 198.0000 - fp: 45.0000 - tn: 399.0000 - fn: 70.0000 - accuracy: 0.8385 - precision: 0.8148 - recall: 0.7388 - auc: 0.8547 - val_loss: 0.5619 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8967\nEpoch 69/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5853 - tp: 194.0000 - fp: 54.0000 - tn: 390.0000 - fn: 74.0000 - accuracy: 0.8202 - precision: 0.7823 - recall: 0.7239 - auc: 0.8344 - val_loss: 0.5597 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8937\nEpoch 70/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.5644 - tp: 193.0000 - fp: 38.0000 - tn: 406.0000 - fn: 75.0000 - accuracy: 0.8413 - precision: 0.8355 - recall: 0.7201 - auc: 0.8639 - val_loss: 0.5826 - val_tp: 62.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 12.0000 - val_accuracy: 0.8045 - val_precision: 0.7294 - val_recall: 0.8378 - val_auc: 0.8927\nEpoch 71/500\n15/23 [==================>...........] - ETA: 0s - loss: 0.5690 - tp: 136.0000 - fp: 44.0000 - tn: 258.0000 - fn: 42.0000 - accuracy: 0.8208 - precision: 0.7556 - recall: 0.7640 - auc: 0.85623/23 [==============================] - 0s 8ms/step - loss: 0.5740 - tp: 191.0000 - fp: 56.0000 - tn: 388.0000 - fn: 77.0000 - accuracy: 0.8132 - precision: 0.7733 - recall: 0.7127 - auc: 0.8502 - val_loss: 0.5528 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8950\nEpoch 72/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5651 - tp: 195.0000 - fp: 57.0000 - tn: 387.0000 - fn: 73.0000 - accuracy: 0.8174 - precision: 0.7738 - recall: 0.7276 - auc: 0.8546 - val_loss: 0.5476 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8992\nEpoch 73/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5775 - tp: 187.0000 - fp: 49.0000 - tn: 395.0000 - fn: 81.0000 - accuracy: 0.8174 - precision: 0.7924 - recall: 0.6978 - auc: 0.8353 - val_loss: 0.5541 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8962\nEpoch 74/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5729 - tp: 193.0000 - fp: 54.0000 - tn: 390.0000 - fn: 75.0000 - accuracy: 0.8188 - precision: 0.7814 - recall: 0.7201 - auc: 0.8416 - val_loss: 0.5537 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8966\nEpoch 75/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5587 - tp: 197.0000 - fp: 54.0000 - tn: 390.0000 - fn: 71.0000 - accuracy: 0.8244 - precision: 0.7849 - recall: 0.7351 - auc: 0.8548 - val_loss: 0.5515 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8952\nEpoch 76/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5637 - tp: 200.0000 - fp: 61.0000 - tn: 383.0000 - fn: 68.0000 - accuracy: 0.8188 - precision: 0.7663 - recall: 0.7463 - auc: 0.8514 - val_loss: 0.5389 - val_tp: 54.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 20.0000 - val_accuracy: 0.8156 - val_precision: 0.8060 - val_recall: 0.7297 - val_auc: 0.8981\nEpoch 77/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5766 - tp: 182.0000 - fp: 47.0000 - tn: 397.0000 - fn: 86.0000 - accuracy: 0.8132 - precision: 0.7948 - recall: 0.6791 - auc: 0.8425 - val_loss: 0.5573 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8936\nEpoch 78/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5624 - tp: 197.0000 - fp: 53.0000 - tn: 391.0000 - fn: 71.0000 - accuracy: 0.8258 - precision: 0.7880 - recall: 0.7351 - auc: 0.8472 - val_loss: 0.5759 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8900\nEpoch 79/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5469 - tp: 200.0000 - fp: 52.0000 - tn: 392.0000 - fn: 68.0000 - accuracy: 0.8315 - precision: 0.7937 - recall: 0.7463 - auc: 0.8599 - val_loss: 0.5385 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8961\nEpoch 80/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5604 - tp: 196.0000 - fp: 45.0000 - tn: 399.0000 - fn: 72.0000 - accuracy: 0.8357 - precision: 0.8133 - recall: 0.7313 - auc: 0.8470 - val_loss: 0.5338 - val_tp: 54.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 20.0000 - val_accuracy: 0.8156 - val_precision: 0.8060 - val_recall: 0.7297 - val_auc: 0.9030\nEpoch 81/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5487 - tp: 182.0000 - fp: 35.0000 - tn: 409.0000 - fn: 86.0000 - accuracy: 0.8301 - precision: 0.8387 - recall: 0.6791 - auc: 0.8670 - val_loss: 0.5654 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8958\nEpoch 82/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5756 - tp: 210.0000 - fp: 71.0000 - tn: 373.0000 - fn: 58.0000 - accuracy: 0.8188 - precision: 0.7473 - recall: 0.7836 - auc: 0.8427 - val_loss: 0.5408 - val_tp: 54.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 20.0000 - val_accuracy: 0.8156 - val_precision: 0.8060 - val_recall: 0.7297 - val_auc: 0.8970\nEpoch 83/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5597 - tp: 197.0000 - fp: 45.0000 - tn: 399.0000 - fn: 71.0000 - accuracy: 0.8371 - precision: 0.8140 - recall: 0.7351 - auc: 0.8512 - val_loss: 0.5491 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8958\nEpoch 84/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5667 - tp: 192.0000 - fp: 49.0000 - tn: 395.0000 - fn: 76.0000 - accuracy: 0.8244 - precision: 0.7967 - recall: 0.7164 - auc: 0.8529 - val_loss: 0.5427 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8983\nEpoch 85/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5624 - tp: 192.0000 - fp: 49.0000 - tn: 395.0000 - fn: 76.0000 - accuracy: 0.8244 - precision: 0.7967 - recall: 0.7164 - auc: 0.8492 - val_loss: 0.5511 - val_tp: 62.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 12.0000 - val_accuracy: 0.8212 - val_precision: 0.7561 - val_recall: 0.8378 - val_auc: 0.8958\nEpoch 86/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5623 - tp: 188.0000 - fp: 54.0000 - tn: 390.0000 - fn: 80.0000 - accuracy: 0.8118 - precision: 0.7769 - recall: 0.7015 - auc: 0.8534 - val_loss: 0.5388 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8910\nEpoch 87/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5854 - tp: 194.0000 - fp: 60.0000 - tn: 384.0000 - fn: 74.0000 - accuracy: 0.8118 - precision: 0.7638 - recall: 0.7239 - auc: 0.8327 - val_loss: 0.5436 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8947\nEpoch 88/500\n19/23 [=======================>......] - ETA: 0s - loss: 0.5700 - tp: 155.0000 - fp: 28.0000 - tn: 351.0000 - fn: 74.0000 - accuracy: 0.8322 - precision: 0.8470 - recall: 0.6769 - auc: 0.84423/23 [==============================] - 0s 11ms/step - loss: 0.5703 - tp: 182.0000 - fp: 38.0000 - tn: 406.0000 - fn: 86.0000 - accuracy: 0.8258 - precision: 0.8273 - recall: 0.6791 - auc: 0.8448 - val_loss: 0.5277 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9013\nEpoch 89/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5562 - tp: 193.0000 - fp: 45.0000 - tn: 399.0000 - fn: 75.0000 - accuracy: 0.8315 - precision: 0.8109 - recall: 0.7201 - auc: 0.8451 - val_loss: 0.5407 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8963\nEpoch 90/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5673 - tp: 188.0000 - fp: 50.0000 - tn: 394.0000 - fn: 80.0000 - accuracy: 0.8174 - precision: 0.7899 - recall: 0.7015 - auc: 0.8412 - val_loss: 0.5328 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8993\nEpoch 91/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5349 - tp: 199.0000 - fp: 51.0000 - tn: 393.0000 - fn: 69.0000 - accuracy: 0.8315 - precision: 0.7960 - recall: 0.7425 - auc: 0.8638 - val_loss: 0.5496 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8958\nEpoch 92/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5510 - tp: 200.0000 - fp: 49.0000 - tn: 395.0000 - fn: 68.0000 - accuracy: 0.8357 - precision: 0.8032 - recall: 0.7463 - auc: 0.8509 - val_loss: 0.5351 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8994\nEpoch 93/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5736 - tp: 188.0000 - fp: 44.0000 - tn: 400.0000 - fn: 80.0000 - accuracy: 0.8258 - precision: 0.8103 - recall: 0.7015 - auc: 0.8333 - val_loss: 0.5416 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8985\nEpoch 94/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5600 - tp: 192.0000 - fp: 58.0000 - tn: 386.0000 - fn: 76.0000 - accuracy: 0.8118 - precision: 0.7680 - recall: 0.7164 - auc: 0.8459 - val_loss: 0.5437 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8966\nEpoch 95/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5527 - tp: 200.0000 - fp: 54.0000 - tn: 390.0000 - fn: 68.0000 - accuracy: 0.8287 - precision: 0.7874 - recall: 0.7463 - auc: 0.8500 - val_loss: 0.5347 - val_tp: 54.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 20.0000 - val_accuracy: 0.8156 - val_precision: 0.8060 - val_recall: 0.7297 - val_auc: 0.8998\nEpoch 96/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5738 - tp: 182.0000 - fp: 39.0000 - tn: 405.0000 - fn: 86.0000 - accuracy: 0.8244 - precision: 0.8235 - recall: 0.6791 - auc: 0.8405 - val_loss: 0.5583 - val_tp: 63.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 11.0000 - val_accuracy: 0.8268 - val_precision: 0.7590 - val_recall: 0.8514 - val_auc: 0.8969\nEpoch 97/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5368 - tp: 201.0000 - fp: 55.0000 - tn: 389.0000 - fn: 67.0000 - accuracy: 0.8287 - precision: 0.7852 - recall: 0.7500 - auc: 0.8667 - val_loss: 0.5288 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8990\nEpoch 98/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5688 - tp: 190.0000 - fp: 47.0000 - tn: 397.0000 - fn: 78.0000 - accuracy: 0.8244 - precision: 0.8017 - recall: 0.7090 - auc: 0.8400 - val_loss: 0.5366 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8970\nEpoch 99/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5463 - tp: 192.0000 - fp: 48.0000 - tn: 396.0000 - fn: 76.0000 - accuracy: 0.8258 - precision: 0.8000 - recall: 0.7164 - auc: 0.8559 - val_loss: 0.5343 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8968\nEpoch 100/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5582 - tp: 203.0000 - fp: 57.0000 - tn: 387.0000 - fn: 65.0000 - accuracy: 0.8287 - precision: 0.7808 - recall: 0.7575 - auc: 0.8424 - val_loss: 0.5212 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9017\nEpoch 101/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5438 - tp: 193.0000 - fp: 44.0000 - tn: 400.0000 - fn: 75.0000 - accuracy: 0.8329 - precision: 0.8143 - recall: 0.7201 - auc: 0.8573 - val_loss: 0.5354 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8988\nEpoch 102/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5509 - tp: 187.0000 - fp: 44.0000 - tn: 400.0000 - fn: 81.0000 - accuracy: 0.8244 - precision: 0.8095 - recall: 0.6978 - auc: 0.8470 - val_loss: 0.5267 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8992\nEpoch 103/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5483 - tp: 192.0000 - fp: 58.0000 - tn: 386.0000 - fn: 76.0000 - accuracy: 0.8118 - precision: 0.7680 - recall: 0.7164 - auc: 0.8529 - val_loss: 0.5142 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9015\nEpoch 104/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5574 - tp: 194.0000 - fp: 59.0000 - tn: 385.0000 - fn: 74.0000 - accuracy: 0.8132 - precision: 0.7668 - recall: 0.7239 - auc: 0.8485 - val_loss: 0.5251 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8986\nEpoch 105/500\n23/23 [==============================] - 0s 7ms/step - loss: 0.5499 - tp: 191.0000 - fp: 47.0000 - tn: 397.0000 - fn: 77.0000 - accuracy: 0.8258 - precision: 0.8025 - recall: 0.7127 - auc: 0.8497 - val_loss: 0.5278 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8964\nEpoch 106/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5528 - tp: 194.0000 - fp: 49.0000 - tn: 395.0000 - fn: 74.0000 - accuracy: 0.8272 - precision: 0.7984 - recall: 0.7239 - auc: 0.8474 - val_loss: 0.5332 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8954\nEpoch 107/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5481 - tp: 195.0000 - fp: 48.0000 - tn: 396.0000 - fn: 73.0000 - accuracy: 0.8301 - precision: 0.8025 - recall: 0.7276 - auc: 0.8518 - val_loss: 0.5339 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8982\nEpoch 108/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5467 - tp: 193.0000 - fp: 51.0000 - tn: 393.0000 - fn: 75.0000 - accuracy: 0.8230 - precision: 0.7910 - recall: 0.7201 - auc: 0.8528 - val_loss: 0.5237 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8947\nEpoch 109/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5485 - tp: 203.0000 - fp: 53.0000 - tn: 391.0000 - fn: 65.0000 - accuracy: 0.8343 - precision: 0.7930 - recall: 0.7575 - auc: 0.8596 - val_loss: 0.5243 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8982\nEpoch 110/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5589 - tp: 189.0000 - fp: 42.0000 - tn: 402.0000 - fn: 79.0000 - accuracy: 0.8301 - precision: 0.8182 - recall: 0.7052 - auc: 0.8440 - val_loss: 0.5363 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8941\nEpoch 111/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5606 - tp: 190.0000 - fp: 55.0000 - tn: 389.0000 - fn: 78.0000 - accuracy: 0.8132 - precision: 0.7755 - recall: 0.7090 - auc: 0.8349 - val_loss: 0.5373 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8952\nEpoch 112/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5607 - tp: 197.0000 - fp: 53.0000 - tn: 391.0000 - fn: 71.0000 - accuracy: 0.8258 - precision: 0.7880 - recall: 0.7351 - auc: 0.8464 - val_loss: 0.5255 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9006\nEpoch 113/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5478 - tp: 186.0000 - fp: 52.0000 - tn: 392.0000 - fn: 82.0000 - accuracy: 0.8118 - precision: 0.7815 - recall: 0.6940 - auc: 0.8404 - val_loss: 0.5301 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8988\nEpoch 114/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5298 - tp: 190.0000 - fp: 42.0000 - tn: 402.0000 - fn: 78.0000 - accuracy: 0.8315 - precision: 0.8190 - recall: 0.7090 - auc: 0.8647 - val_loss: 0.5502 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8945\nEpoch 115/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5532 - tp: 199.0000 - fp: 60.0000 - tn: 384.0000 - fn: 69.0000 - accuracy: 0.8188 - precision: 0.7683 - recall: 0.7425 - auc: 0.8440 - val_loss: 0.5270 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8983\nEpoch 116/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5566 - tp: 190.0000 - fp: 60.0000 - tn: 384.0000 - fn: 78.0000 - accuracy: 0.8062 - precision: 0.7600 - recall: 0.7090 - auc: 0.8380 - val_loss: 0.5103 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9042\nEpoch 117/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5640 - tp: 191.0000 - fp: 50.0000 - tn: 394.0000 - fn: 77.0000 - accuracy: 0.8216 - precision: 0.7925 - recall: 0.7127 - auc: 0.8382 - val_loss: 0.5174 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9027\nEpoch 118/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5310 - tp: 192.0000 - fp: 46.0000 - tn: 398.0000 - fn: 76.0000 - accuracy: 0.8287 - precision: 0.8067 - recall: 0.7164 - auc: 0.8616 - val_loss: 0.5353 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8985\nEpoch 119/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5608 - tp: 186.0000 - fp: 37.0000 - tn: 407.0000 - fn: 82.0000 - accuracy: 0.8329 - precision: 0.8341 - recall: 0.6940 - auc: 0.8359 - val_loss: 0.5192 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9015\nEpoch 120/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5585 - tp: 178.0000 - fp: 42.0000 - tn: 402.0000 - fn: 90.0000 - accuracy: 0.8146 - precision: 0.8091 - recall: 0.6642 - auc: 0.8353 - val_loss: 0.5121 - val_tp: 59.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 15.0000 - val_accuracy: 0.8436 - val_precision: 0.8194 - val_recall: 0.7973 - val_auc: 0.9020\nEpoch 121/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5472 - tp: 190.0000 - fp: 47.0000 - tn: 397.0000 - fn: 78.0000 - accuracy: 0.8244 - precision: 0.8017 - recall: 0.7090 - auc: 0.8402 - val_loss: 0.5216 - val_tp: 59.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 15.0000 - val_accuracy: 0.8436 - val_precision: 0.8194 - val_recall: 0.7973 - val_auc: 0.8991\nEpoch 122/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5317 - tp: 198.0000 - fp: 42.0000 - tn: 402.0000 - fn: 70.0000 - accuracy: 0.8427 - precision: 0.8250 - recall: 0.7388 - auc: 0.8582 - val_loss: 0.5275 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.9001\nEpoch 123/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5472 - tp: 196.0000 - fp: 52.0000 - tn: 392.0000 - fn: 72.0000 - accuracy: 0.8258 - precision: 0.7903 - recall: 0.7313 - auc: 0.8524 - val_loss: 0.5136 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.9016\nEpoch 124/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5279 - tp: 194.0000 - fp: 47.0000 - tn: 397.0000 - fn: 74.0000 - accuracy: 0.8301 - precision: 0.8050 - recall: 0.7239 - auc: 0.8557 - val_loss: 0.5209 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9019\nEpoch 125/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5236 - tp: 195.0000 - fp: 48.0000 - tn: 396.0000 - fn: 73.0000 - accuracy: 0.8301 - precision: 0.8025 - recall: 0.7276 - auc: 0.8650 - val_loss: 0.5138 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8978\nEpoch 126/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5332 - tp: 194.0000 - fp: 48.0000 - tn: 396.0000 - fn: 74.0000 - accuracy: 0.8287 - precision: 0.8017 - recall: 0.7239 - auc: 0.8594 - val_loss: 0.5111 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8994\nEpoch 127/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5472 - tp: 196.0000 - fp: 50.0000 - tn: 394.0000 - fn: 72.0000 - accuracy: 0.8287 - precision: 0.7967 - recall: 0.7313 - auc: 0.8398 - val_loss: 0.5183 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8996\nEpoch 128/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5261 - tp: 197.0000 - fp: 44.0000 - tn: 400.0000 - fn: 71.0000 - accuracy: 0.8385 - precision: 0.8174 - recall: 0.7351 - auc: 0.8594 - val_loss: 0.5150 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9005\nEpoch 129/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5615 - tp: 190.0000 - fp: 54.0000 - tn: 390.0000 - fn: 78.0000 - accuracy: 0.8146 - precision: 0.7787 - recall: 0.7090 - auc: 0.8369 - val_loss: 0.5084 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9022\nEpoch 130/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5279 - tp: 195.0000 - fp: 42.0000 - tn: 402.0000 - fn: 73.0000 - accuracy: 0.8385 - precision: 0.8228 - recall: 0.7276 - auc: 0.8630 - val_loss: 0.5412 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.8995\nEpoch 131/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5413 - tp: 193.0000 - fp: 49.0000 - tn: 395.0000 - fn: 75.0000 - accuracy: 0.8258 - precision: 0.7975 - recall: 0.7201 - auc: 0.8518 - val_loss: 0.5207 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9001\nEpoch 132/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5230 - tp: 200.0000 - fp: 49.0000 - tn: 395.0000 - fn: 68.0000 - accuracy: 0.8357 - precision: 0.8032 - recall: 0.7463 - auc: 0.8685 - val_loss: 0.5241 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8970\nEpoch 133/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5340 - tp: 188.0000 - fp: 42.0000 - tn: 402.0000 - fn: 80.0000 - accuracy: 0.8287 - precision: 0.8174 - recall: 0.7015 - auc: 0.8601 - val_loss: 0.5265 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8966\nEpoch 134/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5415 - tp: 192.0000 - fp: 51.0000 - tn: 393.0000 - fn: 76.0000 - accuracy: 0.8216 - precision: 0.7901 - recall: 0.7164 - auc: 0.8530 - val_loss: 0.5099 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8983\nEpoch 135/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5266 - tp: 202.0000 - fp: 53.0000 - tn: 391.0000 - fn: 66.0000 - accuracy: 0.8329 - precision: 0.7922 - recall: 0.7537 - auc: 0.8647 - val_loss: 0.5222 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8976\nEpoch 136/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5203 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8618 - val_loss: 0.5246 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8928\nEpoch 137/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5348 - tp: 200.0000 - fp: 54.0000 - tn: 390.0000 - fn: 68.0000 - accuracy: 0.8287 - precision: 0.7874 - recall: 0.7463 - auc: 0.8605 - val_loss: 0.5066 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9005\nEpoch 138/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5408 - tp: 192.0000 - fp: 44.0000 - tn: 400.0000 - fn: 76.0000 - accuracy: 0.8315 - precision: 0.8136 - recall: 0.7164 - auc: 0.8511 - val_loss: 0.5097 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9026\nEpoch 139/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5480 - tp: 185.0000 - fp: 45.0000 - tn: 399.0000 - fn: 83.0000 - accuracy: 0.8202 - precision: 0.8043 - recall: 0.6903 - auc: 0.8384 - val_loss: 0.5139 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9033\nEpoch 140/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5325 - tp: 190.0000 - fp: 38.0000 - tn: 406.0000 - fn: 78.0000 - accuracy: 0.8371 - precision: 0.8333 - recall: 0.7090 - auc: 0.8574 - val_loss: 0.5332 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8976\nEpoch 141/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5512 - tp: 201.0000 - fp: 64.0000 - tn: 380.0000 - fn: 67.0000 - accuracy: 0.8160 - precision: 0.7585 - recall: 0.7500 - auc: 0.8537 - val_loss: 0.5146 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8965\nEpoch 142/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5479 - tp: 184.0000 - fp: 39.0000 - tn: 405.0000 - fn: 84.0000 - accuracy: 0.8272 - precision: 0.8251 - recall: 0.6866 - auc: 0.8419 - val_loss: 0.5192 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8991\nEpoch 143/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5470 - tp: 194.0000 - fp: 52.0000 - tn: 392.0000 - fn: 74.0000 - accuracy: 0.8230 - precision: 0.7886 - recall: 0.7239 - auc: 0.8423 - val_loss: 0.5193 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.9011\nEpoch 144/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5442 - tp: 193.0000 - fp: 49.0000 - tn: 395.0000 - fn: 75.0000 - accuracy: 0.8258 - precision: 0.7975 - recall: 0.7201 - auc: 0.8454 - val_loss: 0.5104 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9019\nEpoch 145/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5286 - tp: 197.0000 - fp: 46.0000 - tn: 398.0000 - fn: 71.0000 - accuracy: 0.8357 - precision: 0.8107 - recall: 0.7351 - auc: 0.8563 - val_loss: 0.5158 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9023\nEpoch 146/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5283 - tp: 193.0000 - fp: 44.0000 - tn: 400.0000 - fn: 75.0000 - accuracy: 0.8329 - precision: 0.8143 - recall: 0.7201 - auc: 0.8632 - val_loss: 0.5301 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8934\nEpoch 147/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5577 - tp: 190.0000 - fp: 45.0000 - tn: 399.0000 - fn: 78.0000 - accuracy: 0.8272 - precision: 0.8085 - recall: 0.7090 - auc: 0.8369 - val_loss: 0.5151 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8988\nEpoch 148/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5220 - tp: 192.0000 - fp: 44.0000 - tn: 400.0000 - fn: 76.0000 - accuracy: 0.8315 - precision: 0.8136 - recall: 0.7164 - auc: 0.8574 - val_loss: 0.5285 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8976\nEpoch 149/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5534 - tp: 194.0000 - fp: 62.0000 - tn: 382.0000 - fn: 74.0000 - accuracy: 0.8090 - precision: 0.7578 - recall: 0.7239 - auc: 0.8387 - val_loss: 0.5152 - val_tp: 54.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 20.0000 - val_accuracy: 0.8156 - val_precision: 0.8060 - val_recall: 0.7297 - val_auc: 0.8975\nEpoch 150/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5403 - tp: 187.0000 - fp: 42.0000 - tn: 402.0000 - fn: 81.0000 - accuracy: 0.8272 - precision: 0.8166 - recall: 0.6978 - auc: 0.8503 - val_loss: 0.5302 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8979\nEpoch 151/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5369 - tp: 199.0000 - fp: 53.0000 - tn: 391.0000 - fn: 69.0000 - accuracy: 0.8287 - precision: 0.7897 - recall: 0.7425 - auc: 0.8494 - val_loss: 0.5256 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8988\nEpoch 152/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5528 - tp: 187.0000 - fp: 40.0000 - tn: 404.0000 - fn: 81.0000 - accuracy: 0.8301 - precision: 0.8238 - recall: 0.6978 - auc: 0.8306 - val_loss: 0.5212 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.9008\nEpoch 153/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5147 - tp: 194.0000 - fp: 47.0000 - tn: 397.0000 - fn: 74.0000 - accuracy: 0.8301 - precision: 0.8050 - recall: 0.7239 - auc: 0.8654 - val_loss: 0.5170 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9018\nEpoch 154/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5146 - tp: 196.0000 - fp: 40.0000 - tn: 404.0000 - fn: 72.0000 - accuracy: 0.8427 - precision: 0.8305 - recall: 0.7313 - auc: 0.8659 - val_loss: 0.5294 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8980\nEpoch 155/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5212 - tp: 190.0000 - fp: 43.0000 - tn: 401.0000 - fn: 78.0000 - accuracy: 0.8301 - precision: 0.8155 - recall: 0.7090 - auc: 0.8606 - val_loss: 0.5276 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8972\nEpoch 156/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5410 - tp: 193.0000 - fp: 47.0000 - tn: 397.0000 - fn: 75.0000 - accuracy: 0.8287 - precision: 0.8042 - recall: 0.7201 - auc: 0.8533 - val_loss: 0.5116 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8982\nEpoch 157/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5470 - tp: 194.0000 - fp: 48.0000 - tn: 396.0000 - fn: 74.0000 - accuracy: 0.8287 - precision: 0.8017 - recall: 0.7239 - auc: 0.8341 - val_loss: 0.5214 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8987\nEpoch 158/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5386 - tp: 190.0000 - fp: 34.0000 - tn: 410.0000 - fn: 78.0000 - accuracy: 0.8427 - precision: 0.8482 - recall: 0.7090 - auc: 0.8470 - val_loss: 0.5224 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8972\nEpoch 159/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5364 - tp: 196.0000 - fp: 43.0000 - tn: 401.0000 - fn: 72.0000 - accuracy: 0.8385 - precision: 0.8201 - recall: 0.7313 - auc: 0.8542 - val_loss: 0.5364 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8964\nEpoch 160/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5307 - tp: 181.0000 - fp: 34.0000 - tn: 410.0000 - fn: 87.0000 - accuracy: 0.8301 - precision: 0.8419 - recall: 0.6754 - auc: 0.8513 - val_loss: 0.5406 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8923\nEpoch 161/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5486 - tp: 194.0000 - fp: 50.0000 - tn: 394.0000 - fn: 74.0000 - accuracy: 0.8258 - precision: 0.7951 - recall: 0.7239 - auc: 0.8439 - val_loss: 0.5245 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8953\nEpoch 162/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5507 - tp: 187.0000 - fp: 40.0000 - tn: 404.0000 - fn: 81.0000 - accuracy: 0.8301 - precision: 0.8238 - recall: 0.6978 - auc: 0.8397 - val_loss: 0.5366 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8954\nEpoch 163/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5497 - tp: 187.0000 - fp: 48.0000 - tn: 396.0000 - fn: 81.0000 - accuracy: 0.8188 - precision: 0.7957 - recall: 0.6978 - auc: 0.8501 - val_loss: 0.5076 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9017\nEpoch 164/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5213 - tp: 193.0000 - fp: 43.0000 - tn: 401.0000 - fn: 75.0000 - accuracy: 0.8343 - precision: 0.8178 - recall: 0.7201 - auc: 0.8653 - val_loss: 0.5045 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9028\nEpoch 165/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5359 - tp: 193.0000 - fp: 45.0000 - tn: 399.0000 - fn: 75.0000 - accuracy: 0.8315 - precision: 0.8109 - recall: 0.7201 - auc: 0.8444 - val_loss: 0.5053 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9012\nEpoch 166/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5356 - tp: 188.0000 - fp: 48.0000 - tn: 396.0000 - fn: 80.0000 - accuracy: 0.8202 - precision: 0.7966 - recall: 0.7015 - auc: 0.8412 - val_loss: 0.5232 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8914\nEpoch 167/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5327 - tp: 189.0000 - fp: 48.0000 - tn: 396.0000 - fn: 79.0000 - accuracy: 0.8216 - precision: 0.7975 - recall: 0.7052 - auc: 0.8524 - val_loss: 0.5396 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8958\nEpoch 168/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5425 - tp: 191.0000 - fp: 53.0000 - tn: 391.0000 - fn: 77.0000 - accuracy: 0.8174 - precision: 0.7828 - recall: 0.7127 - auc: 0.8519 - val_loss: 0.5058 - val_tp: 53.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 21.0000 - val_accuracy: 0.8156 - val_precision: 0.8154 - val_recall: 0.7162 - val_auc: 0.9004\nEpoch 169/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5480 - tp: 185.0000 - fp: 39.0000 - tn: 405.0000 - fn: 83.0000 - accuracy: 0.8287 - precision: 0.8259 - recall: 0.6903 - auc: 0.8347 - val_loss: 0.5432 - val_tp: 62.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 12.0000 - val_accuracy: 0.8324 - val_precision: 0.7750 - val_recall: 0.8378 - val_auc: 0.8976\nEpoch 170/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5317 - tp: 202.0000 - fp: 55.0000 - tn: 389.0000 - fn: 66.0000 - accuracy: 0.8301 - precision: 0.7860 - recall: 0.7537 - auc: 0.8427 - val_loss: 0.5189 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8975\nEpoch 171/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5259 - tp: 188.0000 - fp: 43.0000 - tn: 401.0000 - fn: 80.0000 - accuracy: 0.8272 - precision: 0.8139 - recall: 0.7015 - auc: 0.8560 - val_loss: 0.5186 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8968\nEpoch 172/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5380 - tp: 192.0000 - fp: 49.0000 - tn: 395.0000 - fn: 76.0000 - accuracy: 0.8244 - precision: 0.7967 - recall: 0.7164 - auc: 0.8536 - val_loss: 0.5367 - val_tp: 64.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 10.0000 - val_accuracy: 0.8156 - val_precision: 0.7356 - val_recall: 0.8649 - val_auc: 0.8977\nEpoch 173/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5314 - tp: 195.0000 - fp: 55.0000 - tn: 389.0000 - fn: 73.0000 - accuracy: 0.8202 - precision: 0.7800 - recall: 0.7276 - auc: 0.8590 - val_loss: 0.5023 - val_tp: 52.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 22.0000 - val_accuracy: 0.8101 - val_precision: 0.8125 - val_recall: 0.7027 - val_auc: 0.9002\nEpoch 174/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5338 - tp: 184.0000 - fp: 44.0000 - tn: 400.0000 - fn: 84.0000 - accuracy: 0.8202 - precision: 0.8070 - recall: 0.6866 - auc: 0.8591 - val_loss: 0.5299 - val_tp: 63.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 11.0000 - val_accuracy: 0.8492 - val_precision: 0.7975 - val_recall: 0.8514 - val_auc: 0.8997\nEpoch 175/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5371 - tp: 195.0000 - fp: 44.0000 - tn: 400.0000 - fn: 73.0000 - accuracy: 0.8357 - precision: 0.8159 - recall: 0.7276 - auc: 0.8463 - val_loss: 0.5022 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8966\nEpoch 176/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5220 - tp: 197.0000 - fp: 48.0000 - tn: 396.0000 - fn: 71.0000 - accuracy: 0.8329 - precision: 0.8041 - recall: 0.7351 - auc: 0.8592 - val_loss: 0.5175 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8938\nEpoch 177/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5290 - tp: 186.0000 - fp: 38.0000 - tn: 406.0000 - fn: 82.0000 - accuracy: 0.8315 - precision: 0.8304 - recall: 0.6940 - auc: 0.8489 - val_loss: 0.5252 - val_tp: 63.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 11.0000 - val_accuracy: 0.8492 - val_precision: 0.7975 - val_recall: 0.8514 - val_auc: 0.8982\nEpoch 178/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5197 - tp: 201.0000 - fp: 45.0000 - tn: 399.0000 - fn: 67.0000 - accuracy: 0.8427 - precision: 0.8171 - recall: 0.7500 - auc: 0.8616 - val_loss: 0.5173 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8982\nEpoch 179/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5027 - tp: 195.0000 - fp: 42.0000 - tn: 402.0000 - fn: 73.0000 - accuracy: 0.8385 - precision: 0.8228 - recall: 0.7276 - auc: 0.8764 - val_loss: 0.5170 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8988\nEpoch 180/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5233 - tp: 177.0000 - fp: 28.0000 - tn: 416.0000 - fn: 91.0000 - accuracy: 0.8329 - precision: 0.8634 - recall: 0.6604 - auc: 0.8557 - val_loss: 0.5182 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9011\nEpoch 181/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5164 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8565 - val_loss: 0.5292 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8986\nEpoch 182/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5382 - tp: 192.0000 - fp: 44.0000 - tn: 400.0000 - fn: 76.0000 - accuracy: 0.8315 - precision: 0.8136 - recall: 0.7164 - auc: 0.8568 - val_loss: 0.5081 - val_tp: 54.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 20.0000 - val_accuracy: 0.8156 - val_precision: 0.8060 - val_recall: 0.7297 - val_auc: 0.9004\nEpoch 183/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5353 - tp: 182.0000 - fp: 40.0000 - tn: 404.0000 - fn: 86.0000 - accuracy: 0.8230 - precision: 0.8198 - recall: 0.6791 - auc: 0.8503 - val_loss: 0.5170 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8957\nEpoch 184/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5245 - tp: 188.0000 - fp: 43.0000 - tn: 401.0000 - fn: 80.0000 - accuracy: 0.8272 - precision: 0.8139 - recall: 0.7015 - auc: 0.8625 - val_loss: 0.5053 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8970\nEpoch 185/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5107 - tp: 193.0000 - fp: 47.0000 - tn: 397.0000 - fn: 75.0000 - accuracy: 0.8287 - precision: 0.8042 - recall: 0.7201 - auc: 0.8660 - val_loss: 0.5077 - val_tp: 62.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 12.0000 - val_accuracy: 0.8436 - val_precision: 0.7949 - val_recall: 0.8378 - val_auc: 0.9001\nEpoch 186/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5312 - tp: 199.0000 - fp: 58.0000 - tn: 386.0000 - fn: 69.0000 - accuracy: 0.8216 - precision: 0.7743 - recall: 0.7425 - auc: 0.8500 - val_loss: 0.4980 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8979\nEpoch 187/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5218 - tp: 197.0000 - fp: 54.0000 - tn: 390.0000 - fn: 71.0000 - accuracy: 0.8244 - precision: 0.7849 - recall: 0.7351 - auc: 0.8573 - val_loss: 0.5043 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9003\nEpoch 188/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5271 - tp: 192.0000 - fp: 45.0000 - tn: 399.0000 - fn: 76.0000 - accuracy: 0.8301 - precision: 0.8101 - recall: 0.7164 - auc: 0.8587 - val_loss: 0.5097 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9009\nEpoch 189/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5320 - tp: 190.0000 - fp: 48.0000 - tn: 396.0000 - fn: 78.0000 - accuracy: 0.8230 - precision: 0.7983 - recall: 0.7090 - auc: 0.8510 - val_loss: 0.5057 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9009\nEpoch 190/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5282 - tp: 189.0000 - fp: 46.0000 - tn: 398.0000 - fn: 79.0000 - accuracy: 0.8244 - precision: 0.8043 - recall: 0.7052 - auc: 0.8549 - val_loss: 0.5169 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8992\nEpoch 191/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5335 - tp: 198.0000 - fp: 57.0000 - tn: 387.0000 - fn: 70.0000 - accuracy: 0.8216 - precision: 0.7765 - recall: 0.7388 - auc: 0.8565 - val_loss: 0.5221 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8940\nEpoch 192/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5336 - tp: 195.0000 - fp: 44.0000 - tn: 400.0000 - fn: 73.0000 - accuracy: 0.8357 - precision: 0.8159 - recall: 0.7276 - auc: 0.8523 - val_loss: 0.5238 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8947\nEpoch 193/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5201 - tp: 192.0000 - fp: 40.0000 - tn: 404.0000 - fn: 76.0000 - accuracy: 0.8371 - precision: 0.8276 - recall: 0.7164 - auc: 0.8546 - val_loss: 0.5239 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8932\nEpoch 194/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5328 - tp: 191.0000 - fp: 50.0000 - tn: 394.0000 - fn: 77.0000 - accuracy: 0.8216 - precision: 0.7925 - recall: 0.7127 - auc: 0.8477 - val_loss: 0.5080 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8995\nEpoch 195/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5356 - tp: 175.0000 - fp: 30.0000 - tn: 414.0000 - fn: 93.0000 - accuracy: 0.8272 - precision: 0.8537 - recall: 0.6530 - auc: 0.8576 - val_loss: 0.5486 - val_tp: 63.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 11.0000 - val_accuracy: 0.8156 - val_precision: 0.7412 - val_recall: 0.8514 - val_auc: 0.8953\nEpoch 196/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5582 - tp: 196.0000 - fp: 71.0000 - tn: 373.0000 - fn: 72.0000 - accuracy: 0.7992 - precision: 0.7341 - recall: 0.7313 - auc: 0.8478 - val_loss: 0.5053 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8990\nEpoch 197/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5255 - tp: 200.0000 - fp: 46.0000 - tn: 398.0000 - fn: 68.0000 - accuracy: 0.8399 - precision: 0.8130 - recall: 0.7463 - auc: 0.8584 - val_loss: 0.5132 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8990\nEpoch 198/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5162 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8630 - val_loss: 0.5084 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8989\nEpoch 199/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5327 - tp: 196.0000 - fp: 43.0000 - tn: 401.0000 - fn: 72.0000 - accuracy: 0.8385 - precision: 0.8201 - recall: 0.7313 - auc: 0.8540 - val_loss: 0.5016 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8968\nEpoch 200/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5305 - tp: 191.0000 - fp: 53.0000 - tn: 391.0000 - fn: 77.0000 - accuracy: 0.8174 - precision: 0.7828 - recall: 0.7127 - auc: 0.8573 - val_loss: 0.5093 - val_tp: 62.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 12.0000 - val_accuracy: 0.8492 - val_precision: 0.8052 - val_recall: 0.8378 - val_auc: 0.9010\nEpoch 201/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5170 - tp: 207.0000 - fp: 63.0000 - tn: 381.0000 - fn: 61.0000 - accuracy: 0.8258 - precision: 0.7667 - recall: 0.7724 - auc: 0.8636 - val_loss: 0.5000 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9015\nEpoch 202/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5211 - tp: 197.0000 - fp: 43.0000 - tn: 401.0000 - fn: 71.0000 - accuracy: 0.8399 - precision: 0.8208 - recall: 0.7351 - auc: 0.8549 - val_loss: 0.5334 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8958\nEpoch 203/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5253 - tp: 195.0000 - fp: 45.0000 - tn: 399.0000 - fn: 73.0000 - accuracy: 0.8343 - precision: 0.8125 - recall: 0.7276 - auc: 0.8553 - val_loss: 0.5032 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9005\nEpoch 204/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5107 - tp: 198.0000 - fp: 47.0000 - tn: 397.0000 - fn: 70.0000 - accuracy: 0.8357 - precision: 0.8082 - recall: 0.7388 - auc: 0.8659 - val_loss: 0.5213 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.9017\nEpoch 205/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5258 - tp: 185.0000 - fp: 42.0000 - tn: 402.0000 - fn: 83.0000 - accuracy: 0.8244 - precision: 0.8150 - recall: 0.6903 - auc: 0.8518 - val_loss: 0.5166 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.9000\nEpoch 206/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5240 - tp: 193.0000 - fp: 45.0000 - tn: 399.0000 - fn: 75.0000 - accuracy: 0.8315 - precision: 0.8109 - recall: 0.7201 - auc: 0.8570 - val_loss: 0.5263 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8966\nEpoch 207/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5250 - tp: 194.0000 - fp: 45.0000 - tn: 399.0000 - fn: 74.0000 - accuracy: 0.8329 - precision: 0.8117 - recall: 0.7239 - auc: 0.8599 - val_loss: 0.5043 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9008\nEpoch 208/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5242 - tp: 189.0000 - fp: 33.0000 - tn: 411.0000 - fn: 79.0000 - accuracy: 0.8427 - precision: 0.8514 - recall: 0.7052 - auc: 0.8485 - val_loss: 0.5467 - val_tp: 64.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 10.0000 - val_accuracy: 0.8380 - val_precision: 0.7711 - val_recall: 0.8649 - val_auc: 0.8990\nEpoch 209/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5125 - tp: 196.0000 - fp: 53.0000 - tn: 391.0000 - fn: 72.0000 - accuracy: 0.8244 - precision: 0.7871 - recall: 0.7313 - auc: 0.8647 - val_loss: 0.5035 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8996\nEpoch 210/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5112 - tp: 197.0000 - fp: 45.0000 - tn: 399.0000 - fn: 71.0000 - accuracy: 0.8371 - precision: 0.8140 - recall: 0.7351 - auc: 0.8600 - val_loss: 0.5321 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8969\nEpoch 211/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5525 - tp: 180.0000 - fp: 41.0000 - tn: 403.0000 - fn: 88.0000 - accuracy: 0.8188 - precision: 0.8145 - recall: 0.6716 - auc: 0.8407 - val_loss: 0.5181 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8981\nEpoch 212/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5306 - tp: 200.0000 - fp: 51.0000 - tn: 393.0000 - fn: 68.0000 - accuracy: 0.8329 - precision: 0.7968 - recall: 0.7463 - auc: 0.8506 - val_loss: 0.5036 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9006\nEpoch 213/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5304 - tp: 192.0000 - fp: 35.0000 - tn: 409.0000 - fn: 76.0000 - accuracy: 0.8441 - precision: 0.8458 - recall: 0.7164 - auc: 0.8489 - val_loss: 0.5116 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8997\nEpoch 214/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5343 - tp: 188.0000 - fp: 47.0000 - tn: 397.0000 - fn: 80.0000 - accuracy: 0.8216 - precision: 0.8000 - recall: 0.7015 - auc: 0.8613 - val_loss: 0.5117 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8995\nEpoch 215/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5168 - tp: 199.0000 - fp: 57.0000 - tn: 387.0000 - fn: 69.0000 - accuracy: 0.8230 - precision: 0.7773 - recall: 0.7425 - auc: 0.8678 - val_loss: 0.4993 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.9007\nEpoch 216/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5167 - tp: 200.0000 - fp: 39.0000 - tn: 405.0000 - fn: 68.0000 - accuracy: 0.8497 - precision: 0.8368 - recall: 0.7463 - auc: 0.8529 - val_loss: 0.4970 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8998\nEpoch 217/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5289 - tp: 192.0000 - fp: 45.0000 - tn: 399.0000 - fn: 76.0000 - accuracy: 0.8301 - precision: 0.8101 - recall: 0.7164 - auc: 0.8541 - val_loss: 0.5113 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8994\nEpoch 218/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5442 - tp: 187.0000 - fp: 36.0000 - tn: 408.0000 - fn: 81.0000 - accuracy: 0.8357 - precision: 0.8386 - recall: 0.6978 - auc: 0.8474 - val_loss: 0.4935 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8996\nEpoch 219/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5250 - tp: 189.0000 - fp: 44.0000 - tn: 400.0000 - fn: 79.0000 - accuracy: 0.8272 - precision: 0.8112 - recall: 0.7052 - auc: 0.8559 - val_loss: 0.5151 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8986\nEpoch 220/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5411 - tp: 190.0000 - fp: 54.0000 - tn: 390.0000 - fn: 78.0000 - accuracy: 0.8146 - precision: 0.7787 - recall: 0.7090 - auc: 0.8424 - val_loss: 0.5052 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9008\nEpoch 221/500\n22/23 [===========================>..] - ETA: 0s - loss: 0.5376 - tp: 186.0000 - fp: 48.0000 - tn: 392.0000 - fn: 78.0000 - accuracy: 0.8210 - precision: 0.7949 - recall: 0.7045 - auc: 0.84523/23 [==============================] - 0s 15ms/step - loss: 0.5394 - tp: 188.0000 - fp: 48.0000 - tn: 396.0000 - fn: 80.0000 - accuracy: 0.8202 - precision: 0.7966 - recall: 0.7015 - auc: 0.8450 - val_loss: 0.5247 - val_tp: 63.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 11.0000 - val_accuracy: 0.8380 - val_precision: 0.7778 - val_recall: 0.8514 - val_auc: 0.8988\nEpoch 222/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5353 - tp: 195.0000 - fp: 57.0000 - tn: 387.0000 - fn: 73.0000 - accuracy: 0.8174 - precision: 0.7738 - recall: 0.7276 - auc: 0.8542 - val_loss: 0.5060 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8986\nEpoch 223/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5181 - tp: 198.0000 - fp: 47.0000 - tn: 397.0000 - fn: 70.0000 - accuracy: 0.8357 - precision: 0.8082 - recall: 0.7388 - auc: 0.8631 - val_loss: 0.5127 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8968\nEpoch 224/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5166 - tp: 195.0000 - fp: 43.0000 - tn: 401.0000 - fn: 73.0000 - accuracy: 0.8371 - precision: 0.8193 - recall: 0.7276 - auc: 0.8579 - val_loss: 0.5106 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9019\nEpoch 225/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5438 - tp: 185.0000 - fp: 41.0000 - tn: 403.0000 - fn: 83.0000 - accuracy: 0.8258 - precision: 0.8186 - recall: 0.6903 - auc: 0.8555 - val_loss: 0.5018 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.9024\nEpoch 226/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5308 - tp: 194.0000 - fp: 57.0000 - tn: 387.0000 - fn: 74.0000 - accuracy: 0.8160 - precision: 0.7729 - recall: 0.7239 - auc: 0.8564 - val_loss: 0.5099 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9008\nEpoch 227/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5292 - tp: 182.0000 - fp: 31.0000 - tn: 413.0000 - fn: 86.0000 - accuracy: 0.8357 - precision: 0.8545 - recall: 0.6791 - auc: 0.8531 - val_loss: 0.5119 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9003\nEpoch 228/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5191 - tp: 190.0000 - fp: 45.0000 - tn: 399.0000 - fn: 78.0000 - accuracy: 0.8272 - precision: 0.8085 - recall: 0.7090 - auc: 0.8588 - val_loss: 0.5142 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8973\nEpoch 229/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5304 - tp: 194.0000 - fp: 55.0000 - tn: 389.0000 - fn: 74.0000 - accuracy: 0.8188 - precision: 0.7791 - recall: 0.7239 - auc: 0.8536 - val_loss: 0.4964 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9003\nEpoch 230/500\n23/23 [==============================] - ETA: 0s - loss: 0.5381 - tp: 186.0000 - fp: 44.0000 - tn: 400.0000 - fn: 82.0000 - accuracy: 0.8230 - precision: 0.8087 - recall: 0.6940 - auc: 0.83623/23 [==============================] - 0s 16ms/step - loss: 0.5381 - tp: 186.0000 - fp: 44.0000 - tn: 400.0000 - fn: 82.0000 - accuracy: 0.8230 - precision: 0.8087 - recall: 0.6940 - auc: 0.8368 - val_loss: 0.4964 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8972\nEpoch 231/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5162 - tp: 191.0000 - fp: 38.0000 - tn: 406.0000 - fn: 77.0000 - accuracy: 0.8385 - precision: 0.8341 - recall: 0.7127 - auc: 0.8565 - val_loss: 0.5190 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9007\nEpoch 232/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5196 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8638 - val_loss: 0.5183 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8995\nEpoch 233/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5368 - tp: 186.0000 - fp: 47.0000 - tn: 397.0000 - fn: 82.0000 - accuracy: 0.8188 - precision: 0.7983 - recall: 0.6940 - auc: 0.8486 - val_loss: 0.5071 - val_tp: 58.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 16.0000 - val_accuracy: 0.8436 - val_precision: 0.8286 - val_recall: 0.7838 - val_auc: 0.9028\nEpoch 234/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5144 - tp: 194.0000 - fp: 40.0000 - tn: 404.0000 - fn: 74.0000 - accuracy: 0.8399 - precision: 0.8291 - recall: 0.7239 - auc: 0.8542 - val_loss: 0.5245 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8992\nEpoch 235/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5092 - tp: 200.0000 - fp: 46.0000 - tn: 398.0000 - fn: 68.0000 - accuracy: 0.8399 - precision: 0.8130 - recall: 0.7463 - auc: 0.8706 - val_loss: 0.5092 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9031\nEpoch 236/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5104 - tp: 192.0000 - fp: 41.0000 - tn: 403.0000 - fn: 76.0000 - accuracy: 0.8357 - precision: 0.8240 - recall: 0.7164 - auc: 0.8672 - val_loss: 0.5132 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9030\nEpoch 237/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5271 - tp: 189.0000 - fp: 47.0000 - tn: 397.0000 - fn: 79.0000 - accuracy: 0.8230 - precision: 0.8008 - recall: 0.7052 - auc: 0.8602 - val_loss: 0.5050 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8995\nEpoch 238/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5465 - tp: 198.0000 - fp: 58.0000 - tn: 386.0000 - fn: 70.0000 - accuracy: 0.8202 - precision: 0.7734 - recall: 0.7388 - auc: 0.8429 - val_loss: 0.5076 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8968\nEpoch 239/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5193 - tp: 195.0000 - fp: 43.0000 - tn: 401.0000 - fn: 73.0000 - accuracy: 0.8371 - precision: 0.8193 - recall: 0.7276 - auc: 0.8626 - val_loss: 0.5146 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9001\nEpoch 240/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5232 - tp: 186.0000 - fp: 42.0000 - tn: 402.0000 - fn: 82.0000 - accuracy: 0.8258 - precision: 0.8158 - recall: 0.6940 - auc: 0.8609 - val_loss: 0.5007 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8985\nEpoch 241/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5318 - tp: 194.0000 - fp: 49.0000 - tn: 395.0000 - fn: 74.0000 - accuracy: 0.8272 - precision: 0.7984 - recall: 0.7239 - auc: 0.8553 - val_loss: 0.5013 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9028\nEpoch 242/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5211 - tp: 185.0000 - fp: 34.0000 - tn: 410.0000 - fn: 83.0000 - accuracy: 0.8357 - precision: 0.8447 - recall: 0.6903 - auc: 0.8582 - val_loss: 0.5102 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.9005\nEpoch 243/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5029 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8754 - val_loss: 0.5067 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8993\nEpoch 244/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5338 - tp: 191.0000 - fp: 53.0000 - tn: 391.0000 - fn: 77.0000 - accuracy: 0.8174 - precision: 0.7828 - recall: 0.7127 - auc: 0.8551 - val_loss: 0.5007 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8997\nEpoch 245/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5237 - tp: 199.0000 - fp: 58.0000 - tn: 386.0000 - fn: 69.0000 - accuracy: 0.8216 - precision: 0.7743 - recall: 0.7425 - auc: 0.8670 - val_loss: 0.4859 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9048\nEpoch 246/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5206 - tp: 190.0000 - fp: 37.0000 - tn: 407.0000 - fn: 78.0000 - accuracy: 0.8385 - precision: 0.8370 - recall: 0.7090 - auc: 0.8548 - val_loss: 0.5083 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9015\nEpoch 247/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5149 - tp: 193.0000 - fp: 44.0000 - tn: 400.0000 - fn: 75.0000 - accuracy: 0.8329 - precision: 0.8143 - recall: 0.7201 - auc: 0.8622 - val_loss: 0.5330 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8976\nEpoch 248/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5368 - tp: 191.0000 - fp: 42.0000 - tn: 402.0000 - fn: 77.0000 - accuracy: 0.8329 - precision: 0.8197 - recall: 0.7127 - auc: 0.8508 - val_loss: 0.5010 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9034\nEpoch 249/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5287 - tp: 192.0000 - fp: 40.0000 - tn: 404.0000 - fn: 76.0000 - accuracy: 0.8371 - precision: 0.8276 - recall: 0.7164 - auc: 0.8516 - val_loss: 0.5230 - val_tp: 62.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 12.0000 - val_accuracy: 0.8324 - val_precision: 0.7750 - val_recall: 0.8378 - val_auc: 0.9005\nEpoch 250/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5205 - tp: 191.0000 - fp: 56.0000 - tn: 388.0000 - fn: 77.0000 - accuracy: 0.8132 - precision: 0.7733 - recall: 0.7127 - auc: 0.8596 - val_loss: 0.4972 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9030\nEpoch 251/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5098 - tp: 193.0000 - fp: 41.0000 - tn: 403.0000 - fn: 75.0000 - accuracy: 0.8371 - precision: 0.8248 - recall: 0.7201 - auc: 0.8623 - val_loss: 0.5149 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.9003\nEpoch 252/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5158 - tp: 195.0000 - fp: 48.0000 - tn: 396.0000 - fn: 73.0000 - accuracy: 0.8301 - precision: 0.8025 - recall: 0.7276 - auc: 0.8556 - val_loss: 0.4971 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9000\nEpoch 253/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5006 - tp: 193.0000 - fp: 40.0000 - tn: 404.0000 - fn: 75.0000 - accuracy: 0.8385 - precision: 0.8283 - recall: 0.7201 - auc: 0.8773 - val_loss: 0.5192 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8990\nEpoch 254/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5365 - tp: 187.0000 - fp: 48.0000 - tn: 396.0000 - fn: 81.0000 - accuracy: 0.8188 - precision: 0.7957 - recall: 0.6978 - auc: 0.8453 - val_loss: 0.5111 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8980\nEpoch 255/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5061 - tp: 192.0000 - fp: 41.0000 - tn: 403.0000 - fn: 76.0000 - accuracy: 0.8357 - precision: 0.8240 - recall: 0.7164 - auc: 0.8725 - val_loss: 0.5188 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8974\nEpoch 256/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5182 - tp: 198.0000 - fp: 41.0000 - tn: 403.0000 - fn: 70.0000 - accuracy: 0.8441 - precision: 0.8285 - recall: 0.7388 - auc: 0.8638 - val_loss: 0.5338 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8965\nEpoch 257/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5260 - tp: 191.0000 - fp: 56.0000 - tn: 388.0000 - fn: 77.0000 - accuracy: 0.8132 - precision: 0.7733 - recall: 0.7127 - auc: 0.8533 - val_loss: 0.5089 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8964\nEpoch 258/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5223 - tp: 191.0000 - fp: 43.0000 - tn: 401.0000 - fn: 77.0000 - accuracy: 0.8315 - precision: 0.8162 - recall: 0.7127 - auc: 0.8527 - val_loss: 0.5238 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8996\nEpoch 259/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5257 - tp: 193.0000 - fp: 43.0000 - tn: 401.0000 - fn: 75.0000 - accuracy: 0.8343 - precision: 0.8178 - recall: 0.7201 - auc: 0.8490 - val_loss: 0.5198 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8999\nEpoch 260/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5056 - tp: 194.0000 - fp: 49.0000 - tn: 395.0000 - fn: 74.0000 - accuracy: 0.8272 - precision: 0.7984 - recall: 0.7239 - auc: 0.8596 - val_loss: 0.5126 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9011\nEpoch 261/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5253 - tp: 191.0000 - fp: 36.0000 - tn: 408.0000 - fn: 77.0000 - accuracy: 0.8413 - precision: 0.8414 - recall: 0.7127 - auc: 0.8554 - val_loss: 0.4985 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9052\nEpoch 262/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5020 - tp: 206.0000 - fp: 48.0000 - tn: 396.0000 - fn: 62.0000 - accuracy: 0.8455 - precision: 0.8110 - recall: 0.7687 - auc: 0.8626 - val_loss: 0.5151 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9022\nEpoch 263/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5100 - tp: 189.0000 - fp: 42.0000 - tn: 402.0000 - fn: 79.0000 - accuracy: 0.8301 - precision: 0.8182 - recall: 0.7052 - auc: 0.8651 - val_loss: 0.5055 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8995\nEpoch 264/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5074 - tp: 190.0000 - fp: 38.0000 - tn: 406.0000 - fn: 78.0000 - accuracy: 0.8371 - precision: 0.8333 - recall: 0.7090 - auc: 0.8634 - val_loss: 0.5189 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9006\nEpoch 265/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5494 - tp: 184.0000 - fp: 47.0000 - tn: 397.0000 - fn: 84.0000 - accuracy: 0.8160 - precision: 0.7965 - recall: 0.6866 - auc: 0.8379 - val_loss: 0.4967 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9044\nEpoch 266/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5145 - tp: 187.0000 - fp: 33.0000 - tn: 411.0000 - fn: 81.0000 - accuracy: 0.8399 - precision: 0.8500 - recall: 0.6978 - auc: 0.8583 - val_loss: 0.5004 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9021\nEpoch 267/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5285 - tp: 195.0000 - fp: 50.0000 - tn: 394.0000 - fn: 73.0000 - accuracy: 0.8272 - precision: 0.7959 - recall: 0.7276 - auc: 0.8580 - val_loss: 0.4975 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9042\nEpoch 268/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5078 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8689 - val_loss: 0.5140 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9006\nEpoch 269/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5201 - tp: 186.0000 - fp: 35.0000 - tn: 409.0000 - fn: 82.0000 - accuracy: 0.8357 - precision: 0.8416 - recall: 0.6940 - auc: 0.8560 - val_loss: 0.4982 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9053\nEpoch 270/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5361 - tp: 184.0000 - fp: 45.0000 - tn: 399.0000 - fn: 84.0000 - accuracy: 0.8188 - precision: 0.8035 - recall: 0.6866 - auc: 0.8433 - val_loss: 0.5120 - val_tp: 63.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 11.0000 - val_accuracy: 0.8436 - val_precision: 0.7875 - val_recall: 0.8514 - val_auc: 0.9030\nEpoch 271/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5108 - tp: 200.0000 - fp: 46.0000 - tn: 398.0000 - fn: 68.0000 - accuracy: 0.8399 - precision: 0.8130 - recall: 0.7463 - auc: 0.8619 - val_loss: 0.5041 - val_tp: 62.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 12.0000 - val_accuracy: 0.8492 - val_precision: 0.8052 - val_recall: 0.8378 - val_auc: 0.9024\nEpoch 272/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5282 - tp: 197.0000 - fp: 50.0000 - tn: 394.0000 - fn: 71.0000 - accuracy: 0.8301 - precision: 0.7976 - recall: 0.7351 - auc: 0.8514 - val_loss: 0.4950 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9046\nEpoch 273/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5366 - tp: 190.0000 - fp: 56.0000 - tn: 388.0000 - fn: 78.0000 - accuracy: 0.8118 - precision: 0.7724 - recall: 0.7090 - auc: 0.8434 - val_loss: 0.5004 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9001\nEpoch 274/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5317 - tp: 196.0000 - fp: 42.0000 - tn: 402.0000 - fn: 72.0000 - accuracy: 0.8399 - precision: 0.8235 - recall: 0.7313 - auc: 0.8505 - val_loss: 0.4966 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9012\nEpoch 275/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5192 - tp: 183.0000 - fp: 38.0000 - tn: 406.0000 - fn: 85.0000 - accuracy: 0.8272 - precision: 0.8281 - recall: 0.6828 - auc: 0.8590 - val_loss: 0.5300 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8957\nEpoch 276/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5250 - tp: 193.0000 - fp: 42.0000 - tn: 402.0000 - fn: 75.0000 - accuracy: 0.8357 - precision: 0.8213 - recall: 0.7201 - auc: 0.8583 - val_loss: 0.5041 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8983\nEpoch 277/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5130 - tp: 190.0000 - fp: 39.0000 - tn: 405.0000 - fn: 78.0000 - accuracy: 0.8357 - precision: 0.8297 - recall: 0.7090 - auc: 0.8552 - val_loss: 0.5037 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9010\nEpoch 278/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5285 - tp: 192.0000 - fp: 38.0000 - tn: 406.0000 - fn: 76.0000 - accuracy: 0.8399 - precision: 0.8348 - recall: 0.7164 - auc: 0.8591 - val_loss: 0.5031 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9015\nEpoch 279/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5261 - tp: 191.0000 - fp: 46.0000 - tn: 398.0000 - fn: 77.0000 - accuracy: 0.8272 - precision: 0.8059 - recall: 0.7127 - auc: 0.8499 - val_loss: 0.4915 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8997\nEpoch 280/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5138 - tp: 189.0000 - fp: 41.0000 - tn: 403.0000 - fn: 79.0000 - accuracy: 0.8315 - precision: 0.8217 - recall: 0.7052 - auc: 0.8656 - val_loss: 0.4999 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9035\nEpoch 281/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5151 - tp: 194.0000 - fp: 42.0000 - tn: 402.0000 - fn: 74.0000 - accuracy: 0.8371 - precision: 0.8220 - recall: 0.7239 - auc: 0.8601 - val_loss: 0.4990 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9013\nEpoch 282/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5350 - tp: 191.0000 - fp: 62.0000 - tn: 382.0000 - fn: 77.0000 - accuracy: 0.8048 - precision: 0.7549 - recall: 0.7127 - auc: 0.8487 - val_loss: 0.4951 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.9027\nEpoch 283/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5069 - tp: 192.0000 - fp: 41.0000 - tn: 403.0000 - fn: 76.0000 - accuracy: 0.8357 - precision: 0.8240 - recall: 0.7164 - auc: 0.8568 - val_loss: 0.5255 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8994\nEpoch 284/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5016 - tp: 192.0000 - fp: 44.0000 - tn: 400.0000 - fn: 76.0000 - accuracy: 0.8315 - precision: 0.8136 - recall: 0.7164 - auc: 0.8686 - val_loss: 0.5157 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8978\nEpoch 285/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5164 - tp: 189.0000 - fp: 43.0000 - tn: 401.0000 - fn: 79.0000 - accuracy: 0.8287 - precision: 0.8147 - recall: 0.7052 - auc: 0.8618 - val_loss: 0.5128 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9004\nEpoch 286/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5418 - tp: 197.0000 - fp: 52.0000 - tn: 392.0000 - fn: 71.0000 - accuracy: 0.8272 - precision: 0.7912 - recall: 0.7351 - auc: 0.8475 - val_loss: 0.4942 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9057\nEpoch 287/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5020 - tp: 195.0000 - fp: 45.0000 - tn: 399.0000 - fn: 73.0000 - accuracy: 0.8343 - precision: 0.8125 - recall: 0.7276 - auc: 0.8657 - val_loss: 0.5024 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9018\nEpoch 288/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5435 - tp: 189.0000 - fp: 42.0000 - tn: 402.0000 - fn: 79.0000 - accuracy: 0.8301 - precision: 0.8182 - recall: 0.7052 - auc: 0.8438 - val_loss: 0.5013 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9012\nEpoch 289/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5103 - tp: 190.0000 - fp: 43.0000 - tn: 401.0000 - fn: 78.0000 - accuracy: 0.8301 - precision: 0.8155 - recall: 0.7090 - auc: 0.8611 - val_loss: 0.5115 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9029\nEpoch 290/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5382 - tp: 187.0000 - fp: 46.0000 - tn: 398.0000 - fn: 81.0000 - accuracy: 0.8216 - precision: 0.8026 - recall: 0.6978 - auc: 0.8497 - val_loss: 0.4952 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9039\nEpoch 291/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5117 - tp: 201.0000 - fp: 48.0000 - tn: 396.0000 - fn: 67.0000 - accuracy: 0.8385 - precision: 0.8072 - recall: 0.7500 - auc: 0.8600 - val_loss: 0.4967 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9049\nEpoch 292/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5268 - tp: 190.0000 - fp: 36.0000 - tn: 408.0000 - fn: 78.0000 - accuracy: 0.8399 - precision: 0.8407 - recall: 0.7090 - auc: 0.8483 - val_loss: 0.5003 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9059\nEpoch 293/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5243 - tp: 196.0000 - fp: 49.0000 - tn: 395.0000 - fn: 72.0000 - accuracy: 0.8301 - precision: 0.8000 - recall: 0.7313 - auc: 0.8511 - val_loss: 0.5026 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9032\nEpoch 294/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5119 - tp: 191.0000 - fp: 36.0000 - tn: 408.0000 - fn: 77.0000 - accuracy: 0.8413 - precision: 0.8414 - recall: 0.7127 - auc: 0.8674 - val_loss: 0.4939 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9064\nEpoch 295/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5109 - tp: 194.0000 - fp: 45.0000 - tn: 399.0000 - fn: 74.0000 - accuracy: 0.8329 - precision: 0.8117 - recall: 0.7239 - auc: 0.8566 - val_loss: 0.4987 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9051\nEpoch 296/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.4976 - tp: 195.0000 - fp: 35.0000 - tn: 409.0000 - fn: 73.0000 - accuracy: 0.8483 - precision: 0.8478 - recall: 0.7276 - auc: 0.8738 - val_loss: 0.4891 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9038\nEpoch 297/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5096 - tp: 193.0000 - fp: 51.0000 - tn: 393.0000 - fn: 75.0000 - accuracy: 0.8230 - precision: 0.7910 - recall: 0.7201 - auc: 0.8625 - val_loss: 0.5036 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9030\nEpoch 298/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5147 - tp: 195.0000 - fp: 45.0000 - tn: 399.0000 - fn: 73.0000 - accuracy: 0.8343 - precision: 0.8125 - recall: 0.7276 - auc: 0.8605 - val_loss: 0.5011 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9023\nEpoch 299/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5081 - tp: 200.0000 - fp: 42.0000 - tn: 402.0000 - fn: 68.0000 - accuracy: 0.8455 - precision: 0.8264 - recall: 0.7463 - auc: 0.8624 - val_loss: 0.5016 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9005\nEpoch 300/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5270 - tp: 191.0000 - fp: 42.0000 - tn: 402.0000 - fn: 77.0000 - accuracy: 0.8329 - precision: 0.8197 - recall: 0.7127 - auc: 0.8508 - val_loss: 0.5246 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8994\nEpoch 301/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5132 - tp: 193.0000 - fp: 41.0000 - tn: 403.0000 - fn: 75.0000 - accuracy: 0.8371 - precision: 0.8248 - recall: 0.7201 - auc: 0.8656 - val_loss: 0.5033 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9021\nEpoch 302/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5058 - tp: 204.0000 - fp: 46.0000 - tn: 398.0000 - fn: 64.0000 - accuracy: 0.8455 - precision: 0.8160 - recall: 0.7612 - auc: 0.8759 - val_loss: 0.4842 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9001\nEpoch 303/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5140 - tp: 193.0000 - fp: 42.0000 - tn: 402.0000 - fn: 75.0000 - accuracy: 0.8357 - precision: 0.8213 - recall: 0.7201 - auc: 0.8606 - val_loss: 0.4865 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9045\nEpoch 304/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5244 - tp: 188.0000 - fp: 37.0000 - tn: 407.0000 - fn: 80.0000 - accuracy: 0.8357 - precision: 0.8356 - recall: 0.7015 - auc: 0.8579 - val_loss: 0.4911 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9028\nEpoch 305/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5221 - tp: 198.0000 - fp: 59.0000 - tn: 385.0000 - fn: 70.0000 - accuracy: 0.8188 - precision: 0.7704 - recall: 0.7388 - auc: 0.8489 - val_loss: 0.4896 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9053\nEpoch 306/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5255 - tp: 189.0000 - fp: 36.0000 - tn: 408.0000 - fn: 79.0000 - accuracy: 0.8385 - precision: 0.8400 - recall: 0.7052 - auc: 0.8555 - val_loss: 0.5057 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9061\nEpoch 307/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5231 - tp: 195.0000 - fp: 42.0000 - tn: 402.0000 - fn: 73.0000 - accuracy: 0.8385 - precision: 0.8228 - recall: 0.7276 - auc: 0.8546 - val_loss: 0.4925 - val_tp: 57.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 17.0000 - val_accuracy: 0.8380 - val_precision: 0.8261 - val_recall: 0.7703 - val_auc: 0.9023\nEpoch 308/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5171 - tp: 191.0000 - fp: 37.0000 - tn: 407.0000 - fn: 77.0000 - accuracy: 0.8399 - precision: 0.8377 - recall: 0.7127 - auc: 0.8518 - val_loss: 0.4951 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9044\nEpoch 309/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5227 - tp: 195.0000 - fp: 47.0000 - tn: 397.0000 - fn: 73.0000 - accuracy: 0.8315 - precision: 0.8058 - recall: 0.7276 - auc: 0.8566 - val_loss: 0.4898 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9055\nEpoch 310/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5047 - tp: 196.0000 - fp: 33.0000 - tn: 411.0000 - fn: 72.0000 - accuracy: 0.8525 - precision: 0.8559 - recall: 0.7313 - auc: 0.8642 - val_loss: 0.5003 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9055\nEpoch 311/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5177 - tp: 198.0000 - fp: 42.0000 - tn: 402.0000 - fn: 70.0000 - accuracy: 0.8427 - precision: 0.8250 - recall: 0.7388 - auc: 0.8669 - val_loss: 0.4888 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.9031\nEpoch 312/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5152 - tp: 193.0000 - fp: 33.0000 - tn: 411.0000 - fn: 75.0000 - accuracy: 0.8483 - precision: 0.8540 - recall: 0.7201 - auc: 0.8546 - val_loss: 0.5148 - val_tp: 60.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 14.0000 - val_accuracy: 0.8436 - val_precision: 0.8108 - val_recall: 0.8108 - val_auc: 0.8998\nEpoch 313/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5318 - tp: 188.0000 - fp: 44.0000 - tn: 400.0000 - fn: 80.0000 - accuracy: 0.8258 - precision: 0.8103 - recall: 0.7015 - auc: 0.8580 - val_loss: 0.5201 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8964\nEpoch 314/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5026 - tp: 192.0000 - fp: 40.0000 - tn: 404.0000 - fn: 76.0000 - accuracy: 0.8371 - precision: 0.8276 - recall: 0.7164 - auc: 0.8676 - val_loss: 0.5074 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.9042\nEpoch 315/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5296 - tp: 189.0000 - fp: 47.0000 - tn: 397.0000 - fn: 79.0000 - accuracy: 0.8230 - precision: 0.8008 - recall: 0.7052 - auc: 0.8508 - val_loss: 0.4967 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9003\nEpoch 316/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5156 - tp: 201.0000 - fp: 49.0000 - tn: 395.0000 - fn: 67.0000 - accuracy: 0.8371 - precision: 0.8040 - recall: 0.7500 - auc: 0.8594 - val_loss: 0.4936 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9052\nEpoch 317/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5147 - tp: 188.0000 - fp: 39.0000 - tn: 405.0000 - fn: 80.0000 - accuracy: 0.8329 - precision: 0.8282 - recall: 0.7015 - auc: 0.8698 - val_loss: 0.4924 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9057\nEpoch 318/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5069 - tp: 199.0000 - fp: 48.0000 - tn: 396.0000 - fn: 69.0000 - accuracy: 0.8357 - precision: 0.8057 - recall: 0.7425 - auc: 0.8593 - val_loss: 0.4967 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.9028\nEpoch 319/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.4967 - tp: 189.0000 - fp: 28.0000 - tn: 416.0000 - fn: 79.0000 - accuracy: 0.8497 - precision: 0.8710 - recall: 0.7052 - auc: 0.8704 - val_loss: 0.5392 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.9000\nEpoch 320/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5267 - tp: 194.0000 - fp: 49.0000 - tn: 395.0000 - fn: 74.0000 - accuracy: 0.8272 - precision: 0.7984 - recall: 0.7239 - auc: 0.8528 - val_loss: 0.5050 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9042\nEpoch 321/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5181 - tp: 185.0000 - fp: 46.0000 - tn: 398.0000 - fn: 83.0000 - accuracy: 0.8188 - precision: 0.8009 - recall: 0.6903 - auc: 0.8610 - val_loss: 0.4971 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9011\nEpoch 322/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5251 - tp: 191.0000 - fp: 43.0000 - tn: 401.0000 - fn: 77.0000 - accuracy: 0.8315 - precision: 0.8162 - recall: 0.7127 - auc: 0.8603 - val_loss: 0.4937 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9030\nEpoch 323/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5174 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8577 - val_loss: 0.4959 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9024\nEpoch 324/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5030 - tp: 192.0000 - fp: 46.0000 - tn: 398.0000 - fn: 76.0000 - accuracy: 0.8287 - precision: 0.8067 - recall: 0.7164 - auc: 0.8768 - val_loss: 0.5051 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.9021\nEpoch 325/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5156 - tp: 191.0000 - fp: 43.0000 - tn: 401.0000 - fn: 77.0000 - accuracy: 0.8315 - precision: 0.8162 - recall: 0.7127 - auc: 0.8630 - val_loss: 0.4965 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.9015\nEpoch 326/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5188 - tp: 187.0000 - fp: 43.0000 - tn: 401.0000 - fn: 81.0000 - accuracy: 0.8258 - precision: 0.8130 - recall: 0.6978 - auc: 0.8650 - val_loss: 0.5111 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.9021\nEpoch 327/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5146 - tp: 197.0000 - fp: 51.0000 - tn: 393.0000 - fn: 71.0000 - accuracy: 0.8287 - precision: 0.7944 - recall: 0.7351 - auc: 0.8612 - val_loss: 0.5027 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.9032\nEpoch 328/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5151 - tp: 197.0000 - fp: 53.0000 - tn: 391.0000 - fn: 71.0000 - accuracy: 0.8258 - precision: 0.7880 - recall: 0.7351 - auc: 0.8577 - val_loss: 0.4951 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9030\nEpoch 329/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5290 - tp: 185.0000 - fp: 42.0000 - tn: 402.0000 - fn: 83.0000 - accuracy: 0.8244 - precision: 0.8150 - recall: 0.6903 - auc: 0.8457 - val_loss: 0.5027 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9007\nEpoch 330/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5115 - tp: 194.0000 - fp: 47.0000 - tn: 397.0000 - fn: 74.0000 - accuracy: 0.8301 - precision: 0.8050 - recall: 0.7239 - auc: 0.8622 - val_loss: 0.5051 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9008\nEpoch 331/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5216 - tp: 200.0000 - fp: 47.0000 - tn: 397.0000 - fn: 68.0000 - accuracy: 0.8385 - precision: 0.8097 - recall: 0.7463 - auc: 0.8520 - val_loss: 0.4975 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.9009\nEpoch 332/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5113 - tp: 191.0000 - fp: 38.0000 - tn: 406.0000 - fn: 77.0000 - accuracy: 0.8385 - precision: 0.8341 - recall: 0.7127 - auc: 0.8535 - val_loss: 0.5065 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9026\nEpoch 333/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5239 - tp: 194.0000 - fp: 51.0000 - tn: 393.0000 - fn: 74.0000 - accuracy: 0.8244 - precision: 0.7918 - recall: 0.7239 - auc: 0.8582 - val_loss: 0.5117 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.9006\nEpoch 334/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5145 - tp: 196.0000 - fp: 45.0000 - tn: 399.0000 - fn: 72.0000 - accuracy: 0.8357 - precision: 0.8133 - recall: 0.7313 - auc: 0.8611 - val_loss: 0.5013 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.9037\nEpoch 335/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5258 - tp: 187.0000 - fp: 40.0000 - tn: 404.0000 - fn: 81.0000 - accuracy: 0.8301 - precision: 0.8238 - recall: 0.6978 - auc: 0.8563 - val_loss: 0.4973 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9022\nEpoch 336/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5168 - tp: 197.0000 - fp: 49.0000 - tn: 395.0000 - fn: 71.0000 - accuracy: 0.8315 - precision: 0.8008 - recall: 0.7351 - auc: 0.8559 - val_loss: 0.4980 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9022\nEpoch 337/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5377 - tp: 190.0000 - fp: 48.0000 - tn: 396.0000 - fn: 78.0000 - accuracy: 0.8230 - precision: 0.7983 - recall: 0.7090 - auc: 0.8441 - val_loss: 0.4952 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9025\nEpoch 338/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5119 - tp: 195.0000 - fp: 43.0000 - tn: 401.0000 - fn: 73.0000 - accuracy: 0.8371 - precision: 0.8193 - recall: 0.7276 - auc: 0.8608 - val_loss: 0.4978 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9042\nEpoch 339/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5157 - tp: 192.0000 - fp: 38.0000 - tn: 406.0000 - fn: 76.0000 - accuracy: 0.8399 - precision: 0.8348 - recall: 0.7164 - auc: 0.8558 - val_loss: 0.4974 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9037\nEpoch 340/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5179 - tp: 185.0000 - fp: 36.0000 - tn: 408.0000 - fn: 83.0000 - accuracy: 0.8329 - precision: 0.8371 - recall: 0.6903 - auc: 0.8552 - val_loss: 0.5020 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9044\nEpoch 341/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5145 - tp: 197.0000 - fp: 50.0000 - tn: 394.0000 - fn: 71.0000 - accuracy: 0.8301 - precision: 0.7976 - recall: 0.7351 - auc: 0.8648 - val_loss: 0.4931 - val_tp: 59.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 15.0000 - val_accuracy: 0.8436 - val_precision: 0.8194 - val_recall: 0.7973 - val_auc: 0.9046\nEpoch 342/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5225 - tp: 196.0000 - fp: 37.0000 - tn: 407.0000 - fn: 72.0000 - accuracy: 0.8469 - precision: 0.8412 - recall: 0.7313 - auc: 0.8499 - val_loss: 0.4847 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9077\nEpoch 343/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5153 - tp: 197.0000 - fp: 42.0000 - tn: 402.0000 - fn: 71.0000 - accuracy: 0.8413 - precision: 0.8243 - recall: 0.7351 - auc: 0.8583 - val_loss: 0.4982 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9033\nEpoch 344/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5227 - tp: 185.0000 - fp: 37.0000 - tn: 407.0000 - fn: 83.0000 - accuracy: 0.8315 - precision: 0.8333 - recall: 0.6903 - auc: 0.8590 - val_loss: 0.5029 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9047\nEpoch 345/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5002 - tp: 186.0000 - fp: 29.0000 - tn: 415.0000 - fn: 82.0000 - accuracy: 0.8441 - precision: 0.8651 - recall: 0.6940 - auc: 0.8704 - val_loss: 0.4952 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9055\nEpoch 346/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5201 - tp: 193.0000 - fp: 43.0000 - tn: 401.0000 - fn: 75.0000 - accuracy: 0.8343 - precision: 0.8178 - recall: 0.7201 - auc: 0.8557 - val_loss: 0.4986 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9006\nEpoch 347/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5120 - tp: 189.0000 - fp: 42.0000 - tn: 402.0000 - fn: 79.0000 - accuracy: 0.8301 - precision: 0.8182 - recall: 0.7052 - auc: 0.8650 - val_loss: 0.4905 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9048\nEpoch 348/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5062 - tp: 197.0000 - fp: 56.0000 - tn: 388.0000 - fn: 71.0000 - accuracy: 0.8216 - precision: 0.7787 - recall: 0.7351 - auc: 0.8677 - val_loss: 0.4877 - val_tp: 60.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 14.0000 - val_accuracy: 0.8492 - val_precision: 0.8219 - val_recall: 0.8108 - val_auc: 0.9056\nEpoch 349/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5190 - tp: 191.0000 - fp: 42.0000 - tn: 402.0000 - fn: 77.0000 - accuracy: 0.8329 - precision: 0.8197 - recall: 0.7127 - auc: 0.8566 - val_loss: 0.5089 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9057\nEpoch 350/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5265 - tp: 193.0000 - fp: 49.0000 - tn: 395.0000 - fn: 75.0000 - accuracy: 0.8258 - precision: 0.7975 - recall: 0.7201 - auc: 0.8490 - val_loss: 0.4898 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9088\nEpoch 351/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5024 - tp: 200.0000 - fp: 43.0000 - tn: 401.0000 - fn: 68.0000 - accuracy: 0.8441 - precision: 0.8230 - recall: 0.7463 - auc: 0.8663 - val_loss: 0.5033 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9068\nEpoch 352/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5096 - tp: 200.0000 - fp: 43.0000 - tn: 401.0000 - fn: 68.0000 - accuracy: 0.8441 - precision: 0.8230 - recall: 0.7463 - auc: 0.8610 - val_loss: 0.5024 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9066\nEpoch 353/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.4981 - tp: 197.0000 - fp: 43.0000 - tn: 401.0000 - fn: 71.0000 - accuracy: 0.8399 - precision: 0.8208 - recall: 0.7351 - auc: 0.8727 - val_loss: 0.4988 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9058\nEpoch 354/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5135 - tp: 201.0000 - fp: 46.0000 - tn: 398.0000 - fn: 67.0000 - accuracy: 0.8413 - precision: 0.8138 - recall: 0.7500 - auc: 0.8651 - val_loss: 0.4990 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9051\nEpoch 355/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5031 - tp: 195.0000 - fp: 50.0000 - tn: 394.0000 - fn: 73.0000 - accuracy: 0.8272 - precision: 0.7959 - recall: 0.7276 - auc: 0.8693 - val_loss: 0.4928 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9059\nEpoch 356/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5006 - tp: 194.0000 - fp: 31.0000 - tn: 413.0000 - fn: 74.0000 - accuracy: 0.8525 - precision: 0.8622 - recall: 0.7239 - auc: 0.8714 - val_loss: 0.5018 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9016\nEpoch 357/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5190 - tp: 192.0000 - fp: 38.0000 - tn: 406.0000 - fn: 76.0000 - accuracy: 0.8399 - precision: 0.8348 - recall: 0.7164 - auc: 0.8642 - val_loss: 0.5017 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9042\nEpoch 358/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5348 - tp: 199.0000 - fp: 47.0000 - tn: 397.0000 - fn: 69.0000 - accuracy: 0.8371 - precision: 0.8089 - recall: 0.7425 - auc: 0.8509 - val_loss: 0.4805 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9103\nEpoch 359/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5072 - tp: 203.0000 - fp: 46.0000 - tn: 398.0000 - fn: 65.0000 - accuracy: 0.8441 - precision: 0.8153 - recall: 0.7575 - auc: 0.8610 - val_loss: 0.4958 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9037\nEpoch 360/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5247 - tp: 191.0000 - fp: 41.0000 - tn: 403.0000 - fn: 77.0000 - accuracy: 0.8343 - precision: 0.8233 - recall: 0.7127 - auc: 0.8535 - val_loss: 0.5054 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9037\nEpoch 361/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5333 - tp: 198.0000 - fp: 48.0000 - tn: 396.0000 - fn: 70.0000 - accuracy: 0.8343 - precision: 0.8049 - recall: 0.7388 - auc: 0.8524 - val_loss: 0.4975 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9004\nEpoch 362/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5069 - tp: 200.0000 - fp: 46.0000 - tn: 398.0000 - fn: 68.0000 - accuracy: 0.8399 - precision: 0.8130 - recall: 0.7463 - auc: 0.8648 - val_loss: 0.4947 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9055\nEpoch 363/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5281 - tp: 184.0000 - fp: 39.0000 - tn: 405.0000 - fn: 84.0000 - accuracy: 0.8272 - precision: 0.8251 - recall: 0.6866 - auc: 0.8568 - val_loss: 0.4855 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9062\nEpoch 364/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5274 - tp: 189.0000 - fp: 40.0000 - tn: 404.0000 - fn: 79.0000 - accuracy: 0.8329 - precision: 0.8253 - recall: 0.7052 - auc: 0.8544 - val_loss: 0.5108 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.9035\nEpoch 365/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5283 - tp: 196.0000 - fp: 48.0000 - tn: 396.0000 - fn: 72.0000 - accuracy: 0.8315 - precision: 0.8033 - recall: 0.7313 - auc: 0.8550 - val_loss: 0.4979 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9039\nEpoch 366/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5220 - tp: 185.0000 - fp: 36.0000 - tn: 408.0000 - fn: 83.0000 - accuracy: 0.8329 - precision: 0.8371 - recall: 0.6903 - auc: 0.8572 - val_loss: 0.5073 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9042\nEpoch 367/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4989 - tp: 196.0000 - fp: 45.0000 - tn: 399.0000 - fn: 72.0000 - accuracy: 0.8357 - precision: 0.8133 - recall: 0.7313 - auc: 0.8735 - val_loss: 0.4844 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9069\nEpoch 368/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5164 - tp: 190.0000 - fp: 42.0000 - tn: 402.0000 - fn: 78.0000 - accuracy: 0.8315 - precision: 0.8190 - recall: 0.7090 - auc: 0.8648 - val_loss: 0.5120 - val_tp: 62.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 12.0000 - val_accuracy: 0.8324 - val_precision: 0.7750 - val_recall: 0.8378 - val_auc: 0.9085\nEpoch 369/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5186 - tp: 196.0000 - fp: 46.0000 - tn: 398.0000 - fn: 72.0000 - accuracy: 0.8343 - precision: 0.8099 - recall: 0.7313 - auc: 0.8588 - val_loss: 0.4865 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9090\nEpoch 370/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5074 - tp: 197.0000 - fp: 49.0000 - tn: 395.0000 - fn: 71.0000 - accuracy: 0.8315 - precision: 0.8008 - recall: 0.7351 - auc: 0.8594 - val_loss: 0.4955 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9080\nEpoch 371/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5116 - tp: 190.0000 - fp: 42.0000 - tn: 402.0000 - fn: 78.0000 - accuracy: 0.8315 - precision: 0.8190 - recall: 0.7090 - auc: 0.8643 - val_loss: 0.4951 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.9069\nEpoch 372/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5192 - tp: 198.0000 - fp: 51.0000 - tn: 393.0000 - fn: 70.0000 - accuracy: 0.8301 - precision: 0.7952 - recall: 0.7388 - auc: 0.8491 - val_loss: 0.4747 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9097\nEpoch 373/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5113 - tp: 192.0000 - fp: 49.0000 - tn: 395.0000 - fn: 76.0000 - accuracy: 0.8244 - precision: 0.7967 - recall: 0.7164 - auc: 0.8599 - val_loss: 0.4801 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9053\nEpoch 374/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5113 - tp: 193.0000 - fp: 40.0000 - tn: 404.0000 - fn: 75.0000 - accuracy: 0.8385 - precision: 0.8283 - recall: 0.7201 - auc: 0.8642 - val_loss: 0.4940 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9060\nEpoch 375/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5023 - tp: 202.0000 - fp: 43.0000 - tn: 401.0000 - fn: 66.0000 - accuracy: 0.8469 - precision: 0.8245 - recall: 0.7537 - auc: 0.8695 - val_loss: 0.4957 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9048\nEpoch 376/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5044 - tp: 194.0000 - fp: 45.0000 - tn: 399.0000 - fn: 74.0000 - accuracy: 0.8329 - precision: 0.8117 - recall: 0.7239 - auc: 0.8636 - val_loss: 0.5035 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9041\nEpoch 377/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5169 - tp: 193.0000 - fp: 41.0000 - tn: 403.0000 - fn: 75.0000 - accuracy: 0.8371 - precision: 0.8248 - recall: 0.7201 - auc: 0.8540 - val_loss: 0.5001 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9062\nEpoch 378/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5145 - tp: 198.0000 - fp: 38.0000 - tn: 406.0000 - fn: 70.0000 - accuracy: 0.8483 - precision: 0.8390 - recall: 0.7388 - auc: 0.8641 - val_loss: 0.4919 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9071\nEpoch 379/500\n18/23 [======================>.......] - ETA: 0s - loss: 0.5375 - tp: 161.0000 - fp: 37.0000 - tn: 317.0000 - fn: 61.0000 - accuracy: 0.8299 - precision: 0.8131 - recall: 0.7252 - auc: 0.84423/23 [==============================] - 0s 13ms/step - loss: 0.5264 - tp: 195.0000 - fp: 43.0000 - tn: 401.0000 - fn: 73.0000 - accuracy: 0.8371 - precision: 0.8193 - recall: 0.7276 - auc: 0.8527 - val_loss: 0.4909 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9051\nEpoch 380/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5098 - tp: 199.0000 - fp: 53.0000 - tn: 391.0000 - fn: 69.0000 - accuracy: 0.8287 - precision: 0.7897 - recall: 0.7425 - auc: 0.8672 - val_loss: 0.4988 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9039\nEpoch 381/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.4993 - tp: 201.0000 - fp: 46.0000 - tn: 398.0000 - fn: 67.0000 - accuracy: 0.8413 - precision: 0.8138 - recall: 0.7500 - auc: 0.8723 - val_loss: 0.4895 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9069\nEpoch 382/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5229 - tp: 199.0000 - fp: 43.0000 - tn: 401.0000 - fn: 69.0000 - accuracy: 0.8427 - precision: 0.8223 - recall: 0.7425 - auc: 0.8552 - val_loss: 0.4792 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.9055\nEpoch 383/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5076 - tp: 203.0000 - fp: 39.0000 - tn: 405.0000 - fn: 65.0000 - accuracy: 0.8539 - precision: 0.8388 - recall: 0.7575 - auc: 0.8519 - val_loss: 0.4938 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9062\nEpoch 384/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.4990 - tp: 197.0000 - fp: 40.0000 - tn: 404.0000 - fn: 71.0000 - accuracy: 0.8441 - precision: 0.8312 - recall: 0.7351 - auc: 0.8721 - val_loss: 0.5351 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.8992\nEpoch 385/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5180 - tp: 193.0000 - fp: 45.0000 - tn: 399.0000 - fn: 75.0000 - accuracy: 0.8315 - precision: 0.8109 - recall: 0.7201 - auc: 0.8600 - val_loss: 0.4952 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9054\nEpoch 386/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5070 - tp: 197.0000 - fp: 54.0000 - tn: 390.0000 - fn: 71.0000 - accuracy: 0.8244 - precision: 0.7849 - recall: 0.7351 - auc: 0.8668 - val_loss: 0.5011 - val_tp: 58.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 16.0000 - val_accuracy: 0.8436 - val_precision: 0.8286 - val_recall: 0.7838 - val_auc: 0.9068\nEpoch 387/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5299 - tp: 190.0000 - fp: 39.0000 - tn: 405.0000 - fn: 78.0000 - accuracy: 0.8357 - precision: 0.8297 - recall: 0.7090 - auc: 0.8533 - val_loss: 0.4955 - val_tp: 61.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 13.0000 - val_accuracy: 0.8603 - val_precision: 0.8356 - val_recall: 0.8243 - val_auc: 0.9039\nEpoch 388/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5110 - tp: 196.0000 - fp: 43.0000 - tn: 401.0000 - fn: 72.0000 - accuracy: 0.8385 - precision: 0.8201 - recall: 0.7313 - auc: 0.8675 - val_loss: 0.4984 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9051\nEpoch 389/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5157 - tp: 197.0000 - fp: 42.0000 - tn: 402.0000 - fn: 71.0000 - accuracy: 0.8413 - precision: 0.8243 - recall: 0.7351 - auc: 0.8540 - val_loss: 0.5176 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9042\nEpoch 390/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5262 - tp: 201.0000 - fp: 58.0000 - tn: 386.0000 - fn: 67.0000 - accuracy: 0.8244 - precision: 0.7761 - recall: 0.7500 - auc: 0.8584 - val_loss: 0.4816 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.9080\nEpoch 391/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5155 - tp: 194.0000 - fp: 42.0000 - tn: 402.0000 - fn: 74.0000 - accuracy: 0.8371 - precision: 0.8220 - recall: 0.7239 - auc: 0.8610 - val_loss: 0.4911 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9072\nEpoch 392/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5186 - tp: 197.0000 - fp: 50.0000 - tn: 394.0000 - fn: 71.0000 - accuracy: 0.8301 - precision: 0.7976 - recall: 0.7351 - auc: 0.8446 - val_loss: 0.4911 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9055\nEpoch 393/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5174 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8643 - val_loss: 0.4959 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9035\nEpoch 394/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5172 - tp: 193.0000 - fp: 40.0000 - tn: 404.0000 - fn: 75.0000 - accuracy: 0.8385 - precision: 0.8283 - recall: 0.7201 - auc: 0.8581 - val_loss: 0.4981 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9052\nEpoch 395/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5416 - tp: 191.0000 - fp: 37.0000 - tn: 407.0000 - fn: 77.0000 - accuracy: 0.8399 - precision: 0.8377 - recall: 0.7127 - auc: 0.8363 - val_loss: 0.4985 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9066\nEpoch 396/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5186 - tp: 194.0000 - fp: 47.0000 - tn: 397.0000 - fn: 74.0000 - accuracy: 0.8301 - precision: 0.8050 - recall: 0.7239 - auc: 0.8661 - val_loss: 0.4929 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9052\nEpoch 397/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5096 - tp: 190.0000 - fp: 36.0000 - tn: 408.0000 - fn: 78.0000 - accuracy: 0.8399 - precision: 0.8407 - recall: 0.7090 - auc: 0.8593 - val_loss: 0.5154 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9044\nEpoch 398/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5153 - tp: 185.0000 - fp: 35.0000 - tn: 409.0000 - fn: 83.0000 - accuracy: 0.8343 - precision: 0.8409 - recall: 0.6903 - auc: 0.8621 - val_loss: 0.4935 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9053\nEpoch 399/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4865 - tp: 196.0000 - fp: 37.0000 - tn: 407.0000 - fn: 72.0000 - accuracy: 0.8469 - precision: 0.8412 - recall: 0.7313 - auc: 0.8745 - val_loss: 0.5006 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9038\nEpoch 400/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5087 - tp: 201.0000 - fp: 47.0000 - tn: 397.0000 - fn: 67.0000 - accuracy: 0.8399 - precision: 0.8105 - recall: 0.7500 - auc: 0.8603 - val_loss: 0.5083 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9035\nEpoch 401/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5122 - tp: 198.0000 - fp: 43.0000 - tn: 401.0000 - fn: 70.0000 - accuracy: 0.8413 - precision: 0.8216 - recall: 0.7388 - auc: 0.8612 - val_loss: 0.4927 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9039\nEpoch 402/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5033 - tp: 193.0000 - fp: 34.0000 - tn: 410.0000 - fn: 75.0000 - accuracy: 0.8469 - precision: 0.8502 - recall: 0.7201 - auc: 0.8745 - val_loss: 0.5197 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9050\nEpoch 403/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5024 - tp: 195.0000 - fp: 41.0000 - tn: 403.0000 - fn: 73.0000 - accuracy: 0.8399 - precision: 0.8263 - recall: 0.7276 - auc: 0.8679 - val_loss: 0.4901 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9051\nEpoch 404/500\n23/23 [==============================] - 0s 18ms/step - loss: 0.5189 - tp: 194.0000 - fp: 47.0000 - tn: 397.0000 - fn: 74.0000 - accuracy: 0.8301 - precision: 0.8050 - recall: 0.7239 - auc: 0.8604 - val_loss: 0.5140 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.9014\nEpoch 405/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5107 - tp: 195.0000 - fp: 46.0000 - tn: 398.0000 - fn: 73.0000 - accuracy: 0.8329 - precision: 0.8091 - recall: 0.7276 - auc: 0.8608 - val_loss: 0.4962 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9071\nEpoch 406/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5242 - tp: 191.0000 - fp: 46.0000 - tn: 398.0000 - fn: 77.0000 - accuracy: 0.8272 - precision: 0.8059 - recall: 0.7127 - auc: 0.8475 - val_loss: 0.5245 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.9018\nEpoch 407/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5028 - tp: 202.0000 - fp: 51.0000 - tn: 393.0000 - fn: 66.0000 - accuracy: 0.8357 - precision: 0.7984 - recall: 0.7537 - auc: 0.8644 - val_loss: 0.4942 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9027\nEpoch 408/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5038 - tp: 194.0000 - fp: 46.0000 - tn: 398.0000 - fn: 74.0000 - accuracy: 0.8315 - precision: 0.8083 - recall: 0.7239 - auc: 0.8621 - val_loss: 0.4903 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9040\nEpoch 409/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5078 - tp: 203.0000 - fp: 54.0000 - tn: 390.0000 - fn: 65.0000 - accuracy: 0.8329 - precision: 0.7899 - recall: 0.7575 - auc: 0.8706 - val_loss: 0.4870 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9075\nEpoch 410/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5170 - tp: 194.0000 - fp: 40.0000 - tn: 404.0000 - fn: 74.0000 - accuracy: 0.8399 - precision: 0.8291 - recall: 0.7239 - auc: 0.8481 - val_loss: 0.4831 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9068\nEpoch 411/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5103 - tp: 198.0000 - fp: 52.0000 - tn: 392.0000 - fn: 70.0000 - accuracy: 0.8287 - precision: 0.7920 - recall: 0.7388 - auc: 0.8627 - val_loss: 0.4850 - val_tp: 57.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 17.0000 - val_accuracy: 0.8380 - val_precision: 0.8261 - val_recall: 0.7703 - val_auc: 0.9077\nEpoch 412/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5093 - tp: 188.0000 - fp: 32.0000 - tn: 412.0000 - fn: 80.0000 - accuracy: 0.8427 - precision: 0.8545 - recall: 0.7015 - auc: 0.8621 - val_loss: 0.5092 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9046\nEpoch 413/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.4994 - tp: 196.0000 - fp: 39.0000 - tn: 405.0000 - fn: 72.0000 - accuracy: 0.8441 - precision: 0.8340 - recall: 0.7313 - auc: 0.8640 - val_loss: 0.4971 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9055\nEpoch 414/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5233 - tp: 190.0000 - fp: 50.0000 - tn: 394.0000 - fn: 78.0000 - accuracy: 0.8202 - precision: 0.7917 - recall: 0.7090 - auc: 0.8517 - val_loss: 0.4942 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.9038\nEpoch 415/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5229 - tp: 193.0000 - fp: 44.0000 - tn: 400.0000 - fn: 75.0000 - accuracy: 0.8329 - precision: 0.8143 - recall: 0.7201 - auc: 0.8608 - val_loss: 0.4936 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9024\nEpoch 416/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5032 - tp: 198.0000 - fp: 49.0000 - tn: 395.0000 - fn: 70.0000 - accuracy: 0.8329 - precision: 0.8016 - recall: 0.7388 - auc: 0.8613 - val_loss: 0.5007 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9066\nEpoch 417/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.4960 - tp: 207.0000 - fp: 47.0000 - tn: 397.0000 - fn: 61.0000 - accuracy: 0.8483 - precision: 0.8150 - recall: 0.7724 - auc: 0.8688 - val_loss: 0.4859 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9065\nEpoch 418/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5016 - tp: 201.0000 - fp: 39.0000 - tn: 405.0000 - fn: 67.0000 - accuracy: 0.8511 - precision: 0.8375 - recall: 0.7500 - auc: 0.8710 - val_loss: 0.5065 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9051\nEpoch 419/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.4993 - tp: 205.0000 - fp: 42.0000 - tn: 402.0000 - fn: 63.0000 - accuracy: 0.8525 - precision: 0.8300 - recall: 0.7649 - auc: 0.8661 - val_loss: 0.4767 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9056\nEpoch 420/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5062 - tp: 196.0000 - fp: 45.0000 - tn: 399.0000 - fn: 72.0000 - accuracy: 0.8357 - precision: 0.8133 - recall: 0.7313 - auc: 0.8627 - val_loss: 0.4952 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9055\nEpoch 421/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5029 - tp: 186.0000 - fp: 35.0000 - tn: 409.0000 - fn: 82.0000 - accuracy: 0.8357 - precision: 0.8416 - recall: 0.6940 - auc: 0.8686 - val_loss: 0.5176 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9024\nEpoch 422/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.4888 - tp: 201.0000 - fp: 45.0000 - tn: 399.0000 - fn: 67.0000 - accuracy: 0.8427 - precision: 0.8171 - recall: 0.7500 - auc: 0.8822 - val_loss: 0.4774 - val_tp: 53.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 21.0000 - val_accuracy: 0.8156 - val_precision: 0.8154 - val_recall: 0.7162 - val_auc: 0.9061\nEpoch 423/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5098 - tp: 193.0000 - fp: 37.0000 - tn: 407.0000 - fn: 75.0000 - accuracy: 0.8427 - precision: 0.8391 - recall: 0.7201 - auc: 0.8765 - val_loss: 0.4832 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9064\nEpoch 424/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5089 - tp: 201.0000 - fp: 47.0000 - tn: 397.0000 - fn: 67.0000 - accuracy: 0.8399 - precision: 0.8105 - recall: 0.7500 - auc: 0.8618 - val_loss: 0.4738 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9100\nEpoch 425/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5063 - tp: 197.0000 - fp: 43.0000 - tn: 401.0000 - fn: 71.0000 - accuracy: 0.8399 - precision: 0.8208 - recall: 0.7351 - auc: 0.8656 - val_loss: 0.4868 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9044\nEpoch 426/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.4972 - tp: 199.0000 - fp: 43.0000 - tn: 401.0000 - fn: 69.0000 - accuracy: 0.8427 - precision: 0.8223 - recall: 0.7425 - auc: 0.8694 - val_loss: 0.4932 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9050\nEpoch 427/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5155 - tp: 200.0000 - fp: 54.0000 - tn: 390.0000 - fn: 68.0000 - accuracy: 0.8287 - precision: 0.7874 - recall: 0.7463 - auc: 0.8637 - val_loss: 0.4871 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9073\nEpoch 428/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5254 - tp: 189.0000 - fp: 42.0000 - tn: 402.0000 - fn: 79.0000 - accuracy: 0.8301 - precision: 0.8182 - recall: 0.7052 - auc: 0.8514 - val_loss: 0.4842 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9050\nEpoch 429/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5068 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8643 - val_loss: 0.4814 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9077\nEpoch 430/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5171 - tp: 192.0000 - fp: 33.0000 - tn: 411.0000 - fn: 76.0000 - accuracy: 0.8469 - precision: 0.8533 - recall: 0.7164 - auc: 0.8596 - val_loss: 0.4900 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9075\nEpoch 431/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.4979 - tp: 197.0000 - fp: 48.0000 - tn: 396.0000 - fn: 71.0000 - accuracy: 0.8329 - precision: 0.8041 - recall: 0.7351 - auc: 0.8690 - val_loss: 0.4924 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9067\nEpoch 432/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5238 - tp: 182.0000 - fp: 38.0000 - tn: 406.0000 - fn: 86.0000 - accuracy: 0.8258 - precision: 0.8273 - recall: 0.6791 - auc: 0.8444 - val_loss: 0.5221 - val_tp: 62.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 12.0000 - val_accuracy: 0.8436 - val_precision: 0.7949 - val_recall: 0.8378 - val_auc: 0.9035\nEpoch 433/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5173 - tp: 189.0000 - fp: 43.0000 - tn: 401.0000 - fn: 79.0000 - accuracy: 0.8287 - precision: 0.8147 - recall: 0.7052 - auc: 0.8532 - val_loss: 0.5032 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9046\nEpoch 434/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5315 - tp: 195.0000 - fp: 35.0000 - tn: 409.0000 - fn: 73.0000 - accuracy: 0.8483 - precision: 0.8478 - recall: 0.7276 - auc: 0.8446 - val_loss: 0.4936 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9045\nEpoch 435/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5138 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8565 - val_loss: 0.4860 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9062\nEpoch 436/500\n23/23 [==============================] - 0s 15ms/step - loss: 0.5198 - tp: 181.0000 - fp: 32.0000 - tn: 412.0000 - fn: 87.0000 - accuracy: 0.8329 - precision: 0.8498 - recall: 0.6754 - auc: 0.8647 - val_loss: 0.5006 - val_tp: 63.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 11.0000 - val_accuracy: 0.8436 - val_precision: 0.7875 - val_recall: 0.8514 - val_auc: 0.9039\nEpoch 437/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5025 - tp: 203.0000 - fp: 50.0000 - tn: 394.0000 - fn: 65.0000 - accuracy: 0.8385 - precision: 0.8024 - recall: 0.7575 - auc: 0.8642 - val_loss: 0.4894 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9050\nEpoch 438/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5138 - tp: 199.0000 - fp: 47.0000 - tn: 397.0000 - fn: 69.0000 - accuracy: 0.8371 - precision: 0.8089 - recall: 0.7425 - auc: 0.8633 - val_loss: 0.4836 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9038\nEpoch 439/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.4908 - tp: 193.0000 - fp: 31.0000 - tn: 413.0000 - fn: 75.0000 - accuracy: 0.8511 - precision: 0.8616 - recall: 0.7201 - auc: 0.8745 - val_loss: 0.5207 - val_tp: 62.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 12.0000 - val_accuracy: 0.8492 - val_precision: 0.8052 - val_recall: 0.8378 - val_auc: 0.9063\nEpoch 440/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5129 - tp: 201.0000 - fp: 47.0000 - tn: 397.0000 - fn: 67.0000 - accuracy: 0.8399 - precision: 0.8105 - recall: 0.7500 - auc: 0.8582 - val_loss: 0.4805 - val_tp: 57.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 17.0000 - val_accuracy: 0.8380 - val_precision: 0.8261 - val_recall: 0.7703 - val_auc: 0.9102\nEpoch 441/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5042 - tp: 196.0000 - fp: 36.0000 - tn: 408.0000 - fn: 72.0000 - accuracy: 0.8483 - precision: 0.8448 - recall: 0.7313 - auc: 0.8607 - val_loss: 0.5028 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9074\nEpoch 442/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5089 - tp: 202.0000 - fp: 43.0000 - tn: 401.0000 - fn: 66.0000 - accuracy: 0.8469 - precision: 0.8245 - recall: 0.7537 - auc: 0.8633 - val_loss: 0.4776 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.9087\nEpoch 443/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5137 - tp: 193.0000 - fp: 48.0000 - tn: 396.0000 - fn: 75.0000 - accuracy: 0.8272 - precision: 0.8008 - recall: 0.7201 - auc: 0.8580 - val_loss: 0.4936 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9059\nEpoch 444/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5208 - tp: 189.0000 - fp: 39.0000 - tn: 405.0000 - fn: 79.0000 - accuracy: 0.8343 - precision: 0.8289 - recall: 0.7052 - auc: 0.8590 - val_loss: 0.5040 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9060\nEpoch 445/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.4827 - tp: 199.0000 - fp: 37.0000 - tn: 407.0000 - fn: 69.0000 - accuracy: 0.8511 - precision: 0.8432 - recall: 0.7425 - auc: 0.8751 - val_loss: 0.4879 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9078\nEpoch 446/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5276 - tp: 180.0000 - fp: 35.0000 - tn: 409.0000 - fn: 88.0000 - accuracy: 0.8272 - precision: 0.8372 - recall: 0.6716 - auc: 0.8492 - val_loss: 0.5055 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9069\nEpoch 447/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5043 - tp: 204.0000 - fp: 47.0000 - tn: 397.0000 - fn: 64.0000 - accuracy: 0.8441 - precision: 0.8127 - recall: 0.7612 - auc: 0.8646 - val_loss: 0.4860 - val_tp: 54.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 20.0000 - val_accuracy: 0.8268 - val_precision: 0.8308 - val_recall: 0.7297 - val_auc: 0.9083\nEpoch 448/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5135 - tp: 188.0000 - fp: 36.0000 - tn: 408.0000 - fn: 80.0000 - accuracy: 0.8371 - precision: 0.8393 - recall: 0.7015 - auc: 0.8588 - val_loss: 0.5075 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9048\nEpoch 449/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5094 - tp: 190.0000 - fp: 47.0000 - tn: 397.0000 - fn: 78.0000 - accuracy: 0.8244 - precision: 0.8017 - recall: 0.7090 - auc: 0.8636 - val_loss: 0.5051 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9071\nEpoch 450/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5117 - tp: 189.0000 - fp: 35.0000 - tn: 409.0000 - fn: 79.0000 - accuracy: 0.8399 - precision: 0.8438 - recall: 0.7052 - auc: 0.8592 - val_loss: 0.5110 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9042\nEpoch 451/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5007 - tp: 196.0000 - fp: 50.0000 - tn: 394.0000 - fn: 72.0000 - accuracy: 0.8287 - precision: 0.7967 - recall: 0.7313 - auc: 0.8643 - val_loss: 0.4863 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9106\nEpoch 452/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5123 - tp: 196.0000 - fp: 44.0000 - tn: 400.0000 - fn: 72.0000 - accuracy: 0.8371 - precision: 0.8167 - recall: 0.7313 - auc: 0.8584 - val_loss: 0.4912 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9074\nEpoch 453/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4994 - tp: 199.0000 - fp: 48.0000 - tn: 396.0000 - fn: 69.0000 - accuracy: 0.8357 - precision: 0.8057 - recall: 0.7425 - auc: 0.8683 - val_loss: 0.4841 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9088\nEpoch 454/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5116 - tp: 207.0000 - fp: 68.0000 - tn: 376.0000 - fn: 61.0000 - accuracy: 0.8188 - precision: 0.7527 - recall: 0.7724 - auc: 0.8668 - val_loss: 0.4740 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9095\nEpoch 455/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5134 - tp: 202.0000 - fp: 49.0000 - tn: 395.0000 - fn: 66.0000 - accuracy: 0.8385 - precision: 0.8048 - recall: 0.7537 - auc: 0.8707 - val_loss: 0.4780 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9104\nEpoch 456/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5010 - tp: 197.0000 - fp: 37.0000 - tn: 407.0000 - fn: 71.0000 - accuracy: 0.8483 - precision: 0.8419 - recall: 0.7351 - auc: 0.8640 - val_loss: 0.5144 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.9057\nEpoch 457/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5181 - tp: 202.0000 - fp: 50.0000 - tn: 394.0000 - fn: 66.0000 - accuracy: 0.8371 - precision: 0.8016 - recall: 0.7537 - auc: 0.8617 - val_loss: 0.4819 - val_tp: 54.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 20.0000 - val_accuracy: 0.8268 - val_precision: 0.8308 - val_recall: 0.7297 - val_auc: 0.9078\nEpoch 458/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.4954 - tp: 191.0000 - fp: 35.0000 - tn: 409.0000 - fn: 77.0000 - accuracy: 0.8427 - precision: 0.8451 - recall: 0.7127 - auc: 0.8707 - val_loss: 0.5007 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9055\nEpoch 459/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5027 - tp: 192.0000 - fp: 40.0000 - tn: 404.0000 - fn: 76.0000 - accuracy: 0.8371 - precision: 0.8276 - recall: 0.7164 - auc: 0.8702 - val_loss: 0.4949 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9055\nEpoch 460/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5154 - tp: 196.0000 - fp: 42.0000 - tn: 402.0000 - fn: 72.0000 - accuracy: 0.8399 - precision: 0.8235 - recall: 0.7313 - auc: 0.8559 - val_loss: 0.4878 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9091\nEpoch 461/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5105 - tp: 195.0000 - fp: 40.0000 - tn: 404.0000 - fn: 73.0000 - accuracy: 0.8413 - precision: 0.8298 - recall: 0.7276 - auc: 0.8575 - val_loss: 0.4977 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9053\nEpoch 462/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5044 - tp: 209.0000 - fp: 52.0000 - tn: 392.0000 - fn: 59.0000 - accuracy: 0.8441 - precision: 0.8008 - recall: 0.7799 - auc: 0.8671 - val_loss: 0.4843 - val_tp: 53.0000 - val_fp: 10.0000 - val_tn: 95.0000 - val_fn: 21.0000 - val_accuracy: 0.8268 - val_precision: 0.8413 - val_recall: 0.7162 - val_auc: 0.9084\nEpoch 463/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5125 - tp: 189.0000 - fp: 32.0000 - tn: 412.0000 - fn: 79.0000 - accuracy: 0.8441 - precision: 0.8552 - recall: 0.7052 - auc: 0.8680 - val_loss: 0.5076 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.9043\nEpoch 464/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5297 - tp: 191.0000 - fp: 47.0000 - tn: 397.0000 - fn: 77.0000 - accuracy: 0.8258 - precision: 0.8025 - recall: 0.7127 - auc: 0.8505 - val_loss: 0.4746 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9089\nEpoch 465/500\n23/23 [==============================] - 0s 9ms/step - loss: 0.5084 - tp: 201.0000 - fp: 58.0000 - tn: 386.0000 - fn: 67.0000 - accuracy: 0.8244 - precision: 0.7761 - recall: 0.7500 - auc: 0.8624 - val_loss: 0.4780 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.9053\nEpoch 466/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.4954 - tp: 198.0000 - fp: 42.0000 - tn: 402.0000 - fn: 70.0000 - accuracy: 0.8427 - precision: 0.8250 - recall: 0.7388 - auc: 0.8759 - val_loss: 0.4914 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9084\nEpoch 467/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4938 - tp: 192.0000 - fp: 38.0000 - tn: 406.0000 - fn: 76.0000 - accuracy: 0.8399 - precision: 0.8348 - recall: 0.7164 - auc: 0.8751 - val_loss: 0.4912 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9079\nEpoch 468/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5036 - tp: 199.0000 - fp: 48.0000 - tn: 396.0000 - fn: 69.0000 - accuracy: 0.8357 - precision: 0.8057 - recall: 0.7425 - auc: 0.8568 - val_loss: 0.4900 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9106\nEpoch 469/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5190 - tp: 185.0000 - fp: 30.0000 - tn: 414.0000 - fn: 83.0000 - accuracy: 0.8413 - precision: 0.8605 - recall: 0.6903 - auc: 0.8737 - val_loss: 0.5425 - val_tp: 62.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 12.0000 - val_accuracy: 0.8380 - val_precision: 0.7848 - val_recall: 0.8378 - val_auc: 0.9035\nEpoch 470/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5172 - tp: 195.0000 - fp: 39.0000 - tn: 405.0000 - fn: 73.0000 - accuracy: 0.8427 - precision: 0.8333 - recall: 0.7276 - auc: 0.8638 - val_loss: 0.4940 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.9062\nEpoch 471/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5026 - tp: 201.0000 - fp: 41.0000 - tn: 403.0000 - fn: 67.0000 - accuracy: 0.8483 - precision: 0.8306 - recall: 0.7500 - auc: 0.8639 - val_loss: 0.4891 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9095\nEpoch 472/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5099 - tp: 195.0000 - fp: 46.0000 - tn: 398.0000 - fn: 73.0000 - accuracy: 0.8329 - precision: 0.8091 - recall: 0.7276 - auc: 0.8687 - val_loss: 0.4894 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9077\nEpoch 473/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.4909 - tp: 207.0000 - fp: 52.0000 - tn: 392.0000 - fn: 61.0000 - accuracy: 0.8413 - precision: 0.7992 - recall: 0.7724 - auc: 0.8803 - val_loss: 0.4762 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9093\nEpoch 474/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5210 - tp: 191.0000 - fp: 40.0000 - tn: 404.0000 - fn: 77.0000 - accuracy: 0.8357 - precision: 0.8268 - recall: 0.7127 - auc: 0.8577 - val_loss: 0.4870 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9063\nEpoch 475/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5055 - tp: 193.0000 - fp: 46.0000 - tn: 398.0000 - fn: 75.0000 - accuracy: 0.8301 - precision: 0.8075 - recall: 0.7201 - auc: 0.8637 - val_loss: 0.4982 - val_tp: 62.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 12.0000 - val_accuracy: 0.8380 - val_precision: 0.7848 - val_recall: 0.8378 - val_auc: 0.9078\nEpoch 476/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5025 - tp: 190.0000 - fp: 36.0000 - tn: 408.0000 - fn: 78.0000 - accuracy: 0.8399 - precision: 0.8407 - recall: 0.7090 - auc: 0.8681 - val_loss: 0.4881 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9082\nEpoch 477/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5034 - tp: 200.0000 - fp: 52.0000 - tn: 392.0000 - fn: 68.0000 - accuracy: 0.8315 - precision: 0.7937 - recall: 0.7463 - auc: 0.8753 - val_loss: 0.4820 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9088\nEpoch 478/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.4938 - tp: 202.0000 - fp: 46.0000 - tn: 398.0000 - fn: 66.0000 - accuracy: 0.8427 - precision: 0.8145 - recall: 0.7537 - auc: 0.8761 - val_loss: 0.4802 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9086\nEpoch 479/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5113 - tp: 195.0000 - fp: 46.0000 - tn: 398.0000 - fn: 73.0000 - accuracy: 0.8329 - precision: 0.8091 - recall: 0.7276 - auc: 0.8612 - val_loss: 0.4920 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.9083\nEpoch 480/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5073 - tp: 194.0000 - fp: 46.0000 - tn: 398.0000 - fn: 74.0000 - accuracy: 0.8315 - precision: 0.8083 - recall: 0.7239 - auc: 0.8706 - val_loss: 0.4977 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.9085\nEpoch 481/500\n23/23 [==============================] - 0s 10ms/step - loss: 0.5146 - tp: 192.0000 - fp: 43.0000 - tn: 401.0000 - fn: 76.0000 - accuracy: 0.8329 - precision: 0.8170 - recall: 0.7164 - auc: 0.8585 - val_loss: 0.4786 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9098\nEpoch 482/500\n23/23 [==============================] - 0s 8ms/step - loss: 0.5139 - tp: 189.0000 - fp: 41.0000 - tn: 403.0000 - fn: 79.0000 - accuracy: 0.8315 - precision: 0.8217 - recall: 0.7052 - auc: 0.8652 - val_loss: 0.4796 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9133\nEpoch 483/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.4849 - tp: 196.0000 - fp: 43.0000 - tn: 401.0000 - fn: 72.0000 - accuracy: 0.8385 - precision: 0.8201 - recall: 0.7313 - auc: 0.8743 - val_loss: 0.5025 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9044\nEpoch 484/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5004 - tp: 198.0000 - fp: 49.0000 - tn: 395.0000 - fn: 70.0000 - accuracy: 0.8329 - precision: 0.8016 - recall: 0.7388 - auc: 0.8674 - val_loss: 0.4845 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9080\nEpoch 485/500\n23/23 [==============================] - 0s 16ms/step - loss: 0.5243 - tp: 188.0000 - fp: 43.0000 - tn: 401.0000 - fn: 80.0000 - accuracy: 0.8272 - precision: 0.8139 - recall: 0.7015 - auc: 0.8509 - val_loss: 0.5013 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9067\nEpoch 486/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5065 - tp: 200.0000 - fp: 43.0000 - tn: 401.0000 - fn: 68.0000 - accuracy: 0.8441 - precision: 0.8230 - recall: 0.7463 - auc: 0.8564 - val_loss: 0.4928 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9081\nEpoch 487/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5186 - tp: 202.0000 - fp: 41.0000 - tn: 403.0000 - fn: 66.0000 - accuracy: 0.8497 - precision: 0.8313 - recall: 0.7537 - auc: 0.8684 - val_loss: 0.4825 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9102\nEpoch 488/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5193 - tp: 192.0000 - fp: 37.0000 - tn: 407.0000 - fn: 76.0000 - accuracy: 0.8413 - precision: 0.8384 - recall: 0.7164 - auc: 0.8560 - val_loss: 0.5100 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.9052\nEpoch 489/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5011 - tp: 194.0000 - fp: 43.0000 - tn: 401.0000 - fn: 74.0000 - accuracy: 0.8357 - precision: 0.8186 - recall: 0.7239 - auc: 0.8783 - val_loss: 0.5015 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9098\nEpoch 490/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.5100 - tp: 188.0000 - fp: 53.0000 - tn: 391.0000 - fn: 80.0000 - accuracy: 0.8132 - precision: 0.7801 - recall: 0.7015 - auc: 0.8704 - val_loss: 0.4850 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9106\nEpoch 491/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.5099 - tp: 193.0000 - fp: 48.0000 - tn: 396.0000 - fn: 75.0000 - accuracy: 0.8272 - precision: 0.8008 - recall: 0.7201 - auc: 0.8687 - val_loss: 0.4861 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9098\nEpoch 492/500\n23/23 [==============================] - 0s 12ms/step - loss: 0.5184 - tp: 197.0000 - fp: 47.0000 - tn: 397.0000 - fn: 71.0000 - accuracy: 0.8343 - precision: 0.8074 - recall: 0.7351 - auc: 0.8646 - val_loss: 0.4764 - val_tp: 60.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 14.0000 - val_accuracy: 0.8436 - val_precision: 0.8108 - val_recall: 0.8108 - val_auc: 0.9097\nEpoch 493/500\n16/23 [===================>..........] - ETA: 0s - loss: 0.4996 - tp: 144.0000 - fp: 31.0000 - tn: 286.0000 - fn: 51.0000 - accuracy: 0.8398 - precision: 0.8229 - recall: 0.7385 - auc: 0.87923/23 [==============================] - 0s 13ms/step - loss: 0.4970 - tp: 200.0000 - fp: 45.0000 - tn: 399.0000 - fn: 68.0000 - accuracy: 0.8413 - precision: 0.8163 - recall: 0.7463 - auc: 0.8792 - val_loss: 0.4690 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.9115\nEpoch 494/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.5043 - tp: 199.0000 - fp: 43.0000 - tn: 401.0000 - fn: 69.0000 - accuracy: 0.8427 - precision: 0.8223 - recall: 0.7425 - auc: 0.8651 - val_loss: 0.4849 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.9097\nEpoch 495/500\n23/23 [==============================] - 0s 17ms/step - loss: 0.5048 - tp: 202.0000 - fp: 50.0000 - tn: 394.0000 - fn: 66.0000 - accuracy: 0.8371 - precision: 0.8016 - recall: 0.7537 - auc: 0.8670 - val_loss: 0.4716 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9127\nEpoch 496/500\n23/23 [==============================] - 0s 14ms/step - loss: 0.4975 - tp: 193.0000 - fp: 54.0000 - tn: 390.0000 - fn: 75.0000 - accuracy: 0.8188 - precision: 0.7814 - recall: 0.7201 - auc: 0.8784 - val_loss: 0.4784 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9108\nEpoch 497/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.4899 - tp: 192.0000 - fp: 44.0000 - tn: 400.0000 - fn: 76.0000 - accuracy: 0.8315 - precision: 0.8136 - recall: 0.7164 - auc: 0.8772 - val_loss: 0.4808 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9089\nEpoch 498/500\n23/23 [==============================] - 0s 11ms/step - loss: 0.4950 - tp: 201.0000 - fp: 48.0000 - tn: 396.0000 - fn: 67.0000 - accuracy: 0.8385 - precision: 0.8072 - recall: 0.7500 - auc: 0.8718 - val_loss: 0.4865 - val_tp: 61.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 13.0000 - val_accuracy: 0.8492 - val_precision: 0.8133 - val_recall: 0.8243 - val_auc: 0.9097\nEpoch 499/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4944 - tp: 200.0000 - fp: 44.0000 - tn: 400.0000 - fn: 68.0000 - accuracy: 0.8427 - precision: 0.8197 - recall: 0.7463 - auc: 0.8729 - val_loss: 0.4885 - val_tp: 61.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 13.0000 - val_accuracy: 0.8547 - val_precision: 0.8243 - val_recall: 0.8243 - val_auc: 0.9098\nEpoch 500/500\n23/23 [==============================] - 0s 13ms/step - loss: 0.4941 - tp: 192.0000 - fp: 34.0000 - tn: 410.0000 - fn: 76.0000 - accuracy: 0.8455 - precision: 0.8496 - recall: 0.7164 - auc: 0.8762 - val_loss: 0.4855 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.9093\n"
                }
            ],
            "source": [
                "from titanic.titanic_data import get_class_weights\n",
                "\n",
                "# Create a new model each time before running training (otherwise new trainings would just be on already trained model)\n",
                "model = get_model(X_train.shape[1])\n",
                "\n",
                "history = model.fit(X_train, y_train, epochs=500, batch_size=32, validation_data=(X_valid, y_valid), class_weight=get_class_weights(y_train), verbose=1)"
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "## Results of the DL model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "dict_keys(['loss', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', 'recall', 'auc', 'val_loss', 'val_tp', 'val_fp', 'val_tn', 'val_fn', 'val_accuracy', 'val_precision', 'val_recall', 'val_auc'])\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<Figure size 432x288 with 1 Axes>",
                        "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 392.14375 277.314375\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 392.14375 277.314375 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \nL 384.94375 22.318125 \nL 50.14375 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mbb24e455da\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#mbb24e455da\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(62.180682 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.356649\" xlink:href=\"#mbb24e455da\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(116.812899 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.351365\" xlink:href=\"#mbb24e455da\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(177.807615 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.346082\" xlink:href=\"#mbb24e455da\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(238.802332 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.340799\" xlink:href=\"#mbb24e455da\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(299.797049 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"370.335515\" xlink:href=\"#mbb24e455da\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(360.791765 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(202.315625 268.034687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m156eeecf15\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m156eeecf15\" y=\"228.345453\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.50 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(20.878125 232.144672)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m156eeecf15\" y=\"201.128613\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.55 -->\n      <g transform=\"translate(20.878125 204.927832)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m156eeecf15\" y=\"173.911774\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(20.878125 177.710992)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m156eeecf15\" y=\"146.694934\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.65 -->\n      <g transform=\"translate(20.878125 150.494153)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m156eeecf15\" y=\"119.478094\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.70 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 123.277313)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m156eeecf15\" y=\"92.261255\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.75 -->\n      <g transform=\"translate(20.878125 96.060473)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m156eeecf15\" y=\"65.044415\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(20.878125 68.843634)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m156eeecf15\" y=\"37.827575\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.85 -->\n      <g transform=\"translate(20.878125 41.626794)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- accuracy -->\n     <defs>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n     </defs>\n     <g transform=\"translate(14.798438 153.5975)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p5bddf25372)\" d=\"M 65.361932 209.232492 \nL 65.971879 203.880884 \nL 66.581826 229.874489 \nL 67.191773 186.296959 \nL 67.80172 197.000208 \nL 68.411668 181.709852 \nL 69.021615 155.716247 \nL 69.631562 156.480749 \nL 70.241509 138.132322 \nL 70.851456 128.958108 \nL 71.461403 128.958108 \nL 72.071351 122.841966 \nL 72.681298 126.664571 \nL 73.291245 115.961322 \nL 73.901192 123.6065 \nL 74.511139 119.783895 \nL 75.121086 128.958108 \nL 75.731034 120.548429 \nL 76.340981 104.493539 \nL 76.950928 102.964504 \nL 77.560875 80.02897 \nL 78.170822 92.261255 \nL 78.780769 93.79029 \nL 79.390717 73.912828 \nL 80.000664 74.677329 \nL 80.610611 76.970899 \nL 81.220558 82.322507 \nL 81.830505 67.796686 \nL 82.440452 62.445045 \nL 83.0504 65.503116 \nL 83.660347 64.738614 \nL 84.270294 55.564401 \nL 84.880241 56.328903 \nL 85.490188 74.677329 \nL 86.100135 53.270831 \nL 86.710083 58.622472 \nL 87.32003 67.796686 \nL 87.929977 56.328903 \nL 88.539924 83.087041 \nL 89.149871 66.26765 \nL 89.759819 68.561187 \nL 90.369766 60.151508 \nL 90.979713 62.445045 \nL 91.58966 63.97408 \nL 92.199607 63.97408 \nL 92.809554 59.386974 \nL 93.419502 68.561187 \nL 94.029449 54.799867 \nL 94.639396 63.97408 \nL 95.249343 62.445045 \nL 95.85929 51.741796 \nL 96.469237 57.857938 \nL 97.079185 57.093437 \nL 97.689132 48.683725 \nL 98.299079 60.151508 \nL 98.909026 60.916009 \nL 99.518973 49.448259 \nL 100.12892 54.799867 \nL 100.738868 61.680543 \nL 101.348815 45.625654 \nL 101.958762 53.270831 \nL 102.568709 54.799867 \nL 103.178656 62.445045 \nL 103.788603 50.977294 \nL 104.398551 54.799867 \nL 105.008498 50.977294 \nL 105.618445 63.209579 \nL 106.228392 44.096618 \nL 106.838339 54.035365 \nL 107.448286 42.567582 \nL 108.058234 57.857938 \nL 108.668181 55.564401 \nL 109.278128 55.564401 \nL 109.888075 54.799867 \nL 110.498022 51.741796 \nL 111.717917 57.857938 \nL 112.327864 50.977294 \nL 112.937811 47.919223 \nL 113.547758 45.625654 \nL 114.157705 48.683725 \nL 114.767652 54.799867 \nL 115.3776 44.861152 \nL 115.987547 51.741796 \nL 116.597494 51.741796 \nL 117.207441 58.622472 \nL 117.817388 58.622472 \nL 118.427335 50.977294 \nL 119.037283 47.919223 \nL 119.64723 55.564401 \nL 120.257177 47.919223 \nL 120.867124 45.625654 \nL 121.477071 50.977294 \nL 122.087018 58.622472 \nL 122.696966 49.448259 \nL 123.306913 51.741796 \nL 123.91686 49.448259 \nL 124.526807 51.741796 \nL 125.136754 50.977294 \nL 125.746701 49.448259 \nL 126.356649 47.154689 \nL 126.966596 51.741796 \nL 127.576543 58.622472 \nL 128.18649 57.857938 \nL 128.796437 50.977294 \nL 129.406384 50.21276 \nL 130.016332 48.683725 \nL 130.626279 52.50633 \nL 131.236226 46.390188 \nL 131.846173 48.683725 \nL 132.45612 57.857938 \nL 133.066067 50.977294 \nL 133.676015 58.622472 \nL 134.285962 47.919223 \nL 135.505856 61.680543 \nL 136.115803 53.270831 \nL 136.72575 49.448259 \nL 137.335698 47.154689 \nL 137.945645 57.093437 \nL 138.555592 51.741796 \nL 139.165539 41.803081 \nL 139.775486 50.977294 \nL 140.385433 48.683725 \nL 140.995381 48.683725 \nL 141.605328 49.448259 \nL 142.215275 49.448259 \nL 142.825222 44.096618 \nL 143.435169 57.093437 \nL 144.045116 44.096618 \nL 144.655064 50.977294 \nL 145.265011 45.625654 \nL 146.484905 53.270831 \nL 147.094852 47.154689 \nL 147.704799 44.861152 \nL 148.314747 49.448259 \nL 148.924694 47.919223 \nL 149.534641 54.035365 \nL 150.144588 44.861152 \nL 150.754535 56.328903 \nL 151.364482 50.21276 \nL 151.97443 52.50633 \nL 152.584377 50.977294 \nL 153.194324 45.625654 \nL 153.804271 47.154689 \nL 154.414218 50.21276 \nL 155.024165 47.919223 \nL 155.634113 60.151508 \nL 156.24406 50.21276 \nL 157.463954 48.683725 \nL 158.073901 48.683725 \nL 158.683848 41.803081 \nL 159.293796 48.683725 \nL 159.903743 49.448259 \nL 160.51369 49.448259 \nL 161.123637 41.803081 \nL 161.733584 44.096618 \nL 162.343531 48.683725 \nL 162.953479 50.977294 \nL 163.563426 48.683725 \nL 164.173373 54.799867 \nL 164.78332 46.390188 \nL 165.393267 47.919223 \nL 166.003214 54.035365 \nL 166.613162 53.270831 \nL 167.223109 55.564401 \nL 167.833056 49.448259 \nL 168.443003 48.683725 \nL 169.662897 51.741796 \nL 170.272845 54.035365 \nL 170.882792 54.035365 \nL 171.492739 45.625654 \nL 172.102686 47.154689 \nL 172.712633 47.919223 \nL 173.32258 41.803081 \nL 173.932528 44.096618 \nL 174.542475 47.154689 \nL 175.152422 44.861152 \nL 175.762369 47.919223 \nL 176.372316 52.50633 \nL 176.982263 50.21276 \nL 177.592211 49.448259 \nL 178.202158 53.270831 \nL 178.812105 51.741796 \nL 179.422052 48.683725 \nL 180.031999 52.50633 \nL 180.641946 51.741796 \nL 181.251894 53.270831 \nL 181.861841 45.625654 \nL 182.471788 44.861152 \nL 183.081735 53.270831 \nL 183.691682 50.21276 \nL 184.301629 65.503116 \nL 184.911577 43.332116 \nL 185.521524 44.861152 \nL 186.131471 44.096618 \nL 186.741418 55.564401 \nL 187.351365 50.977294 \nL 187.961312 43.332116 \nL 188.57126 46.390188 \nL 189.181207 45.625654 \nL 189.791154 51.741796 \nL 190.401101 47.919223 \nL 191.011048 47.154689 \nL 191.620995 41.803081 \nL 192.230943 51.741796 \nL 192.84089 44.861152 \nL 193.450837 54.799867 \nL 194.060784 47.154689 \nL 194.670731 41.038547 \nL 195.280678 53.270831 \nL 195.890626 52.50633 \nL 196.500573 37.980476 \nL 197.11052 48.683725 \nL 197.720467 45.625654 \nL 198.330414 50.21276 \nL 198.940361 57.093437 \nL 199.550309 54.035365 \nL 200.160256 55.564401 \nL 200.770203 45.625654 \nL 201.38015 44.861152 \nL 202.600044 56.328903 \nL 203.209992 45.625654 \nL 204.429886 54.799867 \nL 205.039833 52.50633 \nL 205.64978 44.096618 \nL 206.259727 47.919223 \nL 206.869675 54.799867 \nL 207.479622 43.332116 \nL 208.089569 43.332116 \nL 208.699516 45.625654 \nL 209.309463 52.50633 \nL 209.91941 54.035365 \nL 210.529358 44.861152 \nL 211.139305 50.977294 \nL 211.749252 50.21276 \nL 212.359199 45.625654 \nL 212.969146 44.861152 \nL 213.579093 55.564401 \nL 214.189041 53.270831 \nL 214.798988 44.096618 \nL 215.408935 47.154689 \nL 216.018882 47.154689 \nL 216.628829 44.861152 \nL 217.238776 57.857938 \nL 217.848724 44.861152 \nL 218.458671 48.683725 \nL 219.068618 44.096618 \nL 219.678565 54.799867 \nL 220.288512 45.625654 \nL 220.898459 41.038547 \nL 221.508407 57.857938 \nL 222.118354 47.919223 \nL 222.728301 46.390188 \nL 223.338248 50.21276 \nL 223.948195 42.567582 \nL 224.558142 40.274045 \nL 225.16809 48.683725 \nL 225.778037 44.861152 \nL 226.387984 56.328903 \nL 226.997931 43.332116 \nL 227.607878 50.21276 \nL 228.217825 46.390188 \nL 228.827773 45.625654 \nL 229.43772 54.799867 \nL 230.047667 43.332116 \nL 230.657614 48.683725 \nL 231.267561 58.622472 \nL 231.877508 43.332116 \nL 232.487456 50.21276 \nL 233.097403 45.625654 \nL 233.70735 45.625654 \nL 234.317297 43.332116 \nL 234.927244 50.21276 \nL 235.537191 47.919223 \nL 236.147139 44.861152 \nL 236.757086 62.445045 \nL 237.367033 45.625654 \nL 237.97698 47.919223 \nL 238.586927 49.448259 \nL 239.196874 50.21276 \nL 239.806822 46.390188 \nL 240.416769 48.683725 \nL 241.026716 48.683725 \nL 241.636663 53.270831 \nL 242.24661 44.096618 \nL 242.856557 43.332116 \nL 243.466505 48.683725 \nL 244.076452 42.567582 \nL 244.686399 47.154689 \nL 245.296346 38.74501 \nL 245.906293 52.50633 \nL 247.126188 40.274045 \nL 247.736135 47.154689 \nL 248.346082 44.861152 \nL 248.956029 40.274045 \nL 249.565976 45.625654 \nL 250.175923 45.625654 \nL 250.785871 54.799867 \nL 251.395818 44.096618 \nL 252.005765 44.096618 \nL 252.615712 43.332116 \nL 253.225659 47.919223 \nL 253.835606 36.45144 \nL 254.445554 41.803081 \nL 255.055501 38.74501 \nL 255.665448 50.977294 \nL 256.275395 44.861152 \nL 256.885342 52.50633 \nL 257.495289 44.861152 \nL 258.105237 47.154689 \nL 258.715184 45.625654 \nL 259.325131 37.980476 \nL 259.935078 50.21276 \nL 260.545025 54.799867 \nL 261.154972 47.919223 \nL 261.76492 46.390188 \nL 262.374867 49.448259 \nL 262.984814 47.919223 \nL 263.594761 50.977294 \nL 264.204708 49.448259 \nL 264.814655 50.977294 \nL 265.424603 51.741796 \nL 266.03455 48.683725 \nL 266.644497 44.096618 \nL 267.254444 44.096618 \nL 267.864391 51.741796 \nL 268.474338 45.625654 \nL 269.084286 48.683725 \nL 269.694233 47.919223 \nL 270.30418 52.50633 \nL 270.914127 44.861152 \nL 271.524074 43.332116 \nL 272.134021 47.154689 \nL 272.743969 48.683725 \nL 273.353916 39.509511 \nL 273.963863 42.567582 \nL 274.57381 47.919223 \nL 275.183757 41.038547 \nL 275.793704 46.390188 \nL 276.403652 48.683725 \nL 277.013599 53.270831 \nL 277.623546 47.154689 \nL 278.233493 50.977294 \nL 278.84344 41.038547 \nL 279.453387 41.038547 \nL 280.063335 43.332116 \nL 280.673282 42.567582 \nL 281.283229 50.21276 \nL 281.893176 36.45144 \nL 282.503123 43.332116 \nL 283.11307 44.861152 \nL 283.723018 41.038547 \nL 284.332965 46.390188 \nL 284.942912 46.390188 \nL 285.552859 43.332116 \nL 286.162806 50.21276 \nL 286.772753 47.154689 \nL 287.382701 47.919223 \nL 287.992648 47.154689 \nL 288.602595 45.625654 \nL 289.212542 47.919223 \nL 289.822489 46.390188 \nL 290.432436 47.919223 \nL 291.042384 47.919223 \nL 291.652331 48.683725 \nL 292.262278 51.741796 \nL 292.872225 44.096618 \nL 293.482172 39.509511 \nL 294.092119 47.154689 \nL 294.702067 44.861152 \nL 295.312014 38.74501 \nL 295.921961 44.861152 \nL 296.531908 49.448259 \nL 297.141855 42.567582 \nL 297.751802 41.803081 \nL 298.36175 35.686939 \nL 298.971697 41.038547 \nL 299.581644 47.919223 \nL 300.191591 51.741796 \nL 300.801538 45.625654 \nL 302.021433 42.567582 \nL 302.63138 51.741796 \nL 303.241327 44.861152 \nL 303.851274 48.683725 \nL 304.461221 51.741796 \nL 305.071168 44.096618 \nL 305.681116 43.332116 \nL 306.291063 48.683725 \nL 306.90101 43.332116 \nL 307.510957 46.390188 \nL 308.120904 39.509511 \nL 308.730851 43.332116 \nL 309.340799 42.567582 \nL 309.950746 39.509511 \nL 310.560693 43.332116 \nL 311.17064 48.683725 \nL 311.780587 47.154689 \nL 312.390534 50.21276 \nL 313.000482 45.625654 \nL 313.610429 47.919223 \nL 314.220376 47.154689 \nL 314.830323 43.332116 \nL 315.44027 49.448259 \nL 316.050217 41.803081 \nL 316.660165 41.038547 \nL 317.270112 54.035365 \nL 317.880059 47.154689 \nL 318.490006 47.154689 \nL 319.099953 38.74501 \nL 319.7099 37.215974 \nL 320.319848 36.45144 \nL 320.929795 45.625654 \nL 321.539742 45.625654 \nL 322.149689 41.803081 \nL 322.759636 41.803081 \nL 323.369583 43.332116 \nL 323.979531 43.332116 \nL 324.589478 41.803081 \nL 325.199425 49.448259 \nL 325.809372 48.683725 \nL 326.419319 46.390188 \nL 327.029266 39.509511 \nL 327.639214 47.154689 \nL 328.249161 50.977294 \nL 328.859108 49.448259 \nL 329.469055 38.74501 \nL 330.079002 46.390188 \nL 330.688949 47.154689 \nL 331.298897 44.096618 \nL 331.908844 44.861152 \nL 332.518791 37.215974 \nL 333.128738 43.332116 \nL 333.738685 38.74501 \nL 334.348632 39.509511 \nL 334.95858 50.21276 \nL 335.568527 46.390188 \nL 336.178474 37.215974 \nL 336.788421 50.21276 \nL 337.398368 41.038547 \nL 338.008315 44.861152 \nL 338.618263 51.741796 \nL 339.22821 43.332116 \nL 339.838157 49.448259 \nL 340.448104 44.861152 \nL 341.058051 45.625654 \nL 341.667998 54.799867 \nL 342.277946 44.096618 \nL 342.887893 38.74501 \nL 343.49784 44.861152 \nL 344.107787 41.803081 \nL 344.717734 44.861152 \nL 345.327681 43.332116 \nL 345.937629 42.567582 \nL 346.547576 41.038547 \nL 347.157523 41.038547 \nL 347.76747 50.977294 \nL 348.377417 51.741796 \nL 348.987365 41.803081 \nL 349.597312 43.332116 \nL 350.207259 45.625654 \nL 350.817206 42.567582 \nL 351.427153 41.803081 \nL 352.0371 38.74501 \nL 352.647048 47.154689 \nL 353.256995 42.567582 \nL 354.476889 48.683725 \nL 355.086836 43.332116 \nL 355.696783 47.919223 \nL 356.306731 41.803081 \nL 356.916678 47.154689 \nL 357.526625 47.919223 \nL 358.136572 47.154689 \nL 358.746519 47.919223 \nL 359.356466 44.096618 \nL 360.576361 50.21276 \nL 361.186308 41.038547 \nL 361.796255 37.980476 \nL 362.406202 42.567582 \nL 363.016149 45.625654 \nL 363.626097 57.857938 \nL 364.236044 50.21276 \nL 365.455938 42.567582 \nL 366.065885 41.803081 \nL 366.675832 44.861152 \nL 367.28578 54.799867 \nL 367.895727 47.919223 \nL 368.505674 44.096618 \nL 369.115621 41.803081 \nL 369.725568 40.274045 \nL 369.725568 40.274045 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p5bddf25372)\" d=\"M 65.361932 178.169161 \nL 65.971879 108.226434 \nL 66.581826 144.7183 \nL 67.191773 150.800278 \nL 67.80172 108.226434 \nL 69.021615 74.775573 \nL 69.631562 77.816578 \nL 70.241509 65.652623 \nL 70.851456 74.775573 \nL 71.461403 80.857551 \nL 72.071351 68.693595 \nL 72.681298 71.7346 \nL 73.291245 86.939528 \nL 73.901192 96.062479 \nL 74.511139 86.939528 \nL 75.121086 93.021506 \nL 75.731034 86.939528 \nL 76.340981 56.52964 \nL 76.950928 77.816578 \nL 77.560875 80.857551 \nL 78.170822 80.857551 \nL 78.780769 71.7346 \nL 79.390717 56.52964 \nL 80.610611 68.693595 \nL 81.220558 65.652623 \nL 81.830505 74.775573 \nL 82.440452 74.775573 \nL 83.0504 83.898523 \nL 83.660347 68.693595 \nL 84.270294 62.611617 \nL 84.880241 62.611617 \nL 85.490188 71.7346 \nL 86.100135 68.693595 \nL 86.710083 59.570645 \nL 87.32003 71.7346 \nL 87.929977 80.857551 \nL 88.539924 68.693595 \nL 89.149871 77.816578 \nL 89.759819 68.693595 \nL 90.369766 71.7346 \nL 90.979713 56.52964 \nL 91.58966 53.488667 \nL 92.199607 56.52964 \nL 92.809554 53.488667 \nL 93.419502 53.488667 \nL 94.639396 59.570645 \nL 95.249343 50.447662 \nL 95.85929 59.570645 \nL 96.469237 41.324712 \nL 97.079185 56.52964 \nL 97.689132 59.570645 \nL 98.299079 74.775573 \nL 98.909026 56.52964 \nL 99.518973 53.488667 \nL 100.12892 59.570645 \nL 100.738868 56.52964 \nL 101.348815 47.406689 \nL 101.958762 50.447662 \nL 102.568709 44.365684 \nL 103.178656 50.447662 \nL 103.788603 53.488667 \nL 105.008498 47.406689 \nL 105.618445 50.447662 \nL 106.228392 47.406689 \nL 106.838339 56.52964 \nL 107.448286 62.611617 \nL 108.058234 50.447662 \nL 108.668181 47.406689 \nL 109.278128 53.488667 \nL 109.888075 53.488667 \nL 110.498022 56.52964 \nL 111.717917 56.52964 \nL 112.327864 68.693595 \nL 112.937811 50.447662 \nL 113.547758 56.52964 \nL 114.767652 56.52964 \nL 115.3776 53.488667 \nL 115.987547 44.365684 \nL 116.597494 53.488667 \nL 117.817388 53.488667 \nL 118.427335 47.406689 \nL 119.037283 50.447662 \nL 119.64723 47.406689 \nL 120.257177 53.488667 \nL 120.867124 47.406689 \nL 122.087018 47.406689 \nL 122.696966 56.52964 \nL 123.306913 50.447662 \nL 123.91686 53.488667 \nL 124.526807 47.406689 \nL 125.136754 50.447662 \nL 125.746701 50.447662 \nL 126.356649 53.488667 \nL 126.966596 50.447662 \nL 127.576543 50.447662 \nL 128.18649 44.365684 \nL 129.406384 50.447662 \nL 130.016332 47.406689 \nL 130.626279 50.447662 \nL 131.236226 44.365684 \nL 131.846173 44.365684 \nL 132.45612 50.447662 \nL 133.676015 44.365684 \nL 134.285962 53.488667 \nL 134.895909 47.406689 \nL 135.505856 47.406689 \nL 136.115803 53.488667 \nL 136.72575 47.406689 \nL 137.945645 41.324712 \nL 138.555592 41.324712 \nL 139.165539 44.365684 \nL 139.775486 44.365684 \nL 140.385433 50.447662 \nL 140.995381 44.365684 \nL 142.825222 44.365684 \nL 143.435169 53.488667 \nL 144.045116 44.365684 \nL 144.655064 50.447662 \nL 145.874958 50.447662 \nL 146.484905 47.406689 \nL 147.094852 53.488667 \nL 147.704799 47.406689 \nL 148.924694 47.406689 \nL 150.144588 53.488667 \nL 150.754535 53.488667 \nL 151.364482 47.406689 \nL 151.97443 47.406689 \nL 152.584377 50.447662 \nL 155.024165 50.447662 \nL 155.634113 56.52964 \nL 156.24406 50.447662 \nL 157.463954 44.365684 \nL 158.073901 50.447662 \nL 158.683848 50.447662 \nL 159.293796 53.488667 \nL 159.903743 50.447662 \nL 160.51369 50.447662 \nL 161.123637 47.406689 \nL 161.733584 53.488667 \nL 162.343531 56.52964 \nL 162.953479 53.488667 \nL 163.563426 56.52964 \nL 164.173373 50.447662 \nL 164.78332 50.447662 \nL 166.003214 56.52964 \nL 166.613162 50.447662 \nL 167.223109 56.52964 \nL 167.833056 47.406689 \nL 168.443003 50.447662 \nL 169.05295 47.406689 \nL 169.662897 56.52964 \nL 170.272845 59.570645 \nL 170.882792 38.283739 \nL 171.492739 47.406689 \nL 172.102686 50.447662 \nL 172.712633 38.283739 \nL 173.32258 47.406689 \nL 173.932528 47.406689 \nL 175.762369 56.52964 \nL 176.982263 50.447662 \nL 177.592211 41.324712 \nL 178.202158 47.406689 \nL 178.812105 50.447662 \nL 179.422052 47.406689 \nL 180.031999 50.447662 \nL 181.251894 44.365684 \nL 181.861841 47.406689 \nL 182.471788 47.406689 \nL 183.081735 53.488667 \nL 183.691682 56.52964 \nL 184.301629 50.447662 \nL 184.911577 50.447662 \nL 185.521524 47.406689 \nL 186.131471 50.447662 \nL 186.741418 38.283739 \nL 187.351365 50.447662 \nL 187.961312 56.52964 \nL 188.57126 50.447662 \nL 189.791154 56.52964 \nL 190.401101 50.447662 \nL 191.011048 50.447662 \nL 191.620995 44.365684 \nL 192.230943 50.447662 \nL 192.84089 50.447662 \nL 193.450837 53.488667 \nL 194.060784 50.447662 \nL 195.280678 50.447662 \nL 195.890626 53.488667 \nL 196.500573 50.447662 \nL 197.11052 50.447662 \nL 197.720467 53.488667 \nL 198.330414 53.488667 \nL 198.940361 50.447662 \nL 199.550309 44.365684 \nL 200.160256 50.447662 \nL 200.770203 50.447662 \nL 201.38015 41.324712 \nL 201.990097 47.406689 \nL 203.209992 53.488667 \nL 203.819939 47.406689 \nL 205.039833 53.488667 \nL 205.64978 47.406689 \nL 206.259727 47.406689 \nL 206.869675 41.324712 \nL 207.479622 50.447662 \nL 208.089569 47.406689 \nL 208.699516 47.406689 \nL 209.309463 53.488667 \nL 209.91941 53.488667 \nL 210.529358 47.406689 \nL 211.139305 50.447662 \nL 211.749252 50.447662 \nL 212.359199 56.52964 \nL 212.969146 47.406689 \nL 213.579093 47.406689 \nL 214.189041 50.447662 \nL 214.798988 44.365684 \nL 215.408935 50.447662 \nL 216.018882 47.406689 \nL 217.238776 47.406689 \nL 217.848724 53.488667 \nL 218.458671 47.406689 \nL 219.068618 53.488667 \nL 219.678565 47.406689 \nL 220.288512 53.488667 \nL 221.508407 53.488667 \nL 222.118354 50.447662 \nL 222.728301 50.447662 \nL 223.338248 47.406689 \nL 223.948195 50.447662 \nL 224.558142 47.406689 \nL 225.16809 53.488667 \nL 225.778037 50.447662 \nL 226.387984 50.447662 \nL 226.997931 47.406689 \nL 228.217825 47.406689 \nL 230.047667 38.283739 \nL 230.657614 47.406689 \nL 231.267561 47.406689 \nL 231.877508 44.365684 \nL 232.487456 50.447662 \nL 233.097403 53.488667 \nL 234.317297 47.406689 \nL 234.927244 50.447662 \nL 235.537191 47.406689 \nL 236.147139 47.406689 \nL 236.757086 53.488667 \nL 237.367033 53.488667 \nL 237.97698 56.52964 \nL 239.196874 44.365684 \nL 240.416769 50.447662 \nL 241.026716 50.447662 \nL 241.636663 47.406689 \nL 242.856557 47.406689 \nL 243.466505 50.447662 \nL 244.076452 47.406689 \nL 244.686399 47.406689 \nL 245.296346 50.447662 \nL 245.906293 47.406689 \nL 246.51624 47.406689 \nL 247.736135 53.488667 \nL 248.346082 47.406689 \nL 248.956029 50.447662 \nL 249.565976 44.365684 \nL 250.175923 44.365684 \nL 250.785871 50.447662 \nL 252.005765 44.365684 \nL 252.615712 47.406689 \nL 253.225659 47.406689 \nL 253.835606 44.365684 \nL 254.445554 53.488667 \nL 255.055501 41.324712 \nL 255.665448 44.365684 \nL 256.275395 53.488667 \nL 256.885342 50.447662 \nL 258.105237 50.447662 \nL 258.715184 56.52964 \nL 259.325131 59.570645 \nL 260.545025 47.406689 \nL 261.154972 53.488667 \nL 261.76492 50.447662 \nL 262.374867 44.365684 \nL 262.984814 44.365684 \nL 263.594761 47.406689 \nL 264.204708 56.52964 \nL 264.814655 50.447662 \nL 266.03455 50.447662 \nL 266.644497 53.488667 \nL 267.254444 44.365684 \nL 267.864391 53.488667 \nL 268.474338 53.488667 \nL 269.084286 44.365684 \nL 270.30418 44.365684 \nL 270.914127 50.447662 \nL 271.524074 47.406689 \nL 272.134021 35.242734 \nL 273.353916 47.406689 \nL 273.963863 47.406689 \nL 274.57381 38.283739 \nL 275.183757 47.406689 \nL 275.793704 44.365684 \nL 276.403652 47.406689 \nL 277.013599 38.283739 \nL 277.623546 41.324712 \nL 278.233493 35.242734 \nL 279.453387 41.324712 \nL 280.063335 41.324712 \nL 280.673282 47.406689 \nL 281.893176 47.406689 \nL 282.503123 41.324712 \nL 284.332965 41.324712 \nL 284.942912 47.406689 \nL 285.552859 44.365684 \nL 286.162806 50.447662 \nL 286.772753 44.365684 \nL 287.382701 44.365684 \nL 287.992648 50.447662 \nL 288.602595 44.365684 \nL 289.212542 47.406689 \nL 289.822489 47.406689 \nL 290.432436 38.283739 \nL 291.042384 47.406689 \nL 291.652331 44.365684 \nL 292.262278 44.365684 \nL 292.872225 47.406689 \nL 294.702067 47.406689 \nL 295.312014 44.365684 \nL 295.921961 44.365684 \nL 296.531908 41.324712 \nL 297.751802 53.488667 \nL 298.36175 35.242734 \nL 298.971697 44.365684 \nL 299.581644 47.406689 \nL 300.191591 41.324712 \nL 300.801538 32.201761 \nL 301.411485 35.242734 \nL 302.63138 47.406689 \nL 303.241327 41.324712 \nL 304.461221 53.488667 \nL 305.071168 38.283739 \nL 305.681116 47.406689 \nL 306.291063 50.447662 \nL 307.510957 44.365684 \nL 308.120904 47.406689 \nL 308.730851 44.365684 \nL 309.340799 50.447662 \nL 309.950746 38.283739 \nL 310.560693 53.488667 \nL 311.17064 44.365684 \nL 311.780587 50.447662 \nL 312.390534 44.365684 \nL 313.000482 41.324712 \nL 313.610429 44.365684 \nL 315.44027 44.365684 \nL 316.050217 47.406689 \nL 317.880059 47.406689 \nL 318.490006 50.447662 \nL 319.099953 47.406689 \nL 319.7099 41.324712 \nL 320.319848 50.447662 \nL 321.539742 38.283739 \nL 322.149689 56.52964 \nL 322.759636 41.324712 \nL 323.369583 35.242734 \nL 323.979531 50.447662 \nL 324.589478 47.406689 \nL 325.199425 47.406689 \nL 325.809372 44.365684 \nL 326.419319 44.365684 \nL 327.029266 38.283739 \nL 327.639214 47.406689 \nL 328.249161 41.324712 \nL 328.859108 53.488667 \nL 329.469055 44.365684 \nL 330.079002 44.365684 \nL 330.688949 41.324712 \nL 331.298897 41.324712 \nL 331.908844 47.406689 \nL 332.518791 38.283739 \nL 333.128738 44.365684 \nL 333.738685 38.283739 \nL 334.348632 47.406689 \nL 334.95858 47.406689 \nL 335.568527 50.447662 \nL 336.178474 47.406689 \nL 336.788421 41.324712 \nL 337.398368 50.447662 \nL 338.008315 50.447662 \nL 338.618263 47.406689 \nL 339.22821 50.447662 \nL 339.838157 47.406689 \nL 340.448104 38.283739 \nL 341.058051 41.324712 \nL 341.667998 50.447662 \nL 342.277946 44.365684 \nL 342.887893 53.488667 \nL 343.49784 50.447662 \nL 344.107787 50.447662 \nL 345.327681 38.283739 \nL 345.937629 47.406689 \nL 346.547576 50.447662 \nL 347.157523 44.365684 \nL 347.76747 47.406689 \nL 348.377417 53.488667 \nL 348.987365 41.324712 \nL 349.597312 38.283739 \nL 350.207259 38.283739 \nL 350.817206 44.365684 \nL 351.427153 47.406689 \nL 352.0371 35.242734 \nL 352.647048 38.283739 \nL 353.256995 50.447662 \nL 353.866942 44.365684 \nL 354.476889 44.365684 \nL 355.086836 38.283739 \nL 355.696783 38.283739 \nL 356.306731 47.406689 \nL 356.916678 44.365684 \nL 357.526625 44.365684 \nL 358.136572 47.406689 \nL 358.746519 35.242734 \nL 359.356466 38.283739 \nL 359.966414 35.242734 \nL 360.576361 38.283739 \nL 361.186308 47.406689 \nL 361.796255 44.365684 \nL 362.406202 44.365684 \nL 363.016149 41.324712 \nL 363.626097 35.242734 \nL 364.236044 41.324712 \nL 364.845991 41.324712 \nL 365.455938 44.365684 \nL 366.675832 38.283739 \nL 367.28578 50.447662 \nL 367.895727 35.242734 \nL 368.505674 38.283739 \nL 369.115621 35.242734 \nL 369.725568 47.406689 \nL 369.725568 47.406689 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 239.758125 \nL 50.14375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 239.758125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 239.758125 \nL 384.94375 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 22.318125 \nL 384.94375 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- model accuracy -->\n    <defs>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path id=\"DejaVuSans-32\"/>\n    </defs>\n    <g transform=\"translate(169.882188 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"158.59375\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"222.070312\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"283.59375\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"311.376953\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"343.164062\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"404.443359\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"459.423828\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"514.404297\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"577.783203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"618.896484\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"680.175781\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"735.15625\" xlink:href=\"#DejaVuSans-121\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 57.14375 59.674375 \nL 112.41875 59.674375 \nQ 114.41875 59.674375 114.41875 57.674375 \nL 114.41875 29.318125 \nQ 114.41875 27.318125 112.41875 27.318125 \nL 57.14375 27.318125 \nQ 55.14375 27.318125 55.14375 29.318125 \nL 55.14375 57.674375 \nQ 55.14375 59.674375 57.14375 59.674375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 59.14375 35.416562 \nL 79.14375 35.416562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_18\">\n     <!-- train -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <g transform=\"translate(87.14375 38.916562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 59.14375 50.094687 \nL 79.14375 50.094687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_19\">\n     <!-- test -->\n     <defs>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(87.14375 53.594687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5bddf25372\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
                        "image/png": "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\n"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "source": [
                "from utils.visualising import plot_model_history\n",
                "\n",
                "plot_model_history(history)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Train evaluation:\n23/23 - 0s - loss: 0.4436 - tp: 204.0000 - fp: 38.0000 - tn: 406.0000 - fn: 64.0000 - accuracy: 0.8567 - precision: 0.8430 - recall: 0.7612 - auc: 0.8882\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<Figure size 288x216 with 2 Axes>",
                        "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"222.954375pt\" version=\"1.1\" viewBox=\"0 0 268.71775 222.954375\" width=\"268.71775pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 222.954375 \nL 268.71775 222.954375 \nL 268.71775 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 37.55625 185.398125 \nL 216.11625 185.398125 \nL 216.11625 22.318125 \nL 37.55625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"QuadMesh_1\">\n    <path clip-path=\"url(#pb2b4aeb935)\" d=\"M 216.11625 185.398125 \nL 126.83625 185.398125 \nL 126.83625 103.858125 \nL 216.11625 103.858125 \nL 216.11625 185.398125 \n\" style=\"fill:#b71657;\"/>\n    <path clip-path=\"url(#pb2b4aeb935)\" d=\"M 126.83625 185.398125 \nL 37.55625 185.398125 \nL 37.55625 103.858125 \nL 126.83625 103.858125 \nL 126.83625 185.398125 \n\" style=\"fill:#03051a;\"/>\n    <path clip-path=\"url(#pb2b4aeb935)\" d=\"M 216.11625 103.858125 \nL 126.83625 103.858125 \nL 126.83625 22.318125 \nL 216.11625 22.318125 \nL 216.11625 103.858125 \n\" style=\"fill:#1b112b;\"/>\n    <path clip-path=\"url(#pb2b4aeb935)\" d=\"M 126.83625 103.858125 \nL 37.55625 103.858125 \nL 37.55625 22.318125 \nL 126.83625 22.318125 \nL 126.83625 103.858125 \n\" style=\"fill:#faebdd;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m19bf5a338d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.47625\" xlink:href=\"#m19bf5a338d\" y=\"185.398125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(168.295 199.996562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"82.19625\" xlink:href=\"#m19bf5a338d\" y=\"185.398125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(79.015 199.996562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- True -->\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     </defs>\n     <g transform=\"translate(116.219063 213.674688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"150.826172\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m99596d0829\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#m99596d0829\" y=\"144.628125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(28.476563 147.191406)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#m99596d0829\" y=\"63.088125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(28.476563 65.651406)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Predicted -->\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <g transform=\"translate(14.798438 127.328437)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"344.388672\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"405.912109\" xlink:href=\"#DejaVuSans-100\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_7\">\n    <!-- 204 -->\n    <defs>\n     <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n     <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(161.9325 147.3875)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-50\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-52\"/>\n    </g>\n   </g>\n   <g id=\"text_8\">\n    <!-- 38 -->\n    <defs>\n     <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n     <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(75.83375 147.3875)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-51\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n    </g>\n   </g>\n   <g id=\"text_9\">\n    <!-- 64 -->\n    <defs>\n     <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(165.11375 65.8475)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-54\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n    </g>\n   </g>\n   <g id=\"text_10\">\n    <!-- 406 -->\n    <g style=\"fill:#262626;\" transform=\"translate(72.6525 65.8475)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-54\"/>\n    </g>\n   </g>\n   <g id=\"text_11\">\n    <!-- Confusion matrix for train -->\n    <defs>\n     <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n    </defs>\n    <g transform=\"translate(49.498125 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"194.384766\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"229.589844\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"292.96875\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"345.068359\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"372.851562\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"434.033203\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"497.412109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"529.199219\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"626.611328\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"687.890625\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"727.099609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"768.212891\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"795.996094\" xlink:href=\"#DejaVuSans-120\"/>\n     <use x=\"855.175781\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"886.962891\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"922.167969\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"983.349609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1024.462891\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1056.25\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1095.458984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1136.572266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1197.851562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1225.634766\" xlink:href=\"#DejaVuSans-110\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pbb43f26276)\" d=\"M 227.27625 185.398125 \nL 227.27625 184.761094 \nL 227.27625 22.955156 \nL 227.27625 22.318125 \nL 235.43025 22.318125 \nL 235.43025 22.955156 \nL 235.43025 184.761094 \nL 235.43025 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\n   </g>\n   <image height=\"163\" id=\"imageabf0d29e65\" transform=\"scale(1 -1)translate(0 -163)\" width=\"8\" x=\"227\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAgAAACjCAYAAAC31F+mAAAABHNCSVQICAgIfAhkiAAAARhJREFUWIXV19ENwyAMhGFs3C7R/ddsugEfklPU5hXr9x0+AYnH83WNxVcRsVoflZEqAGGKwAKKnCmR4y9stgmyScINNkd7mgdsknCgxQ02leoTNtef89AnrD2MUcXIaR84izJh/e0Q6KLf4vuEi6EFgTY5TbdY3po7hBJhY6tR0Bd5hLCu8EbNC4SNwIhADSS0NQQ3KlDgFiSoIEnItzQs1zcIiWlZZCK1FWphmwhE5ZTNtsgNwnL9Dg0hQuKYs8gQAc+HDUKkbj0c56Xz3i3uIHRf5iToct4QOUEYnkWXYA1swbf9AZH6Kf4JmyLoHVSDoZ0i9FuYwFmI4GmKcKAF8zAfX9dgm8rkAZv9PLR3Mto2f0IDpvkB0+4oS7K28UwAAAAASUVORK5CYII=\" y=\"-21\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL 3.5 0 \n\" id=\"me692183cd8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#me692183cd8\" y=\"180.080299\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(242.43025 183.879518)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#me692183cd8\" y=\"157.92269\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 100 -->\n      <g transform=\"translate(242.43025 161.721909)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#me692183cd8\" y=\"135.765082\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 150 -->\n      <g transform=\"translate(242.43025 139.5643)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#me692183cd8\" y=\"113.607473\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 200 -->\n      <g transform=\"translate(242.43025 117.406692)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#me692183cd8\" y=\"91.449864\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 250 -->\n      <g transform=\"translate(242.43025 95.249083)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#me692183cd8\" y=\"69.292255\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 300 -->\n      <g transform=\"translate(242.43025 73.091474)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#me692183cd8\" y=\"47.134647\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 350 -->\n      <g transform=\"translate(242.43025 50.933865)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#me692183cd8\" y=\"24.977038\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 400 -->\n      <g transform=\"translate(242.43025 28.776257)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 227.27625 185.398125 \nL 227.27625 184.761094 \nL 227.27625 22.955156 \nL 227.27625 22.318125 \nL 235.43025 22.318125 \nL 235.43025 22.955156 \nL 235.43025 184.761094 \nL 235.43025 185.398125 \nz\n\" style=\"fill:none;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb2b4aeb935\">\n   <rect height=\"163.08\" width=\"178.56\" x=\"37.55625\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"pbb43f26276\">\n   <rect height=\"163.08\" width=\"8.154\" x=\"227.27625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
                        "image/png": "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\n"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "source": [
                "from utils.visualising import draw_confusion_matrix\n",
                "\n",
                "print(\"Train evaluation:\")\n",
                "evaluation_train = model.evaluate(X_train, y_train, verbose=2)\n",
                "draw_confusion_matrix(evaluation_train, \"train\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Valid evaluation:\n6/6 - 0s - loss: 0.4855 - tp: 57.0000 - fp: 13.0000 - tn: 92.0000 - fn: 17.0000 - accuracy: 0.8324 - precision: 0.8143 - recall: 0.7703 - auc: 0.9093\n"
                },
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<Figure size 288x216 with 2 Axes>",
                        "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"222.954375pt\" version=\"1.1\" viewBox=\"0 0 262.35525 222.954375\" width=\"262.35525pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 222.954375 \nL 262.35525 222.954375 \nL 262.35525 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 37.55625 185.398125 \nL 216.11625 185.398125 \nL 216.11625 22.318125 \nL 37.55625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"QuadMesh_1\">\n    <path clip-path=\"url(#pe222cbc03e)\" d=\"M 216.11625 185.398125 \nL 126.83625 185.398125 \nL 126.83625 103.858125 \nL 216.11625 103.858125 \nL 216.11625 185.398125 \n\" style=\"fill:#dd2c45;\"/>\n    <path clip-path=\"url(#pe222cbc03e)\" d=\"M 126.83625 185.398125 \nL 37.55625 185.398125 \nL 37.55625 103.858125 \nL 126.83625 103.858125 \nL 126.83625 185.398125 \n\" style=\"fill:#03051a;\"/>\n    <path clip-path=\"url(#pe222cbc03e)\" d=\"M 216.11625 103.858125 \nL 126.83625 103.858125 \nL 126.83625 22.318125 \nL 216.11625 22.318125 \nL 216.11625 103.858125 \n\" style=\"fill:#130d25;\"/>\n    <path clip-path=\"url(#pe222cbc03e)\" d=\"M 126.83625 103.858125 \nL 37.55625 103.858125 \nL 37.55625 22.318125 \nL 126.83625 22.318125 \nL 126.83625 103.858125 \n\" style=\"fill:#faebdd;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m773872543d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.47625\" xlink:href=\"#m773872543d\" y=\"185.398125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(168.295 199.996562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"82.19625\" xlink:href=\"#m773872543d\" y=\"185.398125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(79.015 199.996562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- True -->\n     <defs>\n      <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     </defs>\n     <g transform=\"translate(116.219063 213.674688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"150.826172\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md38261129c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#md38261129c\" y=\"144.628125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(28.476563 147.191406)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#md38261129c\" y=\"63.088125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(28.476563 65.651406)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Predicted -->\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <g transform=\"translate(14.798438 127.328437)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"344.388672\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"405.912109\" xlink:href=\"#DejaVuSans-100\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_7\">\n    <!-- 57 -->\n    <defs>\n     <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n     <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(165.11375 147.3875)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-53\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n    </g>\n   </g>\n   <g id=\"text_8\">\n    <!-- 13 -->\n    <defs>\n     <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(75.83375 147.3875)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-49\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-51\"/>\n    </g>\n   </g>\n   <g id=\"text_9\">\n    <!-- 17 -->\n    <g style=\"fill:#ffffff;\" transform=\"translate(165.11375 65.8475)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-49\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n    </g>\n   </g>\n   <g id=\"text_10\">\n    <!-- 92 -->\n    <defs>\n     <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n     <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(75.83375 65.8475)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-57\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n    </g>\n   </g>\n   <g id=\"text_11\">\n    <!-- Confusion matrix for valid -->\n    <defs>\n     <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n    </defs>\n    <g transform=\"translate(49.092188 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"194.384766\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"229.589844\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"292.96875\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"345.068359\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"372.851562\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"434.033203\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"497.412109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"529.199219\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"626.611328\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"687.890625\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"727.099609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"768.212891\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"795.996094\" xlink:href=\"#DejaVuSans-120\"/>\n     <use x=\"855.175781\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"886.962891\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"922.167969\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"983.349609\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1024.462891\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1056.25\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1115.429688\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1176.708984\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1204.492188\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1232.275391\" xlink:href=\"#DejaVuSans-100\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p74c5d6438b)\" d=\"M 227.27625 185.398125 \nL 227.27625 184.761094 \nL 227.27625 22.955156 \nL 227.27625 22.318125 \nL 235.43025 22.318125 \nL 235.43025 22.955156 \nL 235.43025 184.761094 \nL 235.43025 185.398125 \nz\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\n   </g>\n   <image height=\"163\" id=\"imageb9ffe3b0cc\" transform=\"scale(1 -1)translate(0 -163)\" width=\"8\" x=\"227\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAgAAACjCAYAAAC31F+mAAAABHNCSVQICAgIfAhkiAAAARhJREFUWIXV19ENwyAMhGFs3C7R/ddsugEfklPU5hXr9x0+AYnH83WNxVcRsVoflZEqAGGKwAKKnCmR4y9stgmyScINNkd7mgdsknCgxQ02leoTNtef89AnrD2MUcXIaR84izJh/e0Q6KLf4vuEi6EFgTY5TbdY3po7hBJhY6tR0Bd5hLCu8EbNC4SNwIhADSS0NQQ3KlDgFiSoIEnItzQs1zcIiWlZZCK1FWphmwhE5ZTNtsgNwnL9Dg0hQuKYs8gQAc+HDUKkbj0c56Xz3i3uIHRf5iToct4QOUEYnkWXYA1swbf9AZH6Kf4JmyLoHVSDoZ0i9FuYwFmI4GmKcKAF8zAfX9dgm8rkAZv9PLR3Mto2f0IDpvkB0+4oS7K28UwAAAAASUVORK5CYII=\" y=\"-21\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL 3.5 0 \n\" id=\"m780c4497e2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m780c4497e2\" y=\"170.947998\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(242.43025 174.747217)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m780c4497e2\" y=\"150.30496\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 30 -->\n      <g transform=\"translate(242.43025 154.104179)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m780c4497e2\" y=\"129.661922\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(242.43025 133.461141)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m780c4497e2\" y=\"109.018884\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 50 -->\n      <g transform=\"translate(242.43025 112.818103)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m780c4497e2\" y=\"88.375847\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(242.43025 92.175065)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m780c4497e2\" y=\"67.732809\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 70 -->\n      <g transform=\"translate(242.43025 71.532027)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m780c4497e2\" y=\"47.089771\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 80 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(242.43025 50.888989)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.43025\" xlink:href=\"#m780c4497e2\" y=\"26.446733\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 90 -->\n      <g transform=\"translate(242.43025 30.245951)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 227.27625 185.398125 \nL 227.27625 184.761094 \nL 227.27625 22.955156 \nL 227.27625 22.318125 \nL 235.43025 22.318125 \nL 235.43025 22.955156 \nL 235.43025 184.761094 \nL 235.43025 185.398125 \nz\n\" style=\"fill:none;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe222cbc03e\">\n   <rect height=\"163.08\" width=\"178.56\" x=\"37.55625\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p74c5d6438b\">\n   <rect height=\"163.08\" width=\"8.154\" x=\"227.27625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
                        "image/png": "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\n"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "source": [
                "print(\"Valid evaluation:\")\n",
                "evaluation_valid = model.evaluate(X_valid, y_valid, verbose=2)\n",
                "draw_confusion_matrix(evaluation_valid, \"valid\")"
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": [
                "## Predict with DL model"
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": [
                "from utils.predicting import store_predictions"
            ],
            "execution_count": 15,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "tags": []
            },
            "cell_type": "code",
            "source": [
                "store_predictions(model, X_pred=X_pred, index=X_pred.index, submission_name=\"dl\")"
            ],
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "dl:\n[0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0\n 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0\n 1 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 1 0 1 0]...\n"
                }
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "COMET INFO: ---------------------------\nCOMET INFO: Comet.ml Experiment Summary\nCOMET INFO: ---------------------------\nCOMET INFO:   Data:\nCOMET INFO:     display_summary_level : 1\nCOMET INFO:     url                   : https://www.comet.ml/witalia008/titanic/f2deeaa19a724c2ebc2b0b8266342b82\nCOMET INFO:   Metrics [count] (min, max):\nCOMET INFO:     accuracy [500]                 : (0.49719101190567017, 0.8539325594902039)\nCOMET INFO:     auc [500]                      : (0.5135765075683594, 0.8821517825126648)\nCOMET INFO:     batch_accuracy [1500]          : (0.375, 1.0)\nCOMET INFO:     batch_auc [1500]               : (0.3333333134651184, 1.0)\nCOMET INFO:     batch_fn [1500]                : (0.0, 148.0)\nCOMET INFO:     batch_fp [1500]                : (0.0, 202.0)\nCOMET INFO:     batch_loss [1500]              : (0.28788846731185913, 6.919724464416504)\nCOMET INFO:     batch_precision [1500]         : (0.2666666805744171, 1.0)\nCOMET INFO:     batch_recall [1500]            : (0.23076923191547394, 1.0)\nCOMET INFO:     batch_tn [1500]                : (4.0, 399.0)\nCOMET INFO:     batch_tp [1500]                : (2.0, 204.0)\nCOMET INFO:     epoch_duration [500]           : (0.1431806100299582, 3.930071040056646)\nCOMET INFO:     fn [500]                       : (54.0, 155.0)\nCOMET INFO:     fp [500]                       : (28.0, 218.0)\nCOMET INFO:     loss [500]                     : (0.4827035963535309, 6.344738006591797)\nCOMET INFO:     precision [500]                : (0.36994218826293945, 0.8709677457809448)\nCOMET INFO:     recall [500]                   : (0.4216417968273163, 0.7985074520111084)\nCOMET INFO:     tn [500]                       : (226.0, 416.0)\nCOMET INFO:     tp [500]                       : (113.0, 214.0)\nCOMET INFO:     val_accuracy [500]             : (0.5921787619590759, 0.8603351712226868)\nCOMET INFO:     val_auc [500]                  : (0.6499356031417847, 0.913256049156189)\nCOMET INFO:     val_fn [500]                   : (8.0, 56.0)\nCOMET INFO:     val_fp [500]                   : (10.0, 56.0)\nCOMET INFO:     val_loss [500]                 : (0.4690128564834595, 5.700989723205566)\nCOMET INFO:     val_precision [500]            : (0.5142857432365417, 0.841269850730896)\nCOMET INFO:     val_recall [500]               : (0.2432432472705841, 0.8918918967247009)\nCOMET INFO:     val_tn [500]                   : (49.0, 95.0)\nCOMET INFO:     val_tp [500]                   : (18.0, 66.0)\nCOMET INFO:     validate_batch_accuracy [500]  : (0.53125, 0.90625)\nCOMET INFO:     validate_batch_auc [500]       : (0.6527777314186096, 0.9563491940498352)\nCOMET INFO:     validate_batch_fn [500]        : (2.0, 13.0)\nCOMET INFO:     validate_batch_fp [500]        : (0.0, 9.0)\nCOMET INFO:     validate_batch_loss [500]      : (0.41728687286376953, 5.7036285400390625)\nCOMET INFO:     validate_batch_precision [500] : (0.3333333432674408, 1.0)\nCOMET INFO:     validate_batch_recall [500]    : (0.0714285746216774, 0.8571428656578064)\nCOMET INFO:     validate_batch_tn [500]        : (9.0, 18.0)\nCOMET INFO:     validate_batch_tp [500]        : (1.0, 12.0)\nCOMET INFO:   Others:\nCOMET INFO:     trainable_params : 7457\nCOMET INFO:   Parameters:\nCOMET INFO:     Adam_amsgrad       : 1\nCOMET INFO:     Adam_beta_1        : 0.9\nCOMET INFO:     Adam_beta_2        : 0.999\nCOMET INFO:     Adam_decay         : 1\nCOMET INFO:     Adam_epsilon       : 1e-07\nCOMET INFO:     Adam_learning_rate : 0.001\nCOMET INFO:     Adam_name          : Adam\nCOMET INFO:     Optimizer          : Adam\nCOMET INFO:     epochs             : 500\nCOMET INFO:     steps              : 23\nCOMET INFO:   Uploads:\nCOMET INFO:     code                     : 1 (4 KB)\nCOMET INFO:     environment details      : 1\nCOMET INFO:     filename                 : 1\nCOMET INFO:     git metadata             : 1\nCOMET INFO:     git-patch (uncompressed) : 1 (3 KB)\nCOMET INFO:     installed packages       : 1\nCOMET INFO:     model graph              : 1\nCOMET INFO:     notebook                 : 1\nCOMET INFO:     os packages              : 1\nCOMET INFO: ---------------------------\nCOMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
                }
            ],
            "source": [
                "if experiment:\n",
                "    experiment.end()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "language": "python",
            "display_name": "Python 3.7.6 64-bit",
            "name": "python37664bit26036f9dd6fd4360b35d7bd99f8e1f11"
        },
        "language_info": {
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "version": "3.7.6-final",
            "file_extension": ".py",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "name": "python",
            "mimetype": "text/x-python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}