{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONHASHSEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\r\n",
      "  Downloading pip-20.1.1-py2.py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 2.7 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: pip\r\n",
      "  Attempting uninstall: pip\r\n",
      "    Found existing installation: pip 20.1\r\n",
      "    Uninstalling pip-20.1:\r\n",
      "      Successfully uninstalled pip-20.1\r\n",
      "Successfully installed pip-20.1.1\r\n",
      "Collecting keras-tuner\r\n",
      "  Downloading keras-tuner-1.0.1.tar.gz (54 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 54 kB 976 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (0.18.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (1.18.1)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (0.8.7)\r\n",
      "Collecting terminaltables\r\n",
      "  Downloading terminaltables-3.1.0.tar.gz (12 kB)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (4.45.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (2.23.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (1.4.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (0.22.2.post1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (2020.4.5.1)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (1.24.3)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (3.0.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->keras-tuner) (0.14.1)\r\n",
      "Building wheels for collected packages: keras-tuner, terminaltables\r\n",
      "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-py3-none-any.whl size=73198 sha256=034a21b7d3f885e2ed7c3b69a29f4825e24a5b88341d02ae6c306c2a2ae8bf3d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/cf/2f/1a1749d3a3650fac3305a8d7f9237b6de7c41068e2f8520ca2\r\n",
      "  Building wheel for terminaltables (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15354 sha256=2ac8b4a2c60dfab4fa257431691badce332d0a821c720d8e10ff8205c5ff3aee\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/ad/c8/2d98360791161cd3db6daf6b5e730f34021fc9367d5879f497\r\n",
      "Successfully built keras-tuner terminaltables\r\n",
      "Installing collected packages: terminaltables, keras-tuner\r\n",
      "Successfully installed keras-tuner-1.0.1 terminaltables-3.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# notebook\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seed to reprodicibility of random generation\n",
    "SEED = 42\n",
    "\n",
    "DEV_SPLIT=0.2\n",
    "\n",
    "MODE = \"DEV\"\n",
    "# MODE = \"EVAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as python_random\n",
    "\n",
    "# Make sure Keras produces reproducible results.\n",
    "\n",
    "np.random.seed(SEED)\n",
    "python_random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print(physical_devices)\n",
    "for device in (physical_devices or []):\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Read data and extract usable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 9)\n",
      "(891,)\n",
      "(418, 9)\n"
     ]
    }
   ],
   "source": [
    "features = \"Pclass Sex SibSp Parch Fare Embarked Name Cabin Age\".split()\n",
    "\n",
    "X_train_init = train_data[features]\n",
    "Y_train_init = train_data.Survived\n",
    "\n",
    "print(X_train_init.shape)\n",
    "print(Y_train_init.shape)\n",
    "\n",
    "X_test_init = test_data[features]\n",
    "\n",
    "print(X_test_init.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split into train/dev sets\n",
    "## Needs to be done before pre-processing to avoid test-train contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 9) (712,)\n",
      "(179, 9) (179,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# if MODE == \"DEV\":\n",
    "X_train_unproc, X_dev_unproc, Y_train_unproc, Y_dev_unproc = train_test_split(X_train_init, Y_train_init, test_size=DEV_SPLIT, random_state=SEED)\n",
    "\n",
    "print(X_train_unproc.shape, Y_train_unproc.shape)\n",
    "print(X_dev_unproc.shape, Y_dev_unproc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Data observations\n",
    "*Have NaNs:* Age, Fare (some zeros, nans too), Cabin, Embarked\n",
    "*NOTE:* maybe need to approximate missing values using some other technique, like an additional model?\n",
    "\n",
    "* (+) Pclass:\n",
    "  * 1 - 3 number, 1 being the highest\n",
    "  * Range: 1-3\n",
    "  * Current approach: convert to one-hot.\n",
    "  * Previous approaches:\n",
    "      * normalize by 3.\n",
    "* (+) Name:\n",
    "  * has person's title, which could be used (Mr, Ms, Mrs, etc.)\n",
    "  * From title, can infer marital status?\n",
    "  * Current approach: extract titles, replace infrequent ones with \"Others\", convert them to one-hot, and calculate 'Married' based on title (1 - married (Mr, Mrs), -1 - unmarried (Miss, Master), 0 - unknown (other titles))\n",
    "  * Potential improvements: use more titles for getting 'married'; use 'maiden name' in calculation of 'married'; use 'nickname' somehow?\n",
    "* (+) Sex:\n",
    "  * Either male or female\n",
    "  * male: 65%, female: 35%\n",
    "  * Current approach: convert to one-hot.\n",
    "  * Potential improvements: use 1 and -1 for sexes?\n",
    "* (+) Age:\n",
    "  * has fractions if approximated. Has missing values.\n",
    "  * Range: 0.42-80\n",
    "  * Current approach: fill NaN with average in group-by Pclass-Sex, but create a column that identifies missing values. Also, normalize by 80.\n",
    "  * Potential improvements: have a better approximation of age. Convert to age categories?\n",
    "* (+) SibSp:\n",
    "  * how many siblings or spouses on board.\n",
    "  * Range: 0-8\n",
    "  * Current approach: Add to 'Family'.\n",
    "  * Previous approaches:\n",
    "    * normalize by 8.\n",
    "* (+) Parch:\n",
    "  * How many parents/children. (can be 0 for babies, if with nannies)\n",
    "  * Range: 0-6\n",
    "  * Current approach: Add to 'Family'\n",
    "  * Previous approachesL\n",
    "    * normalize by 6.\n",
    "* Ticket:\n",
    "  * A number with some optional letters (which can have some meaning?).\n",
    "  * Has repetitions (maybe for people travelling together).\n",
    "* (+) Fare:\n",
    "  * can have zeros (what do they mean?). Can have omitted (just one in test).\n",
    "  * Range: 0-512.3292\n",
    "  * Current approach: fill nan with mean, normalize by 512.\n",
    "  * Potential improvements: most fare is <= 30 USD, so maybe use fare categories.\n",
    "* (+) Cabin:\n",
    "  * has a lot of omitted values (78%). Can have multiple values (probably for families?).\n",
    "  * One value is a letter with a number. (both probably have meaning and impact?)\n",
    "  * Current approach: convert to one-hot (based on letter), include a 'nan' column for those that are missing values. Create a column for cabin number, and a column to identify missing numbers.\n",
    "  * Potential improvements: maybe cabin number itself doesn't mean much? Also, maybe need to deal with missing values in a different way? Also, maybe deal with multiple values better?\n",
    "* (+) Embarked:\n",
    "  * Either of 3 letters (with different frequency). Has just a few omitted.\n",
    "  * S - 72/65%, C - 19/24%, Q - 9/11%\n",
    "  * Current approach: convert to one-hot matrix (fill 2 missing with mode)\n",
    "  * Potential improvements: somehow take into the account different distribution of embarkation city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X):\n",
    "    # To display all the columns from left to right without breaking into next line.\n",
    "    pd.set_option('display.width', 1500)\n",
    "\n",
    "    import re\n",
    "    \n",
    "    titles = ['Mr', 'Mrs', 'Miss', 'Master', 'Ms', 'Don', 'Rev', 'Dr', 'Mme', 'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess', 'Jonkheer', 'Dona']\n",
    "    \n",
    "    X = X.copy()\n",
    "    \n",
    "    # === Get X - the features. ===\n",
    "\n",
    "    # == Post-process data ==\n",
    "\n",
    "#     if \"SibSp\" in X:\n",
    "#         X.SibSp = X.SibSp.divide(8)\n",
    "\n",
    "#     if \"Parch\" in X:\n",
    "#         X.Parch = X.Parch.divide(6)\n",
    "        \n",
    "    if \"Parch\" in X and \"SibSp\" in X:\n",
    "        X[\"Family\"] = X.Parch + X.SibSp\n",
    "#         X.Family = X.Family.divide(14)\n",
    "        X = X.drop(columns=\"Parch SibSp\".split())\n",
    "\n",
    "    if \"Fare\" in X:\n",
    "        # Since only a few would miss 'fare' value, it's okay to fill with average.\n",
    "        X.Fare = X.Fare.fillna(X.Fare.mean())\n",
    "        \n",
    "        X.Fare = np.where(X.Fare < 50, 1, 2)\n",
    "        \n",
    "#         X.Fare = X.Fare.divide(512)\n",
    "\n",
    "    if \"Embarked\" in X:\n",
    "        X.Embarked = X.Embarked.fillna(X.Embarked.mode()[0])\n",
    "        X.Embarked = X.Embarked.astype(pd.api.types.CategoricalDtype(categories=\"C Q S\".split()))\n",
    "        X = pd.get_dummies(X, columns=[\"Embarked\"])\n",
    "\n",
    "    if \"Name\" in X:\n",
    "        X[\"Title\"] = X.Name.apply(lambda name: re.search(\", ([\\w ]+).\", name).group(1))\n",
    "\n",
    "        # Try to see if the person is married (1), or not (-1), or unknown (0).\n",
    "        X[\"Married\"] = X.Title.apply(lambda title: 1 if title in \"Mrs Mr\".split() else -1 if title in \"Miss Master\".split() else 0)\n",
    "\n",
    "        # Get dummies for title\n",
    "        \n",
    "        # Include all possible values, even those not present in current dataset.\n",
    "#         X.Title = X.Title.astype(pd.api.types.CategoricalDtype(categories=titles))\n",
    "        \n",
    "        # Titles that are rare are converted to 'Others'\n",
    "        important_titles = ['Mr', 'Mrs', 'Miss', 'Master']\n",
    "        X.Title = X.Title.apply(lambda title: title if title in important_titles else \"Others\")\n",
    "        \n",
    "        X = pd.get_dummies(X, columns=[\"Title\"])\n",
    "        \n",
    "        # We don't need the name itself.\n",
    "        X = X.drop(columns=[\"Name\"])\n",
    "        \n",
    "    if \"Cabin\" in X:\n",
    "        X[\"Cabin_Missing\"] = np.where(X.Cabin.isnull(), 1, 0)\n",
    "        X.Cabin = X.Cabin.fillna(\"-\")\n",
    "        \n",
    "#         X[\"Cabin_Number\"] = X.Cabin.apply(lambda cabin: int(re.search(\"\\w(\\d+)\", cabin).group(1)) if len(cabin) > 1 else 0)\n",
    "#         # Do some sort of normalization.\n",
    "#         X.Cabin_Number = X.Cabin_Number.divide(200)\n",
    "#         X[\"Cabin_Number_Missing\"] = np.where(X.Cabin_Number == 0, 1, 0)\n",
    "        \n",
    "        X.Cabin = X.Cabin.apply(lambda cabin: cabin[:1])\n",
    "        \n",
    "        # Convert to one-hot\n",
    "#         X.Cabin = X.Cabin.astype(pd.api.types.CategoricalDtype(categories=list(\"ABCDEFGT\")))\n",
    "#         X = pd.get_dummies(X, columns=[\"Cabin\"], dummy_na=True)\n",
    "\n",
    "        # Convert to numbers with T beeing the lowest deck and S - the highest (sun deck).\n",
    "        X[\"Deck_Level\"] = X.Cabin.apply(lambda cabin: \"SABCDEFGT\".find(cabin[0]))\n",
    "        X = X.drop(columns=[\"Cabin\"])\n",
    "\n",
    "    if \"Age\" in X:\n",
    "        X[\"Age_Missing\"] = np.where(X.Age.isnull(), 1, 0)\n",
    "\n",
    "        # No need to skip 'nan' for Age when calculating mean, as Pandas does that automatically.\n",
    "        # 'transform' will go through each group, and fill its nan values with its mean value.\n",
    "        # Then, all that will be aggregated back into the column, thus replacing nan values with group's mean.\n",
    "        X[\"Age\"] = X.groupby(\"Pclass Sex\".split())[\"Age\"].transform(lambda x: x.fillna(x.mean()))\n",
    "        \n",
    "#         X.Age = X.Age.divide(80)\n",
    "\n",
    "        # Convert age to categories 1 - child, 2 - young, 3 - older, 4 - senile\n",
    "        X.Age = pd.cut(X.Age, bins=[0, 16, 30, 50, 80], labels=False) + 1\n",
    "        \n",
    "    # Needs to be after 'Age', since age is using original Sex column.\n",
    "    if \"Sex\" in X:\n",
    "        X.Sex = X.Sex.astype(pd.api.types.CategoricalDtype(categories=\"male female\".split()))\n",
    "        X = pd.get_dummies(X, columns=[\"Sex\"])\n",
    "\n",
    "    if \"Pclass\" in X:\n",
    "        X = pd.get_dummies(X, columns=[\"Pclass\"])\n",
    "        # Do not normalize small numbers\n",
    "#         X.Pclass = X.Pclass.divide(3)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_clean(X):\n",
    "    import re\n",
    "    \n",
    "    titles = ['Mr', 'Mrs', 'Miss', 'Master', 'Ms', 'Don', 'Rev', 'Dr', 'Mme', 'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess', 'Jonkheer', 'Dona']\n",
    "    \n",
    "    X = X.copy()\n",
    "    \n",
    "    # Remember missing values\n",
    "    for col in \"Age\".split():\n",
    "        X[f\"{col}_Missing\"] = np.where(X[col].isnull(), 1, 0)\n",
    "        \n",
    "    if \"Parch\" in X and \"SibSp\" in X:\n",
    "        X[\"Family\"] = X.Parch + X.SibSp\n",
    "\n",
    "    if \"Fare\" in X:\n",
    "        X.Fare = X.Fare.fillna(X.Fare.mean())\n",
    "        X.Fare = pd.cut(X.Fare, bins=[-1, 15, 30, 50, 70, 100, 600], labels=False) + 1\n",
    "\n",
    "    if \"Embarked\" in X:\n",
    "        X.Embarked = X.Embarked.fillna(X.Embarked.mode()[0])\n",
    "        X.Embarked = X.Embarked.astype(pd.api.types.CategoricalDtype(categories=\"C Q S\".split()))\n",
    "\n",
    "    if \"Name\" in X:\n",
    "        X[\"Title\"] = X.Name.apply(lambda name: re.search(\", ([\\w ]+).\", name).group(1))\n",
    "\n",
    "        important_titles = ['Mr', 'Mrs', 'Miss', 'Master']\n",
    "        X.Title = X.Title.apply(lambda title: title if title in important_titles else \"Others\")\n",
    "\n",
    "#         X[\"Married\"] = X.Title.apply(lambda title: 1 if title in \"Mrs Mr\".split() else -1 if title in \"Miss Master\".split() else 0)\n",
    "        \n",
    "#     if \"Cabin\" in X:   \n",
    "#         X[\"Deck_Level\"] = X.Cabin.fillna(\"-\").apply(lambda cabin: \"SABCDEFGT\".find(cabin[0]))\n",
    "\n",
    "    if \"Age\" in X:\n",
    "        X[\"Age\"] = X.groupby(\"Pclass Sex\".split())[\"Age\"].transform(lambda x: x.fillna(x.mean()))\n",
    "        X.Age = pd.cut(X.Age, bins=[0, 16, 30, 50, 80], labels=False) + 1\n",
    "        \n",
    "    X = X.drop(columns=\"Name Cabin Parch SibSp\".split())\n",
    "        \n",
    "    X = pd.get_dummies(X, columns=\"Sex Embarked Title\".split())\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "     Pclass  Fare  Age  Age_Missing  Family  Sex_female  Sex_male  Embarked_C  \\\n",
      "331       1     2    3            0       0           0         1           0   \n",
      "733       2     1    2            0       0           0         1           0   \n",
      "382       3     1    3            0       0           0         1           0   \n",
      "704       3     1    2            0       1           0         1           0   \n",
      "813       3     3    1            0       6           1         0           0   \n",
      "\n",
      "     Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  Title_Mrs  \\\n",
      "331           0           1             0           0         1          0   \n",
      "733           0           1             0           0         1          0   \n",
      "382           0           1             0           0         1          0   \n",
      "704           0           1             0           0         1          0   \n",
      "813           0           1             0           1         0          0   \n",
      "\n",
      "     Title_Others  \n",
      "331             0  \n",
      "733             0  \n",
      "382             0  \n",
      "704             0  \n",
      "813             0  \n",
      "Dev data:\n",
      "     Pclass  Fare  Age  Age_Missing  Family  Sex_female  Sex_male  Embarked_C  \\\n",
      "709       3     2    2            1       2           0         1           1   \n",
      "439       2     1    3            0       0           0         1           0   \n",
      "840       3     1    2            0       0           0         1           0   \n",
      "720       2     3    1            0       1           1         0           0   \n",
      "39        3     1    1            0       1           1         0           1   \n",
      "\n",
      "     Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  Title_Mrs  \\\n",
      "709           0           0             1           0         0          0   \n",
      "439           0           1             0           0         1          0   \n",
      "840           0           1             0           0         1          0   \n",
      "720           0           1             0           1         0          0   \n",
      "39            0           0             0           1         0          0   \n",
      "\n",
      "     Title_Others  \n",
      "709             0  \n",
      "439             0  \n",
      "840             0  \n",
      "720             0  \n",
      "39              0  \n",
      "Test data:\n",
      "   Pclass  Fare  Age  Age_Missing  Family  Sex_female  Sex_male  Embarked_C  \\\n",
      "0       3     1    3            0       0           0         1           0   \n",
      "1       3     1    3            0       1           1         0           0   \n",
      "2       2     1    4            0       0           0         1           0   \n",
      "3       3     1    2            0       0           0         1           0   \n",
      "4       3     1    2            0       2           1         0           0   \n",
      "\n",
      "   Embarked_Q  Embarked_S  Title_Master  Title_Miss  Title_Mr  Title_Mrs  \\\n",
      "0           1           0             0           0         1          0   \n",
      "1           0           1             0           0         0          1   \n",
      "2           1           0             0           0         1          0   \n",
      "3           0           1             0           0         1          0   \n",
      "4           0           1             0           0         0          1   \n",
      "\n",
      "   Title_Others  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\")\n",
    "X_train = prepare_data_clean(X_train_unproc)\n",
    "Y_train = Y_train_unproc\n",
    "print(X_train.head())\n",
    "\n",
    "print(\"Dev data:\")\n",
    "X_dev = prepare_data_clean(X_dev_unproc)\n",
    "Y_dev = Y_dev_unproc\n",
    "print(X_dev.head())\n",
    "\n",
    "print(\"Test data:\")\n",
    "X_test = prepare_data_clean(X_test_init)\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DL model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'),\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "def get_model(input_size):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(128, input_shape=(input_size,), activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.2),\n",
    "        Dense(20, activation=\"relu\", kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.25),\n",
    "        Dense(6, activation=\"tanh\", kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), metrics=METRICS, loss=\"binary_crossentropy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "died_cnt, survived_cnt = np.bincount(Y_train_init)\n",
    "total_cnt = died_cnt + survived_cnt\n",
    "\n",
    "weight_died = total_cnt / died_cnt / 2\n",
    "weight_survived = total_cnt / survived_cnt / 2\n",
    "\n",
    "class_weights = {0: weight_died, 1: weight_survived}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 712 samples, validate on 179 samples\n",
      "Epoch 1/500\n",
      "712/712 [==============================] - 4s 5ms/sample - loss: 1.7119 - tp: 71.0000 - fp: 111.0000 - tn: 333.0000 - fn: 197.0000 - accuracy: 0.5674 - precision: 0.3901 - recall: 0.2649 - auc: 0.5337 - val_loss: 1.5482 - val_tp: 57.0000 - val_fp: 44.0000 - val_tn: 61.0000 - val_fn: 17.0000 - val_accuracy: 0.6592 - val_precision: 0.5644 - val_recall: 0.7703 - val_auc: 0.7886\n",
      "Epoch 2/500\n",
      "712/712 [==============================] - 0s 202us/sample - loss: 1.4760 - tp: 170.0000 - fp: 164.0000 - tn: 280.0000 - fn: 98.0000 - accuracy: 0.6320 - precision: 0.5090 - recall: 0.6343 - auc: 0.6768 - val_loss: 1.3488 - val_tp: 47.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 27.0000 - val_accuracy: 0.7039 - val_precision: 0.6438 - val_recall: 0.6351 - val_auc: 0.8332\n",
      "Epoch 3/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 1.2907 - tp: 168.0000 - fp: 100.0000 - tn: 344.0000 - fn: 100.0000 - accuracy: 0.7191 - precision: 0.6269 - recall: 0.6269 - auc: 0.7671 - val_loss: 1.1731 - val_tp: 62.0000 - val_fp: 32.0000 - val_tn: 73.0000 - val_fn: 12.0000 - val_accuracy: 0.7542 - val_precision: 0.6596 - val_recall: 0.8378 - val_auc: 0.8400\n",
      "Epoch 4/500\n",
      "712/712 [==============================] - 0s 192us/sample - loss: 1.1697 - tp: 197.0000 - fp: 127.0000 - tn: 317.0000 - fn: 71.0000 - accuracy: 0.7219 - precision: 0.6080 - recall: 0.7351 - auc: 0.7678 - val_loss: 1.0596 - val_tp: 58.0000 - val_fp: 31.0000 - val_tn: 74.0000 - val_fn: 16.0000 - val_accuracy: 0.7374 - val_precision: 0.6517 - val_recall: 0.7838 - val_auc: 0.8567\n",
      "Epoch 5/500\n",
      "712/712 [==============================] - 0s 236us/sample - loss: 1.0694 - tp: 182.0000 - fp: 96.0000 - tn: 348.0000 - fn: 86.0000 - accuracy: 0.7444 - precision: 0.6547 - recall: 0.6791 - auc: 0.7917 - val_loss: 0.9729 - val_tp: 61.0000 - val_fp: 31.0000 - val_tn: 74.0000 - val_fn: 13.0000 - val_accuracy: 0.7542 - val_precision: 0.6630 - val_recall: 0.8243 - val_auc: 0.8508\n",
      "Epoch 6/500\n",
      "712/712 [==============================] - 0s 211us/sample - loss: 0.9830 - tp: 204.0000 - fp: 109.0000 - tn: 335.0000 - fn: 64.0000 - accuracy: 0.7570 - precision: 0.6518 - recall: 0.7612 - auc: 0.8048 - val_loss: 0.9014 - val_tp: 58.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 16.0000 - val_accuracy: 0.7542 - val_precision: 0.6744 - val_recall: 0.7838 - val_auc: 0.8626\n",
      "Epoch 7/500\n",
      "712/712 [==============================] - 0s 197us/sample - loss: 0.8971 - tp: 189.0000 - fp: 72.0000 - tn: 372.0000 - fn: 79.0000 - accuracy: 0.7879 - precision: 0.7241 - recall: 0.7052 - auc: 0.8352 - val_loss: 0.8477 - val_tp: 55.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 19.0000 - val_accuracy: 0.7821 - val_precision: 0.7333 - val_recall: 0.7432 - val_auc: 0.8678\n",
      "Epoch 8/500\n",
      "712/712 [==============================] - 0s 179us/sample - loss: 0.8688 - tp: 203.0000 - fp: 93.0000 - tn: 351.0000 - fn: 65.0000 - accuracy: 0.7781 - precision: 0.6858 - recall: 0.7575 - auc: 0.8158 - val_loss: 0.8053 - val_tp: 54.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 20.0000 - val_accuracy: 0.7765 - val_precision: 0.7297 - val_recall: 0.7297 - val_auc: 0.8720\n",
      "Epoch 9/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.8287 - tp: 195.0000 - fp: 73.0000 - tn: 371.0000 - fn: 73.0000 - accuracy: 0.7949 - precision: 0.7276 - recall: 0.7276 - auc: 0.8188 - val_loss: 0.7716 - val_tp: 62.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 12.0000 - val_accuracy: 0.7709 - val_precision: 0.6813 - val_recall: 0.8378 - val_auc: 0.8643\n",
      "Epoch 10/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.7857 - tp: 199.0000 - fp: 90.0000 - tn: 354.0000 - fn: 69.0000 - accuracy: 0.7767 - precision: 0.6886 - recall: 0.7425 - auc: 0.8262 - val_loss: 0.7416 - val_tp: 54.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 20.0000 - val_accuracy: 0.7821 - val_precision: 0.7397 - val_recall: 0.7297 - val_auc: 0.8745\n",
      "Epoch 11/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.7550 - tp: 201.0000 - fp: 87.0000 - tn: 357.0000 - fn: 67.0000 - accuracy: 0.7837 - precision: 0.6979 - recall: 0.7500 - auc: 0.8330 - val_loss: 0.7096 - val_tp: 58.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 16.0000 - val_accuracy: 0.7989 - val_precision: 0.7436 - val_recall: 0.7838 - val_auc: 0.8732\n",
      "Epoch 12/500\n",
      "712/712 [==============================] - 0s 191us/sample - loss: 0.7437 - tp: 198.0000 - fp: 84.0000 - tn: 360.0000 - fn: 70.0000 - accuracy: 0.7837 - precision: 0.7021 - recall: 0.7388 - auc: 0.8225 - val_loss: 0.6875 - val_tp: 56.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 18.0000 - val_accuracy: 0.7933 - val_precision: 0.7467 - val_recall: 0.7568 - val_auc: 0.8759\n",
      "Epoch 13/500\n",
      "712/712 [==============================] - 0s 189us/sample - loss: 0.7043 - tp: 208.0000 - fp: 82.0000 - tn: 362.0000 - fn: 60.0000 - accuracy: 0.8006 - precision: 0.7172 - recall: 0.7761 - auc: 0.8422 - val_loss: 0.6708 - val_tp: 56.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 18.0000 - val_accuracy: 0.7877 - val_precision: 0.7368 - val_recall: 0.7568 - val_auc: 0.8790\n",
      "Epoch 14/500\n",
      "712/712 [==============================] - 0s 216us/sample - loss: 0.6909 - tp: 203.0000 - fp: 78.0000 - tn: 366.0000 - fn: 65.0000 - accuracy: 0.7992 - precision: 0.7224 - recall: 0.7575 - auc: 0.8373 - val_loss: 0.6521 - val_tp: 59.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 15.0000 - val_accuracy: 0.7933 - val_precision: 0.7284 - val_recall: 0.7973 - val_auc: 0.8760\n",
      "Epoch 15/500\n",
      "712/712 [==============================] - 0s 219us/sample - loss: 0.6814 - tp: 204.0000 - fp: 80.0000 - tn: 364.0000 - fn: 64.0000 - accuracy: 0.7978 - precision: 0.7183 - recall: 0.7612 - auc: 0.8337 - val_loss: 0.6382 - val_tp: 59.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 15.0000 - val_accuracy: 0.7765 - val_precision: 0.7024 - val_recall: 0.7973 - val_auc: 0.8754\n",
      "Epoch 16/500\n",
      "712/712 [==============================] - 0s 197us/sample - loss: 0.6816 - tp: 208.0000 - fp: 90.0000 - tn: 354.0000 - fn: 60.0000 - accuracy: 0.7893 - precision: 0.6980 - recall: 0.7761 - auc: 0.8216 - val_loss: 0.6309 - val_tp: 59.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 15.0000 - val_accuracy: 0.7821 - val_precision: 0.7108 - val_recall: 0.7973 - val_auc: 0.8754\n",
      "Epoch 17/500\n",
      "712/712 [==============================] - 0s 229us/sample - loss: 0.6630 - tp: 208.0000 - fp: 73.0000 - tn: 371.0000 - fn: 60.0000 - accuracy: 0.8132 - precision: 0.7402 - recall: 0.7761 - auc: 0.8317 - val_loss: 0.6169 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8827\n",
      "Epoch 18/500\n",
      "712/712 [==============================] - 0s 231us/sample - loss: 0.6501 - tp: 211.0000 - fp: 78.0000 - tn: 366.0000 - fn: 57.0000 - accuracy: 0.8104 - precision: 0.7301 - recall: 0.7873 - auc: 0.8345 - val_loss: 0.6132 - val_tp: 56.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 18.0000 - val_accuracy: 0.7933 - val_precision: 0.7467 - val_recall: 0.7568 - val_auc: 0.8814\n",
      "Epoch 19/500\n",
      "712/712 [==============================] - 0s 211us/sample - loss: 0.6360 - tp: 198.0000 - fp: 69.0000 - tn: 375.0000 - fn: 70.0000 - accuracy: 0.8048 - precision: 0.7416 - recall: 0.7388 - auc: 0.8378 - val_loss: 0.5992 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8825\n",
      "Epoch 20/500\n",
      "712/712 [==============================] - 0s 230us/sample - loss: 0.6220 - tp: 209.0000 - fp: 71.0000 - tn: 373.0000 - fn: 59.0000 - accuracy: 0.8174 - precision: 0.7464 - recall: 0.7799 - auc: 0.8431 - val_loss: 0.5893 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8824\n",
      "Epoch 21/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.6234 - tp: 201.0000 - fp: 60.0000 - tn: 384.0000 - fn: 67.0000 - accuracy: 0.8216 - precision: 0.7701 - recall: 0.7500 - auc: 0.8353 - val_loss: 0.5883 - val_tp: 59.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 15.0000 - val_accuracy: 0.7821 - val_precision: 0.7108 - val_recall: 0.7973 - val_auc: 0.8778\n",
      "Epoch 22/500\n",
      "712/712 [==============================] - 0s 191us/sample - loss: 0.6107 - tp: 211.0000 - fp: 78.0000 - tn: 366.0000 - fn: 57.0000 - accuracy: 0.8104 - precision: 0.7301 - recall: 0.7873 - auc: 0.8407 - val_loss: 0.5839 - val_tp: 57.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 17.0000 - val_accuracy: 0.7989 - val_precision: 0.7500 - val_recall: 0.7703 - val_auc: 0.8825\n",
      "Epoch 23/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.6250 - tp: 203.0000 - fp: 66.0000 - tn: 378.0000 - fn: 65.0000 - accuracy: 0.8160 - precision: 0.7546 - recall: 0.7575 - auc: 0.8277 - val_loss: 0.5822 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8808\n",
      "Epoch 24/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.5875 - tp: 203.0000 - fp: 65.0000 - tn: 379.0000 - fn: 65.0000 - accuracy: 0.8174 - precision: 0.7575 - recall: 0.7575 - auc: 0.8546 - val_loss: 0.5686 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8829\n",
      "Epoch 25/500\n",
      "712/712 [==============================] - 0s 196us/sample - loss: 0.5946 - tp: 205.0000 - fp: 72.0000 - tn: 372.0000 - fn: 63.0000 - accuracy: 0.8104 - precision: 0.7401 - recall: 0.7649 - auc: 0.8465 - val_loss: 0.5626 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8822\n",
      "Epoch 26/500\n",
      "712/712 [==============================] - 0s 225us/sample - loss: 0.5931 - tp: 204.0000 - fp: 69.0000 - tn: 375.0000 - fn: 64.0000 - accuracy: 0.8132 - precision: 0.7473 - recall: 0.7612 - auc: 0.8434 - val_loss: 0.5601 - val_tp: 59.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 15.0000 - val_accuracy: 0.8101 - val_precision: 0.7564 - val_recall: 0.7973 - val_auc: 0.8831\n",
      "Epoch 27/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.5661 - tp: 209.0000 - fp: 72.0000 - tn: 372.0000 - fn: 59.0000 - accuracy: 0.8160 - precision: 0.7438 - recall: 0.7799 - auc: 0.8581 - val_loss: 0.5534 - val_tp: 58.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 16.0000 - val_accuracy: 0.8045 - val_precision: 0.7532 - val_recall: 0.7838 - val_auc: 0.8838\n",
      "Epoch 28/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.5690 - tp: 205.0000 - fp: 53.0000 - tn: 391.0000 - fn: 63.0000 - accuracy: 0.8371 - precision: 0.7946 - recall: 0.7649 - auc: 0.8580 - val_loss: 0.5532 - val_tp: 61.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 13.0000 - val_accuracy: 0.7821 - val_precision: 0.7011 - val_recall: 0.8243 - val_auc: 0.8804\n",
      "Epoch 29/500\n",
      "712/712 [==============================] - 0s 195us/sample - loss: 0.5544 - tp: 210.0000 - fp: 86.0000 - tn: 358.0000 - fn: 58.0000 - accuracy: 0.7978 - precision: 0.7095 - recall: 0.7836 - auc: 0.8647 - val_loss: 0.5485 - val_tp: 57.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 17.0000 - val_accuracy: 0.8045 - val_precision: 0.7600 - val_recall: 0.7703 - val_auc: 0.8867\n",
      "Epoch 30/500\n",
      "712/712 [==============================] - 0s 197us/sample - loss: 0.5907 - tp: 197.0000 - fp: 69.0000 - tn: 375.0000 - fn: 71.0000 - accuracy: 0.8034 - precision: 0.7406 - recall: 0.7351 - auc: 0.8373 - val_loss: 0.5524 - val_tp: 62.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 12.0000 - val_accuracy: 0.7821 - val_precision: 0.6966 - val_recall: 0.8378 - val_auc: 0.8794\n",
      "Epoch 31/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.5856 - tp: 199.0000 - fp: 64.0000 - tn: 380.0000 - fn: 69.0000 - accuracy: 0.8132 - precision: 0.7567 - recall: 0.7425 - auc: 0.8425 - val_loss: 0.5481 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8832\n",
      "Epoch 32/500\n",
      "712/712 [==============================] - 0s 216us/sample - loss: 0.5606 - tp: 205.0000 - fp: 72.0000 - tn: 372.0000 - fn: 63.0000 - accuracy: 0.8104 - precision: 0.7401 - recall: 0.7649 - auc: 0.8519 - val_loss: 0.5417 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8862\n",
      "Epoch 33/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.5444 - tp: 204.0000 - fp: 62.0000 - tn: 382.0000 - fn: 64.0000 - accuracy: 0.8230 - precision: 0.7669 - recall: 0.7612 - auc: 0.8681 - val_loss: 0.5395 - val_tp: 57.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 17.0000 - val_accuracy: 0.7989 - val_precision: 0.7500 - val_recall: 0.7703 - val_auc: 0.8849\n",
      "Epoch 34/500\n",
      "712/712 [==============================] - 0s 179us/sample - loss: 0.5622 - tp: 207.0000 - fp: 79.0000 - tn: 365.0000 - fn: 61.0000 - accuracy: 0.8034 - precision: 0.7238 - recall: 0.7724 - auc: 0.8517 - val_loss: 0.5361 - val_tp: 63.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 11.0000 - val_accuracy: 0.7821 - val_precision: 0.6923 - val_recall: 0.8514 - val_auc: 0.8793\n",
      "Epoch 35/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.5425 - tp: 210.0000 - fp: 63.0000 - tn: 381.0000 - fn: 58.0000 - accuracy: 0.8301 - precision: 0.7692 - recall: 0.7836 - auc: 0.8607 - val_loss: 0.5332 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8874\n",
      "Epoch 36/500\n",
      "712/712 [==============================] - 0s 198us/sample - loss: 0.5557 - tp: 209.0000 - fp: 72.0000 - tn: 372.0000 - fn: 59.0000 - accuracy: 0.8160 - precision: 0.7438 - recall: 0.7799 - auc: 0.8558 - val_loss: 0.5374 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8869\n",
      "Epoch 37/500\n",
      "712/712 [==============================] - 0s 216us/sample - loss: 0.5590 - tp: 204.0000 - fp: 76.0000 - tn: 368.0000 - fn: 64.0000 - accuracy: 0.8034 - precision: 0.7286 - recall: 0.7612 - auc: 0.8545 - val_loss: 0.5308 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8846\n",
      "Epoch 38/500\n",
      "712/712 [==============================] - 0s 186us/sample - loss: 0.5696 - tp: 198.0000 - fp: 60.0000 - tn: 384.0000 - fn: 70.0000 - accuracy: 0.8174 - precision: 0.7674 - recall: 0.7388 - auc: 0.8390 - val_loss: 0.5448 - val_tp: 65.0000 - val_fp: 32.0000 - val_tn: 73.0000 - val_fn: 9.0000 - val_accuracy: 0.7709 - val_precision: 0.6701 - val_recall: 0.8784 - val_auc: 0.8773\n",
      "Epoch 39/500\n",
      "712/712 [==============================] - 0s 205us/sample - loss: 0.5557 - tp: 213.0000 - fp: 73.0000 - tn: 371.0000 - fn: 55.0000 - accuracy: 0.8202 - precision: 0.7448 - recall: 0.7948 - auc: 0.8475 - val_loss: 0.5328 - val_tp: 60.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 14.0000 - val_accuracy: 0.7877 - val_precision: 0.7143 - val_recall: 0.8108 - val_auc: 0.8833\n",
      "Epoch 40/500\n",
      "712/712 [==============================] - 0s 201us/sample - loss: 0.5417 - tp: 209.0000 - fp: 66.0000 - tn: 378.0000 - fn: 59.0000 - accuracy: 0.8244 - precision: 0.7600 - recall: 0.7799 - auc: 0.8601 - val_loss: 0.5293 - val_tp: 59.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 15.0000 - val_accuracy: 0.8045 - val_precision: 0.7468 - val_recall: 0.7973 - val_auc: 0.8851\n",
      "Epoch 41/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.5456 - tp: 204.0000 - fp: 61.0000 - tn: 383.0000 - fn: 64.0000 - accuracy: 0.8244 - precision: 0.7698 - recall: 0.7612 - auc: 0.8575 - val_loss: 0.5263 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8855\n",
      "Epoch 42/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.5572 - tp: 206.0000 - fp: 76.0000 - tn: 368.0000 - fn: 62.0000 - accuracy: 0.8062 - precision: 0.7305 - recall: 0.7687 - auc: 0.8456 - val_loss: 0.5268 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8864\n",
      "Epoch 43/500\n",
      "712/712 [==============================] - 0s 188us/sample - loss: 0.5261 - tp: 210.0000 - fp: 63.0000 - tn: 381.0000 - fn: 58.0000 - accuracy: 0.8301 - precision: 0.7692 - recall: 0.7836 - auc: 0.8696 - val_loss: 0.5201 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8894\n",
      "Epoch 44/500\n",
      "712/712 [==============================] - 0s 199us/sample - loss: 0.5500 - tp: 209.0000 - fp: 63.0000 - tn: 381.0000 - fn: 59.0000 - accuracy: 0.8287 - precision: 0.7684 - recall: 0.7799 - auc: 0.8559 - val_loss: 0.5188 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8874\n",
      "Epoch 45/500\n",
      "712/712 [==============================] - 0s 216us/sample - loss: 0.5411 - tp: 200.0000 - fp: 67.0000 - tn: 377.0000 - fn: 68.0000 - accuracy: 0.8104 - precision: 0.7491 - recall: 0.7463 - auc: 0.8563 - val_loss: 0.5170 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8866\n",
      "Epoch 46/500\n",
      "712/712 [==============================] - 0s 261us/sample - loss: 0.5476 - tp: 206.0000 - fp: 69.0000 - tn: 375.0000 - fn: 62.0000 - accuracy: 0.8160 - precision: 0.7491 - recall: 0.7687 - auc: 0.8513 - val_loss: 0.5217 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8869\n",
      "Epoch 47/500\n",
      "712/712 [==============================] - 0s 238us/sample - loss: 0.5481 - tp: 205.0000 - fp: 72.0000 - tn: 372.0000 - fn: 63.0000 - accuracy: 0.8104 - precision: 0.7401 - recall: 0.7649 - auc: 0.8503 - val_loss: 0.5157 - val_tp: 58.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 16.0000 - val_accuracy: 0.8101 - val_precision: 0.7632 - val_recall: 0.7838 - val_auc: 0.8902\n",
      "Epoch 48/500\n",
      "712/712 [==============================] - 0s 228us/sample - loss: 0.5268 - tp: 209.0000 - fp: 72.0000 - tn: 372.0000 - fn: 59.0000 - accuracy: 0.8160 - precision: 0.7438 - recall: 0.7799 - auc: 0.8653 - val_loss: 0.5138 - val_tp: 57.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 17.0000 - val_accuracy: 0.8101 - val_precision: 0.7703 - val_recall: 0.7703 - val_auc: 0.8884\n",
      "Epoch 49/500\n",
      "712/712 [==============================] - 0s 217us/sample - loss: 0.5586 - tp: 205.0000 - fp: 73.0000 - tn: 371.0000 - fn: 63.0000 - accuracy: 0.8090 - precision: 0.7374 - recall: 0.7649 - auc: 0.8391 - val_loss: 0.5175 - val_tp: 61.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 13.0000 - val_accuracy: 0.7933 - val_precision: 0.7176 - val_recall: 0.8243 - val_auc: 0.8869\n",
      "Epoch 50/500\n",
      "712/712 [==============================] - 0s 210us/sample - loss: 0.5389 - tp: 209.0000 - fp: 60.0000 - tn: 384.0000 - fn: 59.0000 - accuracy: 0.8329 - precision: 0.7770 - recall: 0.7799 - auc: 0.8522 - val_loss: 0.5172 - val_tp: 61.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 13.0000 - val_accuracy: 0.7765 - val_precision: 0.6932 - val_recall: 0.8243 - val_auc: 0.8851\n",
      "Epoch 51/500\n",
      "712/712 [==============================] - 0s 261us/sample - loss: 0.5479 - tp: 204.0000 - fp: 78.0000 - tn: 366.0000 - fn: 64.0000 - accuracy: 0.8006 - precision: 0.7234 - recall: 0.7612 - auc: 0.8492 - val_loss: 0.5138 - val_tp: 64.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 10.0000 - val_accuracy: 0.7877 - val_precision: 0.6957 - val_recall: 0.8649 - val_auc: 0.8887\n",
      "Epoch 52/500\n",
      "712/712 [==============================] - 0s 257us/sample - loss: 0.5316 - tp: 201.0000 - fp: 60.0000 - tn: 384.0000 - fn: 67.0000 - accuracy: 0.8216 - precision: 0.7701 - recall: 0.7500 - auc: 0.8599 - val_loss: 0.5159 - val_tp: 64.0000 - val_fp: 31.0000 - val_tn: 74.0000 - val_fn: 10.0000 - val_accuracy: 0.7709 - val_precision: 0.6737 - val_recall: 0.8649 - val_auc: 0.8871\n",
      "Epoch 53/500\n",
      "712/712 [==============================] - 0s 334us/sample - loss: 0.5391 - tp: 206.0000 - fp: 72.0000 - tn: 372.0000 - fn: 62.0000 - accuracy: 0.8118 - precision: 0.7410 - recall: 0.7687 - auc: 0.8513 - val_loss: 0.5145 - val_tp: 62.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 12.0000 - val_accuracy: 0.7821 - val_precision: 0.6966 - val_recall: 0.8378 - val_auc: 0.8860\n",
      "Epoch 54/500\n",
      "712/712 [==============================] - 0s 199us/sample - loss: 0.5465 - tp: 209.0000 - fp: 78.0000 - tn: 366.0000 - fn: 59.0000 - accuracy: 0.8076 - precision: 0.7282 - recall: 0.7799 - auc: 0.8487 - val_loss: 0.5131 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8919\n",
      "Epoch 55/500\n",
      "712/712 [==============================] - 0s 188us/sample - loss: 0.5354 - tp: 204.0000 - fp: 71.0000 - tn: 373.0000 - fn: 64.0000 - accuracy: 0.8104 - precision: 0.7418 - recall: 0.7612 - auc: 0.8549 - val_loss: 0.5128 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8927\n",
      "Epoch 56/500\n",
      "712/712 [==============================] - 0s 190us/sample - loss: 0.5308 - tp: 203.0000 - fp: 57.0000 - tn: 387.0000 - fn: 65.0000 - accuracy: 0.8287 - precision: 0.7808 - recall: 0.7575 - auc: 0.8560 - val_loss: 0.5174 - val_tp: 64.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 10.0000 - val_accuracy: 0.7821 - val_precision: 0.6882 - val_recall: 0.8649 - val_auc: 0.8878\n",
      "Epoch 57/500\n",
      "712/712 [==============================] - 0s 211us/sample - loss: 0.5403 - tp: 207.0000 - fp: 83.0000 - tn: 361.0000 - fn: 61.0000 - accuracy: 0.7978 - precision: 0.7138 - recall: 0.7724 - auc: 0.8504 - val_loss: 0.5083 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8919\n",
      "Epoch 58/500\n",
      "712/712 [==============================] - 0s 195us/sample - loss: 0.5357 - tp: 205.0000 - fp: 74.0000 - tn: 370.0000 - fn: 63.0000 - accuracy: 0.8076 - precision: 0.7348 - recall: 0.7649 - auc: 0.8497 - val_loss: 0.5080 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8902\n",
      "Epoch 59/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.5311 - tp: 207.0000 - fp: 71.0000 - tn: 373.0000 - fn: 61.0000 - accuracy: 0.8146 - precision: 0.7446 - recall: 0.7724 - auc: 0.8526 - val_loss: 0.5084 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8898\n",
      "Epoch 60/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.5429 - tp: 202.0000 - fp: 71.0000 - tn: 373.0000 - fn: 66.0000 - accuracy: 0.8076 - precision: 0.7399 - recall: 0.7537 - auc: 0.8458 - val_loss: 0.5040 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8912\n",
      "Epoch 61/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.5214 - tp: 212.0000 - fp: 81.0000 - tn: 363.0000 - fn: 56.0000 - accuracy: 0.8076 - precision: 0.7235 - recall: 0.7910 - auc: 0.8669 - val_loss: 0.5067 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8921\n",
      "Epoch 62/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.5371 - tp: 202.0000 - fp: 57.0000 - tn: 387.0000 - fn: 66.0000 - accuracy: 0.8272 - precision: 0.7799 - recall: 0.7537 - auc: 0.8493 - val_loss: 0.5149 - val_tp: 65.0000 - val_fp: 30.0000 - val_tn: 75.0000 - val_fn: 9.0000 - val_accuracy: 0.7821 - val_precision: 0.6842 - val_recall: 0.8784 - val_auc: 0.8859\n",
      "Epoch 63/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.5305 - tp: 205.0000 - fp: 66.0000 - tn: 378.0000 - fn: 63.0000 - accuracy: 0.8188 - precision: 0.7565 - recall: 0.7649 - auc: 0.8569 - val_loss: 0.5024 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8902\n",
      "Epoch 64/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.5211 - tp: 198.0000 - fp: 59.0000 - tn: 385.0000 - fn: 70.0000 - accuracy: 0.8188 - precision: 0.7704 - recall: 0.7388 - auc: 0.8637 - val_loss: 0.5022 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8912\n",
      "Epoch 65/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.5483 - tp: 209.0000 - fp: 68.0000 - tn: 376.0000 - fn: 59.0000 - accuracy: 0.8216 - precision: 0.7545 - recall: 0.7799 - auc: 0.8403 - val_loss: 0.5010 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8937\n",
      "Epoch 66/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.5161 - tp: 212.0000 - fp: 63.0000 - tn: 381.0000 - fn: 56.0000 - accuracy: 0.8329 - precision: 0.7709 - recall: 0.7910 - auc: 0.8672 - val_loss: 0.5014 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8909\n",
      "Epoch 67/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.5182 - tp: 203.0000 - fp: 63.0000 - tn: 381.0000 - fn: 65.0000 - accuracy: 0.8202 - precision: 0.7632 - recall: 0.7575 - auc: 0.8626 - val_loss: 0.4937 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8931\n",
      "Epoch 68/500\n",
      "712/712 [==============================] - 0s 205us/sample - loss: 0.5279 - tp: 207.0000 - fp: 79.0000 - tn: 365.0000 - fn: 61.0000 - accuracy: 0.8034 - precision: 0.7238 - recall: 0.7724 - auc: 0.8559 - val_loss: 0.4981 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8943\n",
      "Epoch 69/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.5297 - tp: 205.0000 - fp: 67.0000 - tn: 377.0000 - fn: 63.0000 - accuracy: 0.8174 - precision: 0.7537 - recall: 0.7649 - auc: 0.8530 - val_loss: 0.5027 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8910\n",
      "Epoch 70/500\n",
      "712/712 [==============================] - 0s 190us/sample - loss: 0.5209 - tp: 208.0000 - fp: 64.0000 - tn: 380.0000 - fn: 60.0000 - accuracy: 0.8258 - precision: 0.7647 - recall: 0.7761 - auc: 0.8540 - val_loss: 0.5020 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8925\n",
      "Epoch 71/500\n",
      "712/712 [==============================] - 0s 194us/sample - loss: 0.5138 - tp: 208.0000 - fp: 70.0000 - tn: 374.0000 - fn: 60.0000 - accuracy: 0.8174 - precision: 0.7482 - recall: 0.7761 - auc: 0.8655 - val_loss: 0.4962 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8921\n",
      "Epoch 72/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 0.5260 - tp: 208.0000 - fp: 67.0000 - tn: 377.0000 - fn: 60.0000 - accuracy: 0.8216 - precision: 0.7564 - recall: 0.7761 - auc: 0.8555 - val_loss: 0.4993 - val_tp: 61.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 13.0000 - val_accuracy: 0.8156 - val_precision: 0.7531 - val_recall: 0.8243 - val_auc: 0.8916\n",
      "Epoch 73/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.5123 - tp: 204.0000 - fp: 63.0000 - tn: 381.0000 - fn: 64.0000 - accuracy: 0.8216 - precision: 0.7640 - recall: 0.7612 - auc: 0.8620 - val_loss: 0.4982 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8949\n",
      "Epoch 74/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.5219 - tp: 207.0000 - fp: 76.0000 - tn: 368.0000 - fn: 61.0000 - accuracy: 0.8076 - precision: 0.7314 - recall: 0.7724 - auc: 0.8571 - val_loss: 0.5042 - val_tp: 54.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 20.0000 - val_accuracy: 0.8156 - val_precision: 0.8060 - val_recall: 0.7297 - val_auc: 0.8922\n",
      "Epoch 75/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.5363 - tp: 199.0000 - fp: 58.0000 - tn: 386.0000 - fn: 69.0000 - accuracy: 0.8216 - precision: 0.7743 - recall: 0.7425 - auc: 0.8488 - val_loss: 0.4959 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8903\n",
      "Epoch 76/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.5157 - tp: 203.0000 - fp: 65.0000 - tn: 379.0000 - fn: 65.0000 - accuracy: 0.8174 - precision: 0.7575 - recall: 0.7575 - auc: 0.8610 - val_loss: 0.4976 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8917\n",
      "Epoch 77/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.5157 - tp: 203.0000 - fp: 61.0000 - tn: 383.0000 - fn: 65.0000 - accuracy: 0.8230 - precision: 0.7689 - recall: 0.7575 - auc: 0.8564 - val_loss: 0.4950 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8918\n",
      "Epoch 78/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.5194 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8585 - val_loss: 0.4945 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8921\n",
      "Epoch 79/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.5202 - tp: 212.0000 - fp: 78.0000 - tn: 366.0000 - fn: 56.0000 - accuracy: 0.8118 - precision: 0.7310 - recall: 0.7910 - auc: 0.8606 - val_loss: 0.5057 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8910\n",
      "Epoch 80/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.5308 - tp: 199.0000 - fp: 54.0000 - tn: 390.0000 - fn: 69.0000 - accuracy: 0.8272 - precision: 0.7866 - recall: 0.7425 - auc: 0.8484 - val_loss: 0.4968 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8905\n",
      "Epoch 81/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.5130 - tp: 209.0000 - fp: 69.0000 - tn: 375.0000 - fn: 59.0000 - accuracy: 0.8202 - precision: 0.7518 - recall: 0.7799 - auc: 0.8631 - val_loss: 0.4920 - val_tp: 62.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 12.0000 - val_accuracy: 0.8268 - val_precision: 0.7654 - val_recall: 0.8378 - val_auc: 0.8921\n",
      "Epoch 82/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.5291 - tp: 206.0000 - fp: 67.0000 - tn: 377.0000 - fn: 62.0000 - accuracy: 0.8188 - precision: 0.7546 - recall: 0.7687 - auc: 0.8500 - val_loss: 0.4996 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8936\n",
      "Epoch 83/500\n",
      "712/712 [==============================] - 0s 261us/sample - loss: 0.5018 - tp: 209.0000 - fp: 71.0000 - tn: 373.0000 - fn: 59.0000 - accuracy: 0.8174 - precision: 0.7464 - recall: 0.7799 - auc: 0.8747 - val_loss: 0.4893 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8933\n",
      "Epoch 84/500\n",
      "712/712 [==============================] - 0s 218us/sample - loss: 0.5126 - tp: 210.0000 - fp: 68.0000 - tn: 376.0000 - fn: 58.0000 - accuracy: 0.8230 - precision: 0.7554 - recall: 0.7836 - auc: 0.8647 - val_loss: 0.4930 - val_tp: 62.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 12.0000 - val_accuracy: 0.8436 - val_precision: 0.7949 - val_recall: 0.8378 - val_auc: 0.8914\n",
      "Epoch 85/500\n",
      "712/712 [==============================] - 0s 190us/sample - loss: 0.4953 - tp: 208.0000 - fp: 63.0000 - tn: 381.0000 - fn: 60.0000 - accuracy: 0.8272 - precision: 0.7675 - recall: 0.7761 - auc: 0.8766 - val_loss: 0.4907 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8939\n",
      "Epoch 86/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.5198 - tp: 199.0000 - fp: 61.0000 - tn: 383.0000 - fn: 69.0000 - accuracy: 0.8174 - precision: 0.7654 - recall: 0.7425 - auc: 0.8637 - val_loss: 0.4921 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8937\n",
      "Epoch 87/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.5213 - tp: 208.0000 - fp: 76.0000 - tn: 368.0000 - fn: 60.0000 - accuracy: 0.8090 - precision: 0.7324 - recall: 0.7761 - auc: 0.8567 - val_loss: 0.4936 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8943\n",
      "Epoch 88/500\n",
      "712/712 [==============================] - 0s 154us/sample - loss: 0.5130 - tp: 206.0000 - fp: 66.0000 - tn: 378.0000 - fn: 62.0000 - accuracy: 0.8202 - precision: 0.7574 - recall: 0.7687 - auc: 0.8623 - val_loss: 0.4943 - val_tp: 56.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 18.0000 - val_accuracy: 0.8101 - val_precision: 0.7778 - val_recall: 0.7568 - val_auc: 0.8940\n",
      "Epoch 89/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.5135 - tp: 206.0000 - fp: 67.0000 - tn: 377.0000 - fn: 62.0000 - accuracy: 0.8188 - precision: 0.7546 - recall: 0.7687 - auc: 0.8616 - val_loss: 0.4913 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8930\n",
      "Epoch 90/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.5076 - tp: 204.0000 - fp: 52.0000 - tn: 392.0000 - fn: 64.0000 - accuracy: 0.8371 - precision: 0.7969 - recall: 0.7612 - auc: 0.8675 - val_loss: 0.4981 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8939\n",
      "Epoch 91/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.5078 - tp: 206.0000 - fp: 70.0000 - tn: 374.0000 - fn: 62.0000 - accuracy: 0.8146 - precision: 0.7464 - recall: 0.7687 - auc: 0.8623 - val_loss: 0.4915 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8950\n",
      "Epoch 92/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.5189 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8574 - val_loss: 0.4970 - val_tp: 62.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 12.0000 - val_accuracy: 0.7877 - val_precision: 0.7045 - val_recall: 0.8378 - val_auc: 0.8941\n",
      "Epoch 93/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.5124 - tp: 209.0000 - fp: 61.0000 - tn: 383.0000 - fn: 59.0000 - accuracy: 0.8315 - precision: 0.7741 - recall: 0.7799 - auc: 0.8618 - val_loss: 0.4944 - val_tp: 62.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 12.0000 - val_accuracy: 0.7821 - val_precision: 0.6966 - val_recall: 0.8378 - val_auc: 0.8921\n",
      "Epoch 94/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.5138 - tp: 207.0000 - fp: 67.0000 - tn: 377.0000 - fn: 61.0000 - accuracy: 0.8202 - precision: 0.7555 - recall: 0.7724 - auc: 0.8621 - val_loss: 0.5044 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8940\n",
      "Epoch 95/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.5227 - tp: 206.0000 - fp: 71.0000 - tn: 373.0000 - fn: 62.0000 - accuracy: 0.8132 - precision: 0.7437 - recall: 0.7687 - auc: 0.8588 - val_loss: 0.4967 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8941\n",
      "Epoch 96/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.5131 - tp: 201.0000 - fp: 51.0000 - tn: 393.0000 - fn: 67.0000 - accuracy: 0.8343 - precision: 0.7976 - recall: 0.7500 - auc: 0.8591 - val_loss: 0.4969 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8922\n",
      "Epoch 97/500\n",
      "712/712 [==============================] - 0s 156us/sample - loss: 0.5090 - tp: 205.0000 - fp: 58.0000 - tn: 386.0000 - fn: 63.0000 - accuracy: 0.8301 - precision: 0.7795 - recall: 0.7649 - auc: 0.8572 - val_loss: 0.4917 - val_tp: 62.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 12.0000 - val_accuracy: 0.8212 - val_precision: 0.7561 - val_recall: 0.8378 - val_auc: 0.8940\n",
      "Epoch 98/500\n",
      "712/712 [==============================] - 0s 155us/sample - loss: 0.5028 - tp: 215.0000 - fp: 68.0000 - tn: 376.0000 - fn: 53.0000 - accuracy: 0.8301 - precision: 0.7597 - recall: 0.8022 - auc: 0.8708 - val_loss: 0.4993 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8945\n",
      "Epoch 99/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.5146 - tp: 198.0000 - fp: 51.0000 - tn: 393.0000 - fn: 70.0000 - accuracy: 0.8301 - precision: 0.7952 - recall: 0.7388 - auc: 0.8581 - val_loss: 0.4938 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8933\n",
      "Epoch 100/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.5164 - tp: 206.0000 - fp: 70.0000 - tn: 374.0000 - fn: 62.0000 - accuracy: 0.8146 - precision: 0.7464 - recall: 0.7687 - auc: 0.8548 - val_loss: 0.4899 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8941\n",
      "Epoch 101/500\n",
      "712/712 [==============================] - 0s 155us/sample - loss: 0.5006 - tp: 200.0000 - fp: 44.0000 - tn: 400.0000 - fn: 68.0000 - accuracy: 0.8427 - precision: 0.8197 - recall: 0.7463 - auc: 0.8704 - val_loss: 0.4920 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8921\n",
      "Epoch 102/500\n",
      "712/712 [==============================] - 0s 152us/sample - loss: 0.5120 - tp: 199.0000 - fp: 51.0000 - tn: 393.0000 - fn: 69.0000 - accuracy: 0.8315 - precision: 0.7960 - recall: 0.7425 - auc: 0.8583 - val_loss: 0.4931 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8934\n",
      "Epoch 103/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4983 - tp: 205.0000 - fp: 62.0000 - tn: 382.0000 - fn: 63.0000 - accuracy: 0.8244 - precision: 0.7678 - recall: 0.7649 - auc: 0.8708 - val_loss: 0.4896 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8938\n",
      "Epoch 104/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.5190 - tp: 206.0000 - fp: 66.0000 - tn: 378.0000 - fn: 62.0000 - accuracy: 0.8202 - precision: 0.7574 - recall: 0.7687 - auc: 0.8530 - val_loss: 0.4878 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8951\n",
      "Epoch 105/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.5108 - tp: 206.0000 - fp: 60.0000 - tn: 384.0000 - fn: 62.0000 - accuracy: 0.8287 - precision: 0.7744 - recall: 0.7687 - auc: 0.8625 - val_loss: 0.4922 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8937\n",
      "Epoch 106/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.5123 - tp: 209.0000 - fp: 65.0000 - tn: 379.0000 - fn: 59.0000 - accuracy: 0.8258 - precision: 0.7628 - recall: 0.7799 - auc: 0.8619 - val_loss: 0.4962 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8925\n",
      "Epoch 107/500\n",
      "712/712 [==============================] - 0s 190us/sample - loss: 0.4985 - tp: 200.0000 - fp: 59.0000 - tn: 385.0000 - fn: 68.0000 - accuracy: 0.8216 - precision: 0.7722 - recall: 0.7463 - auc: 0.8690 - val_loss: 0.4919 - val_tp: 61.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 13.0000 - val_accuracy: 0.7877 - val_precision: 0.7093 - val_recall: 0.8243 - val_auc: 0.8915\n",
      "Epoch 108/500\n",
      "712/712 [==============================] - 0s 198us/sample - loss: 0.5025 - tp: 204.0000 - fp: 63.0000 - tn: 381.0000 - fn: 64.0000 - accuracy: 0.8216 - precision: 0.7640 - recall: 0.7612 - auc: 0.8674 - val_loss: 0.4887 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8916\n",
      "Epoch 109/500\n",
      "712/712 [==============================] - 0s 183us/sample - loss: 0.5055 - tp: 208.0000 - fp: 67.0000 - tn: 377.0000 - fn: 60.0000 - accuracy: 0.8216 - precision: 0.7564 - recall: 0.7761 - auc: 0.8633 - val_loss: 0.4967 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8947\n",
      "Epoch 110/500\n",
      "712/712 [==============================] - 0s 179us/sample - loss: 0.5148 - tp: 202.0000 - fp: 59.0000 - tn: 385.0000 - fn: 66.0000 - accuracy: 0.8244 - precision: 0.7739 - recall: 0.7537 - auc: 0.8506 - val_loss: 0.4928 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8950\n",
      "Epoch 111/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.5160 - tp: 206.0000 - fp: 71.0000 - tn: 373.0000 - fn: 62.0000 - accuracy: 0.8132 - precision: 0.7437 - recall: 0.7687 - auc: 0.8608 - val_loss: 0.4872 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8945\n",
      "Epoch 112/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4917 - tp: 198.0000 - fp: 50.0000 - tn: 394.0000 - fn: 70.0000 - accuracy: 0.8315 - precision: 0.7984 - recall: 0.7388 - auc: 0.8717 - val_loss: 0.4915 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8940\n",
      "Epoch 113/500\n",
      "712/712 [==============================] - 0s 153us/sample - loss: 0.5130 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8557 - val_loss: 0.4956 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8905\n",
      "Epoch 114/500\n",
      "712/712 [==============================] - 0s 154us/sample - loss: 0.5123 - tp: 199.0000 - fp: 58.0000 - tn: 386.0000 - fn: 69.0000 - accuracy: 0.8216 - precision: 0.7743 - recall: 0.7425 - auc: 0.8665 - val_loss: 0.4852 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8944\n",
      "Epoch 115/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.5130 - tp: 207.0000 - fp: 72.0000 - tn: 372.0000 - fn: 61.0000 - accuracy: 0.8132 - precision: 0.7419 - recall: 0.7724 - auc: 0.8612 - val_loss: 0.4923 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8957\n",
      "Epoch 116/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.5155 - tp: 202.0000 - fp: 62.0000 - tn: 382.0000 - fn: 66.0000 - accuracy: 0.8202 - precision: 0.7652 - recall: 0.7537 - auc: 0.8574 - val_loss: 0.4851 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8956\n",
      "Epoch 117/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.5085 - tp: 203.0000 - fp: 59.0000 - tn: 385.0000 - fn: 65.0000 - accuracy: 0.8258 - precision: 0.7748 - recall: 0.7575 - auc: 0.8619 - val_loss: 0.4891 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8940\n",
      "Epoch 118/500\n",
      "712/712 [==============================] - 0s 152us/sample - loss: 0.4953 - tp: 202.0000 - fp: 40.0000 - tn: 404.0000 - fn: 66.0000 - accuracy: 0.8511 - precision: 0.8347 - recall: 0.7537 - auc: 0.8690 - val_loss: 0.4988 - val_tp: 62.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 12.0000 - val_accuracy: 0.7877 - val_precision: 0.7045 - val_recall: 0.8378 - val_auc: 0.8887\n",
      "Epoch 119/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.5015 - tp: 211.0000 - fp: 77.0000 - tn: 367.0000 - fn: 57.0000 - accuracy: 0.8118 - precision: 0.7326 - recall: 0.7873 - auc: 0.8668 - val_loss: 0.4949 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8937\n",
      "Epoch 120/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.5076 - tp: 201.0000 - fp: 60.0000 - tn: 384.0000 - fn: 67.0000 - accuracy: 0.8216 - precision: 0.7701 - recall: 0.7500 - auc: 0.8631 - val_loss: 0.4885 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8945\n",
      "Epoch 121/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.5205 - tp: 206.0000 - fp: 66.0000 - tn: 378.0000 - fn: 62.0000 - accuracy: 0.8202 - precision: 0.7574 - recall: 0.7687 - auc: 0.8514 - val_loss: 0.4882 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.8934\n",
      "Epoch 122/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.5070 - tp: 194.0000 - fp: 53.0000 - tn: 391.0000 - fn: 74.0000 - accuracy: 0.8216 - precision: 0.7854 - recall: 0.7239 - auc: 0.8582 - val_loss: 0.4918 - val_tp: 64.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 10.0000 - val_accuracy: 0.7877 - val_precision: 0.6957 - val_recall: 0.8649 - val_auc: 0.8916\n",
      "Epoch 123/500\n",
      "712/712 [==============================] - 0s 153us/sample - loss: 0.5140 - tp: 213.0000 - fp: 75.0000 - tn: 369.0000 - fn: 55.0000 - accuracy: 0.8174 - precision: 0.7396 - recall: 0.7948 - auc: 0.8586 - val_loss: 0.4879 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8945\n",
      "Epoch 124/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.5039 - tp: 202.0000 - fp: 61.0000 - tn: 383.0000 - fn: 66.0000 - accuracy: 0.8216 - precision: 0.7681 - recall: 0.7537 - auc: 0.8643 - val_loss: 0.4915 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8951\n",
      "Epoch 125/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.5126 - tp: 204.0000 - fp: 67.0000 - tn: 377.0000 - fn: 64.0000 - accuracy: 0.8160 - precision: 0.7528 - recall: 0.7612 - auc: 0.8613 - val_loss: 0.4833 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8954\n",
      "Epoch 126/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.5187 - tp: 191.0000 - fp: 43.0000 - tn: 401.0000 - fn: 77.0000 - accuracy: 0.8315 - precision: 0.8162 - recall: 0.7127 - auc: 0.8560 - val_loss: 0.4903 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8908\n",
      "Epoch 127/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.5051 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8596 - val_loss: 0.4863 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8936\n",
      "Epoch 128/500\n",
      "712/712 [==============================] - 0s 152us/sample - loss: 0.4935 - tp: 208.0000 - fp: 67.0000 - tn: 377.0000 - fn: 60.0000 - accuracy: 0.8216 - precision: 0.7564 - recall: 0.7761 - auc: 0.8717 - val_loss: 0.4854 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8934\n",
      "Epoch 129/500\n",
      "712/712 [==============================] - 0s 192us/sample - loss: 0.4989 - tp: 205.0000 - fp: 71.0000 - tn: 373.0000 - fn: 63.0000 - accuracy: 0.8118 - precision: 0.7428 - recall: 0.7649 - auc: 0.8688 - val_loss: 0.4920 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8936\n",
      "Epoch 130/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.5005 - tp: 204.0000 - fp: 65.0000 - tn: 379.0000 - fn: 64.0000 - accuracy: 0.8188 - precision: 0.7584 - recall: 0.7612 - auc: 0.8669 - val_loss: 0.4843 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8946\n",
      "Epoch 131/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4958 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8700 - val_loss: 0.4869 - val_tp: 62.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 12.0000 - val_accuracy: 0.8324 - val_precision: 0.7750 - val_recall: 0.8378 - val_auc: 0.8949\n",
      "Epoch 132/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.4980 - tp: 204.0000 - fp: 64.0000 - tn: 380.0000 - fn: 64.0000 - accuracy: 0.8202 - precision: 0.7612 - recall: 0.7612 - auc: 0.8722 - val_loss: 0.4862 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8956\n",
      "Epoch 133/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4930 - tp: 210.0000 - fp: 59.0000 - tn: 385.0000 - fn: 58.0000 - accuracy: 0.8357 - precision: 0.7807 - recall: 0.7836 - auc: 0.8758 - val_loss: 0.4837 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8955\n",
      "Epoch 134/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.5015 - tp: 202.0000 - fp: 53.0000 - tn: 391.0000 - fn: 66.0000 - accuracy: 0.8329 - precision: 0.7922 - recall: 0.7537 - auc: 0.8693 - val_loss: 0.4876 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8942\n",
      "Epoch 135/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.5022 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8657 - val_loss: 0.4792 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.8969\n",
      "Epoch 136/500\n",
      "712/712 [==============================] - 0s 188us/sample - loss: 0.5093 - tp: 206.0000 - fp: 62.0000 - tn: 382.0000 - fn: 62.0000 - accuracy: 0.8258 - precision: 0.7687 - recall: 0.7687 - auc: 0.8607 - val_loss: 0.4884 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8939\n",
      "Epoch 137/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.4824 - tp: 212.0000 - fp: 65.0000 - tn: 379.0000 - fn: 56.0000 - accuracy: 0.8301 - precision: 0.7653 - recall: 0.7910 - auc: 0.8771 - val_loss: 0.4871 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8970\n",
      "Epoch 138/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.4984 - tp: 202.0000 - fp: 47.0000 - tn: 397.0000 - fn: 66.0000 - accuracy: 0.8413 - precision: 0.8112 - recall: 0.7537 - auc: 0.8674 - val_loss: 0.4870 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.8939\n",
      "Epoch 139/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.4952 - tp: 206.0000 - fp: 67.0000 - tn: 377.0000 - fn: 62.0000 - accuracy: 0.8188 - precision: 0.7546 - recall: 0.7687 - auc: 0.8718 - val_loss: 0.4853 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8956\n",
      "Epoch 140/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.5146 - tp: 196.0000 - fp: 54.0000 - tn: 390.0000 - fn: 72.0000 - accuracy: 0.8230 - precision: 0.7840 - recall: 0.7313 - auc: 0.8546 - val_loss: 0.4827 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8984\n",
      "Epoch 141/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.5004 - tp: 199.0000 - fp: 55.0000 - tn: 389.0000 - fn: 69.0000 - accuracy: 0.8258 - precision: 0.7835 - recall: 0.7425 - auc: 0.8652 - val_loss: 0.4793 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.8964\n",
      "Epoch 142/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.5159 - tp: 204.0000 - fp: 63.0000 - tn: 381.0000 - fn: 64.0000 - accuracy: 0.8216 - precision: 0.7640 - recall: 0.7612 - auc: 0.8596 - val_loss: 0.4815 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8950\n",
      "Epoch 143/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.5012 - tp: 191.0000 - fp: 47.0000 - tn: 397.0000 - fn: 77.0000 - accuracy: 0.8258 - precision: 0.8025 - recall: 0.7127 - auc: 0.8661 - val_loss: 0.4847 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8947\n",
      "Epoch 144/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.5050 - tp: 204.0000 - fp: 62.0000 - tn: 382.0000 - fn: 64.0000 - accuracy: 0.8230 - precision: 0.7669 - recall: 0.7612 - auc: 0.8657 - val_loss: 0.5107 - val_tp: 69.0000 - val_fp: 31.0000 - val_tn: 74.0000 - val_fn: 5.0000 - val_accuracy: 0.7989 - val_precision: 0.6900 - val_recall: 0.9324 - val_auc: 0.8894\n",
      "Epoch 145/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.5001 - tp: 210.0000 - fp: 71.0000 - tn: 373.0000 - fn: 58.0000 - accuracy: 0.8188 - precision: 0.7473 - recall: 0.7836 - auc: 0.8704 - val_loss: 0.4842 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8952\n",
      "Epoch 146/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.5062 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8638 - val_loss: 0.4811 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8936\n",
      "Epoch 147/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.5078 - tp: 195.0000 - fp: 46.0000 - tn: 398.0000 - fn: 73.0000 - accuracy: 0.8329 - precision: 0.8091 - recall: 0.7276 - auc: 0.8575 - val_loss: 0.4851 - val_tp: 61.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 13.0000 - val_accuracy: 0.8156 - val_precision: 0.7531 - val_recall: 0.8243 - val_auc: 0.8916\n",
      "Epoch 148/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.4924 - tp: 208.0000 - fp: 68.0000 - tn: 376.0000 - fn: 60.0000 - accuracy: 0.8202 - precision: 0.7536 - recall: 0.7761 - auc: 0.8711 - val_loss: 0.4821 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8943\n",
      "Epoch 149/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4997 - tp: 203.0000 - fp: 58.0000 - tn: 386.0000 - fn: 65.0000 - accuracy: 0.8272 - precision: 0.7778 - recall: 0.7575 - auc: 0.8686 - val_loss: 0.5009 - val_tp: 64.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 10.0000 - val_accuracy: 0.7821 - val_precision: 0.6882 - val_recall: 0.8649 - val_auc: 0.8893\n",
      "Epoch 150/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.5152 - tp: 201.0000 - fp: 73.0000 - tn: 371.0000 - fn: 67.0000 - accuracy: 0.8034 - precision: 0.7336 - recall: 0.7500 - auc: 0.8546 - val_loss: 0.4902 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8940\n",
      "Epoch 151/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.5015 - tp: 205.0000 - fp: 56.0000 - tn: 388.0000 - fn: 63.0000 - accuracy: 0.8329 - precision: 0.7854 - recall: 0.7649 - auc: 0.8669 - val_loss: 0.4868 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8923\n",
      "Epoch 152/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.4903 - tp: 203.0000 - fp: 59.0000 - tn: 385.0000 - fn: 65.0000 - accuracy: 0.8258 - precision: 0.7748 - recall: 0.7575 - auc: 0.8775 - val_loss: 0.4836 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8929\n",
      "Epoch 153/500\n",
      "712/712 [==============================] - 0s 157us/sample - loss: 0.4941 - tp: 201.0000 - fp: 61.0000 - tn: 383.0000 - fn: 67.0000 - accuracy: 0.8202 - precision: 0.7672 - recall: 0.7500 - auc: 0.8697 - val_loss: 0.4857 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8939\n",
      "Epoch 154/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.4862 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8747 - val_loss: 0.4896 - val_tp: 62.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 12.0000 - val_accuracy: 0.8156 - val_precision: 0.7470 - val_recall: 0.8378 - val_auc: 0.8897\n",
      "Epoch 155/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4989 - tp: 201.0000 - fp: 55.0000 - tn: 389.0000 - fn: 67.0000 - accuracy: 0.8287 - precision: 0.7852 - recall: 0.7500 - auc: 0.8646 - val_loss: 0.4949 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8952\n",
      "Epoch 156/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4861 - tp: 198.0000 - fp: 38.0000 - tn: 406.0000 - fn: 70.0000 - accuracy: 0.8483 - precision: 0.8390 - recall: 0.7388 - auc: 0.8741 - val_loss: 0.4889 - val_tp: 62.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 12.0000 - val_accuracy: 0.8045 - val_precision: 0.7294 - val_recall: 0.8378 - val_auc: 0.8925\n",
      "Epoch 157/500\n",
      "712/712 [==============================] - 0s 179us/sample - loss: 0.5024 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8657 - val_loss: 0.4874 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8936\n",
      "Epoch 158/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.5034 - tp: 200.0000 - fp: 58.0000 - tn: 386.0000 - fn: 68.0000 - accuracy: 0.8230 - precision: 0.7752 - recall: 0.7463 - auc: 0.8606 - val_loss: 0.4792 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8947\n",
      "Epoch 159/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.5146 - tp: 202.0000 - fp: 73.0000 - tn: 371.0000 - fn: 66.0000 - accuracy: 0.8048 - precision: 0.7345 - recall: 0.7537 - auc: 0.8556 - val_loss: 0.4815 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8928\n",
      "Epoch 160/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4901 - tp: 204.0000 - fp: 66.0000 - tn: 378.0000 - fn: 64.0000 - accuracy: 0.8174 - precision: 0.7556 - recall: 0.7612 - auc: 0.8721 - val_loss: 0.4827 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8970\n",
      "Epoch 161/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4944 - tp: 203.0000 - fp: 60.0000 - tn: 384.0000 - fn: 65.0000 - accuracy: 0.8244 - precision: 0.7719 - recall: 0.7575 - auc: 0.8627 - val_loss: 0.4772 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8941\n",
      "Epoch 162/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.4993 - tp: 199.0000 - fp: 57.0000 - tn: 387.0000 - fn: 69.0000 - accuracy: 0.8230 - precision: 0.7773 - recall: 0.7425 - auc: 0.8666 - val_loss: 0.4798 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8954\n",
      "Epoch 163/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4970 - tp: 200.0000 - fp: 61.0000 - tn: 383.0000 - fn: 68.0000 - accuracy: 0.8188 - precision: 0.7663 - recall: 0.7463 - auc: 0.8644 - val_loss: 0.4780 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8932\n",
      "Epoch 164/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.5023 - tp: 206.0000 - fp: 73.0000 - tn: 371.0000 - fn: 62.0000 - accuracy: 0.8104 - precision: 0.7384 - recall: 0.7687 - auc: 0.8626 - val_loss: 0.4813 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8948\n",
      "Epoch 165/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.4890 - tp: 205.0000 - fp: 60.0000 - tn: 384.0000 - fn: 63.0000 - accuracy: 0.8272 - precision: 0.7736 - recall: 0.7649 - auc: 0.8730 - val_loss: 0.4839 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8949\n",
      "Epoch 166/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.5005 - tp: 203.0000 - fp: 52.0000 - tn: 392.0000 - fn: 65.0000 - accuracy: 0.8357 - precision: 0.7961 - recall: 0.7575 - auc: 0.8672 - val_loss: 0.4843 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8938\n",
      "Epoch 167/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.4845 - tp: 206.0000 - fp: 63.0000 - tn: 381.0000 - fn: 62.0000 - accuracy: 0.8244 - precision: 0.7658 - recall: 0.7687 - auc: 0.8749 - val_loss: 0.4855 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8943\n",
      "Epoch 168/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.5060 - tp: 201.0000 - fp: 59.0000 - tn: 385.0000 - fn: 67.0000 - accuracy: 0.8230 - precision: 0.7731 - recall: 0.7500 - auc: 0.8561 - val_loss: 0.4761 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8983\n",
      "Epoch 169/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.5037 - tp: 202.0000 - fp: 66.0000 - tn: 378.0000 - fn: 66.0000 - accuracy: 0.8146 - precision: 0.7537 - recall: 0.7537 - auc: 0.8646 - val_loss: 0.4845 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8969\n",
      "Epoch 170/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.4922 - tp: 194.0000 - fp: 53.0000 - tn: 391.0000 - fn: 74.0000 - accuracy: 0.8216 - precision: 0.7854 - recall: 0.7239 - auc: 0.8717 - val_loss: 0.4826 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8924\n",
      "Epoch 171/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4864 - tp: 199.0000 - fp: 53.0000 - tn: 391.0000 - fn: 69.0000 - accuracy: 0.8287 - precision: 0.7897 - recall: 0.7425 - auc: 0.8729 - val_loss: 0.4855 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8955\n",
      "Epoch 172/500\n",
      "712/712 [==============================] - 0s 186us/sample - loss: 0.4827 - tp: 205.0000 - fp: 55.0000 - tn: 389.0000 - fn: 63.0000 - accuracy: 0.8343 - precision: 0.7885 - recall: 0.7649 - auc: 0.8787 - val_loss: 0.4806 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8940\n",
      "Epoch 173/500\n",
      "712/712 [==============================] - 0s 220us/sample - loss: 0.5052 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8639 - val_loss: 0.4766 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8963\n",
      "Epoch 174/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.5048 - tp: 211.0000 - fp: 66.0000 - tn: 378.0000 - fn: 57.0000 - accuracy: 0.8272 - precision: 0.7617 - recall: 0.7873 - auc: 0.8655 - val_loss: 0.4794 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8972\n",
      "Epoch 175/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4983 - tp: 197.0000 - fp: 47.0000 - tn: 397.0000 - fn: 71.0000 - accuracy: 0.8343 - precision: 0.8074 - recall: 0.7351 - auc: 0.8688 - val_loss: 0.4819 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8940\n",
      "Epoch 176/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4953 - tp: 205.0000 - fp: 58.0000 - tn: 386.0000 - fn: 63.0000 - accuracy: 0.8301 - precision: 0.7795 - recall: 0.7649 - auc: 0.8663 - val_loss: 0.4797 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8921\n",
      "Epoch 177/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.4954 - tp: 207.0000 - fp: 55.0000 - tn: 389.0000 - fn: 61.0000 - accuracy: 0.8371 - precision: 0.7901 - recall: 0.7724 - auc: 0.8688 - val_loss: 0.4758 - val_tp: 61.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 13.0000 - val_accuracy: 0.8380 - val_precision: 0.7922 - val_recall: 0.8243 - val_auc: 0.8955\n",
      "Epoch 178/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.5008 - tp: 196.0000 - fp: 58.0000 - tn: 386.0000 - fn: 72.0000 - accuracy: 0.8174 - precision: 0.7717 - recall: 0.7313 - auc: 0.8577 - val_loss: 0.4805 - val_tp: 64.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 10.0000 - val_accuracy: 0.7989 - val_precision: 0.7111 - val_recall: 0.8649 - val_auc: 0.8950\n",
      "Epoch 179/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.5020 - tp: 198.0000 - fp: 56.0000 - tn: 388.0000 - fn: 70.0000 - accuracy: 0.8230 - precision: 0.7795 - recall: 0.7388 - auc: 0.8626 - val_loss: 0.4812 - val_tp: 62.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 12.0000 - val_accuracy: 0.8268 - val_precision: 0.7654 - val_recall: 0.8378 - val_auc: 0.8939\n",
      "Epoch 180/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.5117 - tp: 205.0000 - fp: 76.0000 - tn: 368.0000 - fn: 63.0000 - accuracy: 0.8048 - precision: 0.7295 - recall: 0.7649 - auc: 0.8534 - val_loss: 0.4772 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8949\n",
      "Epoch 181/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.5044 - tp: 205.0000 - fp: 65.0000 - tn: 379.0000 - fn: 63.0000 - accuracy: 0.8202 - precision: 0.7593 - recall: 0.7649 - auc: 0.8607 - val_loss: 0.4773 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8967\n",
      "Epoch 182/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4866 - tp: 209.0000 - fp: 64.0000 - tn: 380.0000 - fn: 59.0000 - accuracy: 0.8272 - precision: 0.7656 - recall: 0.7799 - auc: 0.8733 - val_loss: 0.4783 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8949\n",
      "Epoch 183/500\n",
      "712/712 [==============================] - 0s 148us/sample - loss: 0.4959 - tp: 199.0000 - fp: 48.0000 - tn: 396.0000 - fn: 69.0000 - accuracy: 0.8357 - precision: 0.8057 - recall: 0.7425 - auc: 0.8648 - val_loss: 0.4792 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8948\n",
      "Epoch 184/500\n",
      "712/712 [==============================] - 0s 151us/sample - loss: 0.4922 - tp: 205.0000 - fp: 55.0000 - tn: 389.0000 - fn: 63.0000 - accuracy: 0.8343 - precision: 0.7885 - recall: 0.7649 - auc: 0.8685 - val_loss: 0.4734 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8964\n",
      "Epoch 185/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.4907 - tp: 207.0000 - fp: 59.0000 - tn: 385.0000 - fn: 61.0000 - accuracy: 0.8315 - precision: 0.7782 - recall: 0.7724 - auc: 0.8692 - val_loss: 0.4771 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8943\n",
      "Epoch 186/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.4872 - tp: 198.0000 - fp: 54.0000 - tn: 390.0000 - fn: 70.0000 - accuracy: 0.8258 - precision: 0.7857 - recall: 0.7388 - auc: 0.8735 - val_loss: 0.4767 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8959\n",
      "Epoch 187/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4870 - tp: 207.0000 - fp: 60.0000 - tn: 384.0000 - fn: 61.0000 - accuracy: 0.8301 - precision: 0.7753 - recall: 0.7724 - auc: 0.8712 - val_loss: 0.4773 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8950\n",
      "Epoch 188/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4918 - tp: 199.0000 - fp: 51.0000 - tn: 393.0000 - fn: 69.0000 - accuracy: 0.8315 - precision: 0.7960 - recall: 0.7425 - auc: 0.8684 - val_loss: 0.4797 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8954\n",
      "Epoch 189/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4725 - tp: 208.0000 - fp: 51.0000 - tn: 393.0000 - fn: 60.0000 - accuracy: 0.8441 - precision: 0.8031 - recall: 0.7761 - auc: 0.8835 - val_loss: 0.4720 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8952\n",
      "Epoch 190/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.5041 - tp: 203.0000 - fp: 73.0000 - tn: 371.0000 - fn: 65.0000 - accuracy: 0.8062 - precision: 0.7355 - recall: 0.7575 - auc: 0.8602 - val_loss: 0.4755 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8946\n",
      "Epoch 191/500\n",
      "712/712 [==============================] - 0s 245us/sample - loss: 0.4900 - tp: 202.0000 - fp: 52.0000 - tn: 392.0000 - fn: 66.0000 - accuracy: 0.8343 - precision: 0.7953 - recall: 0.7537 - auc: 0.8706 - val_loss: 0.4839 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8968\n",
      "Epoch 192/500\n",
      "712/712 [==============================] - 0s 228us/sample - loss: 0.4873 - tp: 207.0000 - fp: 64.0000 - tn: 380.0000 - fn: 61.0000 - accuracy: 0.8244 - precision: 0.7638 - recall: 0.7724 - auc: 0.8761 - val_loss: 0.4782 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8940\n",
      "Epoch 193/500\n",
      "712/712 [==============================] - 0s 201us/sample - loss: 0.4933 - tp: 202.0000 - fp: 58.0000 - tn: 386.0000 - fn: 66.0000 - accuracy: 0.8258 - precision: 0.7769 - recall: 0.7537 - auc: 0.8636 - val_loss: 0.4839 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8946\n",
      "Epoch 194/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.5173 - tp: 197.0000 - fp: 64.0000 - tn: 380.0000 - fn: 71.0000 - accuracy: 0.8104 - precision: 0.7548 - recall: 0.7351 - auc: 0.8539 - val_loss: 0.4807 - val_tp: 62.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 12.0000 - val_accuracy: 0.7821 - val_precision: 0.6966 - val_recall: 0.8378 - val_auc: 0.8934\n",
      "Epoch 195/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.5065 - tp: 197.0000 - fp: 46.0000 - tn: 398.0000 - fn: 71.0000 - accuracy: 0.8357 - precision: 0.8107 - recall: 0.7351 - auc: 0.8584 - val_loss: 0.4770 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8945\n",
      "Epoch 196/500\n",
      "712/712 [==============================] - 0s 199us/sample - loss: 0.4948 - tp: 208.0000 - fp: 58.0000 - tn: 386.0000 - fn: 60.0000 - accuracy: 0.8343 - precision: 0.7820 - recall: 0.7761 - auc: 0.8728 - val_loss: 0.4791 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8994\n",
      "Epoch 197/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4882 - tp: 204.0000 - fp: 56.0000 - tn: 388.0000 - fn: 64.0000 - accuracy: 0.8315 - precision: 0.7846 - recall: 0.7612 - auc: 0.8677 - val_loss: 0.4805 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8979\n",
      "Epoch 198/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.5019 - tp: 196.0000 - fp: 48.0000 - tn: 396.0000 - fn: 72.0000 - accuracy: 0.8315 - precision: 0.8033 - recall: 0.7313 - auc: 0.8621 - val_loss: 0.4769 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8956\n",
      "Epoch 199/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.5068 - tp: 197.0000 - fp: 61.0000 - tn: 383.0000 - fn: 71.0000 - accuracy: 0.8146 - precision: 0.7636 - recall: 0.7351 - auc: 0.8549 - val_loss: 0.4750 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8965\n",
      "Epoch 200/500\n",
      "712/712 [==============================] - 0s 156us/sample - loss: 0.4952 - tp: 203.0000 - fp: 60.0000 - tn: 384.0000 - fn: 65.0000 - accuracy: 0.8244 - precision: 0.7719 - recall: 0.7575 - auc: 0.8671 - val_loss: 0.4814 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8970\n",
      "Epoch 201/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4955 - tp: 192.0000 - fp: 40.0000 - tn: 404.0000 - fn: 76.0000 - accuracy: 0.8371 - precision: 0.8276 - recall: 0.7164 - auc: 0.8626 - val_loss: 0.5008 - val_tp: 69.0000 - val_fp: 33.0000 - val_tn: 72.0000 - val_fn: 5.0000 - val_accuracy: 0.7877 - val_precision: 0.6765 - val_recall: 0.9324 - val_auc: 0.8922\n",
      "Epoch 202/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.5046 - tp: 199.0000 - fp: 72.0000 - tn: 372.0000 - fn: 69.0000 - accuracy: 0.8020 - precision: 0.7343 - recall: 0.7425 - auc: 0.8646 - val_loss: 0.4768 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8935\n",
      "Epoch 203/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.5059 - tp: 206.0000 - fp: 61.0000 - tn: 383.0000 - fn: 62.0000 - accuracy: 0.8272 - precision: 0.7715 - recall: 0.7687 - auc: 0.8574 - val_loss: 0.4907 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8945\n",
      "Epoch 204/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4982 - tp: 197.0000 - fp: 48.0000 - tn: 396.0000 - fn: 71.0000 - accuracy: 0.8329 - precision: 0.8041 - recall: 0.7351 - auc: 0.8634 - val_loss: 0.4793 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8938\n",
      "Epoch 205/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.4945 - tp: 213.0000 - fp: 64.0000 - tn: 380.0000 - fn: 55.0000 - accuracy: 0.8329 - precision: 0.7690 - recall: 0.7948 - auc: 0.8632 - val_loss: 0.4798 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8958\n",
      "Epoch 206/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4832 - tp: 203.0000 - fp: 48.0000 - tn: 396.0000 - fn: 65.0000 - accuracy: 0.8413 - precision: 0.8088 - recall: 0.7575 - auc: 0.8717 - val_loss: 0.4830 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8940\n",
      "Epoch 207/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.4763 - tp: 206.0000 - fp: 60.0000 - tn: 384.0000 - fn: 62.0000 - accuracy: 0.8287 - precision: 0.7744 - recall: 0.7687 - auc: 0.8847 - val_loss: 0.4815 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8954\n",
      "Epoch 208/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.4973 - tp: 203.0000 - fp: 60.0000 - tn: 384.0000 - fn: 65.0000 - accuracy: 0.8244 - precision: 0.7719 - recall: 0.7575 - auc: 0.8671 - val_loss: 0.4760 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8965\n",
      "Epoch 209/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4948 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8629 - val_loss: 0.4754 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8964\n",
      "Epoch 210/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.4809 - tp: 206.0000 - fp: 50.0000 - tn: 394.0000 - fn: 62.0000 - accuracy: 0.8427 - precision: 0.8047 - recall: 0.7687 - auc: 0.8761 - val_loss: 0.4698 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8968\n",
      "Epoch 211/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.5008 - tp: 206.0000 - fp: 61.0000 - tn: 383.0000 - fn: 62.0000 - accuracy: 0.8272 - precision: 0.7715 - recall: 0.7687 - auc: 0.8576 - val_loss: 0.4715 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8946\n",
      "Epoch 212/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4920 - tp: 205.0000 - fp: 65.0000 - tn: 379.0000 - fn: 63.0000 - accuracy: 0.8202 - precision: 0.7593 - recall: 0.7649 - auc: 0.8663 - val_loss: 0.4751 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8988\n",
      "Epoch 213/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.4979 - tp: 198.0000 - fp: 50.0000 - tn: 394.0000 - fn: 70.0000 - accuracy: 0.8315 - precision: 0.7984 - recall: 0.7388 - auc: 0.8635 - val_loss: 0.4770 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8974\n",
      "Epoch 214/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.4919 - tp: 203.0000 - fp: 73.0000 - tn: 371.0000 - fn: 65.0000 - accuracy: 0.8062 - precision: 0.7355 - recall: 0.7575 - auc: 0.8672 - val_loss: 0.4752 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8979\n",
      "Epoch 215/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.5033 - tp: 201.0000 - fp: 62.0000 - tn: 382.0000 - fn: 67.0000 - accuracy: 0.8188 - precision: 0.7643 - recall: 0.7500 - auc: 0.8554 - val_loss: 0.4753 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8958\n",
      "Epoch 216/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.4951 - tp: 197.0000 - fp: 48.0000 - tn: 396.0000 - fn: 71.0000 - accuracy: 0.8329 - precision: 0.8041 - recall: 0.7351 - auc: 0.8728 - val_loss: 0.4770 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.9018\n",
      "Epoch 217/500\n",
      "712/712 [==============================] - 0s 193us/sample - loss: 0.4904 - tp: 208.0000 - fp: 60.0000 - tn: 384.0000 - fn: 60.0000 - accuracy: 0.8315 - precision: 0.7761 - recall: 0.7761 - auc: 0.8684 - val_loss: 0.4771 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8960\n",
      "Epoch 218/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.4845 - tp: 204.0000 - fp: 55.0000 - tn: 389.0000 - fn: 64.0000 - accuracy: 0.8329 - precision: 0.7876 - recall: 0.7612 - auc: 0.8727 - val_loss: 0.4811 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9001\n",
      "Epoch 219/500\n",
      "712/712 [==============================] - 0s 194us/sample - loss: 0.5030 - tp: 201.0000 - fp: 58.0000 - tn: 386.0000 - fn: 67.0000 - accuracy: 0.8244 - precision: 0.7761 - recall: 0.7500 - auc: 0.8524 - val_loss: 0.4726 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8975\n",
      "Epoch 220/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.4975 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8608 - val_loss: 0.4748 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.9005\n",
      "Epoch 221/500\n",
      "712/712 [==============================] - 0s 221us/sample - loss: 0.5019 - tp: 197.0000 - fp: 61.0000 - tn: 383.0000 - fn: 71.0000 - accuracy: 0.8146 - precision: 0.7636 - recall: 0.7351 - auc: 0.8595 - val_loss: 0.4769 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8925\n",
      "Epoch 222/500\n",
      "712/712 [==============================] - 0s 196us/sample - loss: 0.4928 - tp: 203.0000 - fp: 60.0000 - tn: 384.0000 - fn: 65.0000 - accuracy: 0.8244 - precision: 0.7719 - recall: 0.7575 - auc: 0.8651 - val_loss: 0.4808 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8948\n",
      "Epoch 223/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.4958 - tp: 198.0000 - fp: 49.0000 - tn: 395.0000 - fn: 70.0000 - accuracy: 0.8329 - precision: 0.8016 - recall: 0.7388 - auc: 0.8712 - val_loss: 0.4760 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8950\n",
      "Epoch 224/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.4994 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8562 - val_loss: 0.4790 - val_tp: 62.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 12.0000 - val_accuracy: 0.8380 - val_precision: 0.7848 - val_recall: 0.8378 - val_auc: 0.8985\n",
      "Epoch 225/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.4970 - tp: 202.0000 - fp: 46.0000 - tn: 398.0000 - fn: 66.0000 - accuracy: 0.8427 - precision: 0.8145 - recall: 0.7537 - auc: 0.8631 - val_loss: 0.4773 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8956\n",
      "Epoch 226/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.4971 - tp: 197.0000 - fp: 50.0000 - tn: 394.0000 - fn: 71.0000 - accuracy: 0.8301 - precision: 0.7976 - recall: 0.7351 - auc: 0.8633 - val_loss: 0.4779 - val_tp: 62.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 12.0000 - val_accuracy: 0.8268 - val_precision: 0.7654 - val_recall: 0.8378 - val_auc: 0.8966\n",
      "Epoch 227/500\n",
      "712/712 [==============================] - 0s 152us/sample - loss: 0.4827 - tp: 205.0000 - fp: 55.0000 - tn: 389.0000 - fn: 63.0000 - accuracy: 0.8343 - precision: 0.7885 - recall: 0.7649 - auc: 0.8704 - val_loss: 0.4768 - val_tp: 61.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 13.0000 - val_accuracy: 0.7933 - val_precision: 0.7176 - val_recall: 0.8243 - val_auc: 0.8945\n",
      "Epoch 228/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.5066 - tp: 201.0000 - fp: 58.0000 - tn: 386.0000 - fn: 67.0000 - accuracy: 0.8244 - precision: 0.7761 - recall: 0.7500 - auc: 0.8511 - val_loss: 0.4777 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8964\n",
      "Epoch 229/500\n",
      "712/712 [==============================] - 0s 186us/sample - loss: 0.4967 - tp: 199.0000 - fp: 55.0000 - tn: 389.0000 - fn: 69.0000 - accuracy: 0.8258 - precision: 0.7835 - recall: 0.7425 - auc: 0.8691 - val_loss: 0.4814 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8956\n",
      "Epoch 230/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.4952 - tp: 197.0000 - fp: 60.0000 - tn: 384.0000 - fn: 71.0000 - accuracy: 0.8160 - precision: 0.7665 - recall: 0.7351 - auc: 0.8679 - val_loss: 0.4754 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8975\n",
      "Epoch 231/500\n",
      "712/712 [==============================] - 0s 237us/sample - loss: 0.4970 - tp: 205.0000 - fp: 59.0000 - tn: 385.0000 - fn: 63.0000 - accuracy: 0.8287 - precision: 0.7765 - recall: 0.7649 - auc: 0.8641 - val_loss: 0.4787 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8960\n",
      "Epoch 232/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 0.4896 - tp: 200.0000 - fp: 56.0000 - tn: 388.0000 - fn: 68.0000 - accuracy: 0.8258 - precision: 0.7812 - recall: 0.7463 - auc: 0.8694 - val_loss: 0.4812 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.9016\n",
      "Epoch 233/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.4977 - tp: 198.0000 - fp: 46.0000 - tn: 398.0000 - fn: 70.0000 - accuracy: 0.8371 - precision: 0.8115 - recall: 0.7388 - auc: 0.8603 - val_loss: 0.4749 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8955\n",
      "Epoch 234/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4893 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8683 - val_loss: 0.4768 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8986\n",
      "Epoch 235/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.4930 - tp: 204.0000 - fp: 58.0000 - tn: 386.0000 - fn: 64.0000 - accuracy: 0.8287 - precision: 0.7786 - recall: 0.7612 - auc: 0.8708 - val_loss: 0.4747 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8975\n",
      "Epoch 236/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4976 - tp: 206.0000 - fp: 57.0000 - tn: 387.0000 - fn: 62.0000 - accuracy: 0.8329 - precision: 0.7833 - recall: 0.7687 - auc: 0.8601 - val_loss: 0.4771 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8972\n",
      "Epoch 237/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4930 - tp: 200.0000 - fp: 56.0000 - tn: 388.0000 - fn: 68.0000 - accuracy: 0.8258 - precision: 0.7812 - recall: 0.7463 - auc: 0.8730 - val_loss: 0.4725 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8975\n",
      "Epoch 238/500\n",
      "712/712 [==============================] - 0s 157us/sample - loss: 0.4913 - tp: 195.0000 - fp: 47.0000 - tn: 397.0000 - fn: 73.0000 - accuracy: 0.8315 - precision: 0.8058 - recall: 0.7276 - auc: 0.8654 - val_loss: 0.4753 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8968\n",
      "Epoch 239/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4892 - tp: 206.0000 - fp: 58.0000 - tn: 386.0000 - fn: 62.0000 - accuracy: 0.8315 - precision: 0.7803 - recall: 0.7687 - auc: 0.8676 - val_loss: 0.4729 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8963\n",
      "Epoch 240/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.5046 - tp: 193.0000 - fp: 62.0000 - tn: 382.0000 - fn: 75.0000 - accuracy: 0.8076 - precision: 0.7569 - recall: 0.7201 - auc: 0.8593 - val_loss: 0.4818 - val_tp: 62.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 12.0000 - val_accuracy: 0.7989 - val_precision: 0.7209 - val_recall: 0.8378 - val_auc: 0.8944\n",
      "Epoch 241/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 0.4898 - tp: 197.0000 - fp: 45.0000 - tn: 399.0000 - fn: 71.0000 - accuracy: 0.8371 - precision: 0.8140 - recall: 0.7351 - auc: 0.8678 - val_loss: 0.4794 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8998\n",
      "Epoch 242/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4905 - tp: 200.0000 - fp: 52.0000 - tn: 392.0000 - fn: 68.0000 - accuracy: 0.8315 - precision: 0.7937 - recall: 0.7463 - auc: 0.8706 - val_loss: 0.4751 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8955\n",
      "Epoch 243/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.5075 - tp: 210.0000 - fp: 68.0000 - tn: 376.0000 - fn: 58.0000 - accuracy: 0.8230 - precision: 0.7554 - recall: 0.7836 - auc: 0.8560 - val_loss: 0.4756 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8976\n",
      "Epoch 244/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.4955 - tp: 196.0000 - fp: 49.0000 - tn: 395.0000 - fn: 72.0000 - accuracy: 0.8301 - precision: 0.8000 - recall: 0.7313 - auc: 0.8659 - val_loss: 0.4834 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8952\n",
      "Epoch 245/500\n",
      "712/712 [==============================] - 0s 207us/sample - loss: 0.4739 - tp: 206.0000 - fp: 58.0000 - tn: 386.0000 - fn: 62.0000 - accuracy: 0.8315 - precision: 0.7803 - recall: 0.7687 - auc: 0.8757 - val_loss: 0.4762 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8986\n",
      "Epoch 246/500\n",
      "712/712 [==============================] - 0s 194us/sample - loss: 0.4771 - tp: 207.0000 - fp: 59.0000 - tn: 385.0000 - fn: 61.0000 - accuracy: 0.8315 - precision: 0.7782 - recall: 0.7724 - auc: 0.8783 - val_loss: 0.4737 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8977\n",
      "Epoch 247/500\n",
      "712/712 [==============================] - 0s 196us/sample - loss: 0.4995 - tp: 202.0000 - fp: 57.0000 - tn: 387.0000 - fn: 66.0000 - accuracy: 0.8272 - precision: 0.7799 - recall: 0.7537 - auc: 0.8620 - val_loss: 0.4819 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8934\n",
      "Epoch 248/500\n",
      "712/712 [==============================] - 0s 194us/sample - loss: 0.4965 - tp: 197.0000 - fp: 51.0000 - tn: 393.0000 - fn: 71.0000 - accuracy: 0.8287 - precision: 0.7944 - recall: 0.7351 - auc: 0.8631 - val_loss: 0.4774 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8963\n",
      "Epoch 249/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.4875 - tp: 201.0000 - fp: 56.0000 - tn: 388.0000 - fn: 67.0000 - accuracy: 0.8272 - precision: 0.7821 - recall: 0.7500 - auc: 0.8709 - val_loss: 0.4714 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8948\n",
      "Epoch 250/500\n",
      "712/712 [==============================] - 0s 150us/sample - loss: 0.4867 - tp: 200.0000 - fp: 48.0000 - tn: 396.0000 - fn: 68.0000 - accuracy: 0.8371 - precision: 0.8065 - recall: 0.7463 - auc: 0.8737 - val_loss: 0.4766 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8979\n",
      "Epoch 251/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4894 - tp: 202.0000 - fp: 56.0000 - tn: 388.0000 - fn: 66.0000 - accuracy: 0.8287 - precision: 0.7829 - recall: 0.7537 - auc: 0.8692 - val_loss: 0.4803 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8934\n",
      "Epoch 252/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.5079 - tp: 202.0000 - fp: 59.0000 - tn: 385.0000 - fn: 66.0000 - accuracy: 0.8244 - precision: 0.7739 - recall: 0.7537 - auc: 0.8588 - val_loss: 0.4808 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8951\n",
      "Epoch 253/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.5022 - tp: 195.0000 - fp: 47.0000 - tn: 397.0000 - fn: 73.0000 - accuracy: 0.8315 - precision: 0.8058 - recall: 0.7276 - auc: 0.8607 - val_loss: 0.4746 - val_tp: 61.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 13.0000 - val_accuracy: 0.8436 - val_precision: 0.8026 - val_recall: 0.8243 - val_auc: 0.8961\n",
      "Epoch 254/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4901 - tp: 204.0000 - fp: 67.0000 - tn: 377.0000 - fn: 64.0000 - accuracy: 0.8160 - precision: 0.7528 - recall: 0.7612 - auc: 0.8695 - val_loss: 0.4741 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9001\n",
      "Epoch 255/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4796 - tp: 207.0000 - fp: 53.0000 - tn: 391.0000 - fn: 61.0000 - accuracy: 0.8399 - precision: 0.7962 - recall: 0.7724 - auc: 0.8727 - val_loss: 0.4765 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8968\n",
      "Epoch 256/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.4914 - tp: 197.0000 - fp: 51.0000 - tn: 393.0000 - fn: 71.0000 - accuracy: 0.8287 - precision: 0.7944 - recall: 0.7351 - auc: 0.8664 - val_loss: 0.4782 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8993\n",
      "Epoch 257/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.4764 - tp: 196.0000 - fp: 44.0000 - tn: 400.0000 - fn: 72.0000 - accuracy: 0.8371 - precision: 0.8167 - recall: 0.7313 - auc: 0.8805 - val_loss: 0.4874 - val_tp: 62.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 12.0000 - val_accuracy: 0.7933 - val_precision: 0.7126 - val_recall: 0.8378 - val_auc: 0.8947\n",
      "Epoch 258/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 0.4898 - tp: 208.0000 - fp: 60.0000 - tn: 384.0000 - fn: 60.0000 - accuracy: 0.8315 - precision: 0.7761 - recall: 0.7761 - auc: 0.8703 - val_loss: 0.4763 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8999\n",
      "Epoch 259/500\n",
      "712/712 [==============================] - 0s 250us/sample - loss: 0.4982 - tp: 203.0000 - fp: 55.0000 - tn: 389.0000 - fn: 65.0000 - accuracy: 0.8315 - precision: 0.7868 - recall: 0.7575 - auc: 0.8619 - val_loss: 0.4762 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8969\n",
      "Epoch 260/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.4936 - tp: 197.0000 - fp: 51.0000 - tn: 393.0000 - fn: 71.0000 - accuracy: 0.8287 - precision: 0.7944 - recall: 0.7351 - auc: 0.8611 - val_loss: 0.4739 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8989\n",
      "Epoch 261/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.4786 - tp: 197.0000 - fp: 50.0000 - tn: 394.0000 - fn: 71.0000 - accuracy: 0.8301 - precision: 0.7976 - recall: 0.7351 - auc: 0.8748 - val_loss: 0.4823 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8965\n",
      "Epoch 262/500\n",
      "712/712 [==============================] - 0s 203us/sample - loss: 0.4883 - tp: 205.0000 - fp: 60.0000 - tn: 384.0000 - fn: 63.0000 - accuracy: 0.8272 - precision: 0.7736 - recall: 0.7649 - auc: 0.8762 - val_loss: 0.4753 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8976\n",
      "Epoch 263/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.4906 - tp: 204.0000 - fp: 54.0000 - tn: 390.0000 - fn: 64.0000 - accuracy: 0.8343 - precision: 0.7907 - recall: 0.7612 - auc: 0.8638 - val_loss: 0.4728 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8980\n",
      "Epoch 264/500\n",
      "712/712 [==============================] - 0s 233us/sample - loss: 0.4915 - tp: 198.0000 - fp: 45.0000 - tn: 399.0000 - fn: 70.0000 - accuracy: 0.8385 - precision: 0.8148 - recall: 0.7388 - auc: 0.8654 - val_loss: 0.4729 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8960\n",
      "Epoch 265/500\n",
      "712/712 [==============================] - 0s 228us/sample - loss: 0.4944 - tp: 201.0000 - fp: 54.0000 - tn: 390.0000 - fn: 67.0000 - accuracy: 0.8301 - precision: 0.7882 - recall: 0.7500 - auc: 0.8649 - val_loss: 0.4698 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.9031\n",
      "Epoch 266/500\n",
      "712/712 [==============================] - 0s 220us/sample - loss: 0.5022 - tp: 199.0000 - fp: 55.0000 - tn: 389.0000 - fn: 69.0000 - accuracy: 0.8258 - precision: 0.7835 - recall: 0.7425 - auc: 0.8610 - val_loss: 0.4769 - val_tp: 62.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 12.0000 - val_accuracy: 0.8212 - val_precision: 0.7561 - val_recall: 0.8378 - val_auc: 0.8953\n",
      "Epoch 267/500\n",
      "712/712 [==============================] - 0s 218us/sample - loss: 0.4800 - tp: 204.0000 - fp: 51.0000 - tn: 393.0000 - fn: 64.0000 - accuracy: 0.8385 - precision: 0.8000 - recall: 0.7612 - auc: 0.8735 - val_loss: 0.4749 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8953\n",
      "Epoch 268/500\n",
      "712/712 [==============================] - 0s 216us/sample - loss: 0.4898 - tp: 205.0000 - fp: 65.0000 - tn: 379.0000 - fn: 63.0000 - accuracy: 0.8202 - precision: 0.7593 - recall: 0.7649 - auc: 0.8724 - val_loss: 0.4771 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8967\n",
      "Epoch 269/500\n",
      "712/712 [==============================] - 0s 193us/sample - loss: 0.4884 - tp: 203.0000 - fp: 52.0000 - tn: 392.0000 - fn: 65.0000 - accuracy: 0.8357 - precision: 0.7961 - recall: 0.7575 - auc: 0.8717 - val_loss: 0.4769 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8965\n",
      "Epoch 270/500\n",
      "712/712 [==============================] - 0s 199us/sample - loss: 0.4776 - tp: 208.0000 - fp: 55.0000 - tn: 389.0000 - fn: 60.0000 - accuracy: 0.8385 - precision: 0.7909 - recall: 0.7761 - auc: 0.8791 - val_loss: 0.4720 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8961\n",
      "Epoch 271/500\n",
      "712/712 [==============================] - 0s 194us/sample - loss: 0.4868 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8669 - val_loss: 0.4797 - val_tp: 62.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 12.0000 - val_accuracy: 0.8045 - val_precision: 0.7294 - val_recall: 0.8378 - val_auc: 0.8965\n",
      "Epoch 272/500\n",
      "712/712 [==============================] - 0s 208us/sample - loss: 0.4806 - tp: 202.0000 - fp: 58.0000 - tn: 386.0000 - fn: 66.0000 - accuracy: 0.8258 - precision: 0.7769 - recall: 0.7537 - auc: 0.8692 - val_loss: 0.4762 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8949\n",
      "Epoch 273/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.4847 - tp: 201.0000 - fp: 59.0000 - tn: 385.0000 - fn: 67.0000 - accuracy: 0.8230 - precision: 0.7731 - recall: 0.7500 - auc: 0.8693 - val_loss: 0.4807 - val_tp: 55.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 19.0000 - val_accuracy: 0.8156 - val_precision: 0.7971 - val_recall: 0.7432 - val_auc: 0.8938\n",
      "Epoch 274/500\n",
      "712/712 [==============================] - 0s 179us/sample - loss: 0.4869 - tp: 210.0000 - fp: 62.0000 - tn: 382.0000 - fn: 58.0000 - accuracy: 0.8315 - precision: 0.7721 - recall: 0.7836 - auc: 0.8664 - val_loss: 0.4701 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8966\n",
      "Epoch 275/500\n",
      "712/712 [==============================] - 0s 196us/sample - loss: 0.4879 - tp: 204.0000 - fp: 57.0000 - tn: 387.0000 - fn: 64.0000 - accuracy: 0.8301 - precision: 0.7816 - recall: 0.7612 - auc: 0.8651 - val_loss: 0.4773 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8977\n",
      "Epoch 276/500\n",
      "712/712 [==============================] - 0s 196us/sample - loss: 0.4792 - tp: 205.0000 - fp: 61.0000 - tn: 383.0000 - fn: 63.0000 - accuracy: 0.8258 - precision: 0.7707 - recall: 0.7649 - auc: 0.8752 - val_loss: 0.4708 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8961\n",
      "Epoch 277/500\n",
      "712/712 [==============================] - 0s 153us/sample - loss: 0.4856 - tp: 201.0000 - fp: 56.0000 - tn: 388.0000 - fn: 67.0000 - accuracy: 0.8272 - precision: 0.7821 - recall: 0.7500 - auc: 0.8698 - val_loss: 0.4771 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8958\n",
      "Epoch 278/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4813 - tp: 200.0000 - fp: 50.0000 - tn: 394.0000 - fn: 68.0000 - accuracy: 0.8343 - precision: 0.8000 - recall: 0.7463 - auc: 0.8700 - val_loss: 0.4761 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8967\n",
      "Epoch 279/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4861 - tp: 205.0000 - fp: 63.0000 - tn: 381.0000 - fn: 63.0000 - accuracy: 0.8230 - precision: 0.7649 - recall: 0.7649 - auc: 0.8623 - val_loss: 0.4825 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8931\n",
      "Epoch 280/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4928 - tp: 193.0000 - fp: 45.0000 - tn: 399.0000 - fn: 75.0000 - accuracy: 0.8315 - precision: 0.8109 - recall: 0.7201 - auc: 0.8605 - val_loss: 0.4778 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8965\n",
      "Epoch 281/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.4940 - tp: 204.0000 - fp: 63.0000 - tn: 381.0000 - fn: 64.0000 - accuracy: 0.8216 - precision: 0.7640 - recall: 0.7612 - auc: 0.8627 - val_loss: 0.4879 - val_tp: 56.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 18.0000 - val_accuracy: 0.8380 - val_precision: 0.8358 - val_recall: 0.7568 - val_auc: 0.8968\n",
      "Epoch 282/500\n",
      "712/712 [==============================] - 0s 208us/sample - loss: 0.5044 - tp: 205.0000 - fp: 52.0000 - tn: 392.0000 - fn: 63.0000 - accuracy: 0.8385 - precision: 0.7977 - recall: 0.7649 - auc: 0.8490 - val_loss: 0.4731 - val_tp: 61.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 13.0000 - val_accuracy: 0.8324 - val_precision: 0.7821 - val_recall: 0.8243 - val_auc: 0.8986\n",
      "Epoch 283/500\n",
      "712/712 [==============================] - 0s 192us/sample - loss: 0.4963 - tp: 204.0000 - fp: 49.0000 - tn: 395.0000 - fn: 64.0000 - accuracy: 0.8413 - precision: 0.8063 - recall: 0.7612 - auc: 0.8502 - val_loss: 0.4712 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8963\n",
      "Epoch 284/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 0.4868 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8635 - val_loss: 0.4761 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8940\n",
      "Epoch 285/500\n",
      "712/712 [==============================] - 0s 207us/sample - loss: 0.5141 - tp: 205.0000 - fp: 63.0000 - tn: 381.0000 - fn: 63.0000 - accuracy: 0.8230 - precision: 0.7649 - recall: 0.7649 - auc: 0.8534 - val_loss: 0.4832 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8959\n",
      "Epoch 286/500\n",
      "712/712 [==============================] - 0s 247us/sample - loss: 0.4829 - tp: 205.0000 - fp: 56.0000 - tn: 388.0000 - fn: 63.0000 - accuracy: 0.8329 - precision: 0.7854 - recall: 0.7649 - auc: 0.8727 - val_loss: 0.4779 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8958\n",
      "Epoch 287/500\n",
      "712/712 [==============================] - 0s 234us/sample - loss: 0.4922 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8679 - val_loss: 0.4815 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8959\n",
      "Epoch 288/500\n",
      "712/712 [==============================] - 0s 237us/sample - loss: 0.4832 - tp: 202.0000 - fp: 45.0000 - tn: 399.0000 - fn: 66.0000 - accuracy: 0.8441 - precision: 0.8178 - recall: 0.7537 - auc: 0.8707 - val_loss: 0.4865 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8943\n",
      "Epoch 289/500\n",
      "712/712 [==============================] - 0s 233us/sample - loss: 0.4892 - tp: 199.0000 - fp: 54.0000 - tn: 390.0000 - fn: 69.0000 - accuracy: 0.8272 - precision: 0.7866 - recall: 0.7425 - auc: 0.8742 - val_loss: 0.4939 - val_tp: 64.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 10.0000 - val_accuracy: 0.7989 - val_precision: 0.7111 - val_recall: 0.8649 - val_auc: 0.8933\n",
      "Epoch 290/500\n",
      "712/712 [==============================] - 0s 191us/sample - loss: 0.4721 - tp: 202.0000 - fp: 50.0000 - tn: 394.0000 - fn: 66.0000 - accuracy: 0.8371 - precision: 0.8016 - recall: 0.7537 - auc: 0.8804 - val_loss: 0.4721 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8950\n",
      "Epoch 291/500\n",
      "712/712 [==============================] - 0s 220us/sample - loss: 0.4809 - tp: 206.0000 - fp: 54.0000 - tn: 390.0000 - fn: 62.0000 - accuracy: 0.8371 - precision: 0.7923 - recall: 0.7687 - auc: 0.8724 - val_loss: 0.4703 - val_tp: 61.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 13.0000 - val_accuracy: 0.8101 - val_precision: 0.7439 - val_recall: 0.8243 - val_auc: 0.8953\n",
      "Epoch 292/500\n",
      "712/712 [==============================] - 0s 188us/sample - loss: 0.5035 - tp: 197.0000 - fp: 62.0000 - tn: 382.0000 - fn: 71.0000 - accuracy: 0.8132 - precision: 0.7606 - recall: 0.7351 - auc: 0.8566 - val_loss: 0.4683 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8994\n",
      "Epoch 293/500\n",
      "712/712 [==============================] - 0s 196us/sample - loss: 0.4856 - tp: 204.0000 - fp: 59.0000 - tn: 385.0000 - fn: 64.0000 - accuracy: 0.8272 - precision: 0.7757 - recall: 0.7612 - auc: 0.8688 - val_loss: 0.4727 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8987\n",
      "Epoch 294/500\n",
      "712/712 [==============================] - 0s 199us/sample - loss: 0.4864 - tp: 208.0000 - fp: 54.0000 - tn: 390.0000 - fn: 60.0000 - accuracy: 0.8399 - precision: 0.7939 - recall: 0.7761 - auc: 0.8634 - val_loss: 0.4744 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8948\n",
      "Epoch 295/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4930 - tp: 203.0000 - fp: 56.0000 - tn: 388.0000 - fn: 65.0000 - accuracy: 0.8301 - precision: 0.7838 - recall: 0.7575 - auc: 0.8606 - val_loss: 0.4827 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8969\n",
      "Epoch 296/500\n",
      "712/712 [==============================] - 0s 191us/sample - loss: 0.4881 - tp: 204.0000 - fp: 55.0000 - tn: 389.0000 - fn: 64.0000 - accuracy: 0.8329 - precision: 0.7876 - recall: 0.7612 - auc: 0.8668 - val_loss: 0.4815 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8923\n",
      "Epoch 297/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4794 - tp: 197.0000 - fp: 46.0000 - tn: 398.0000 - fn: 71.0000 - accuracy: 0.8357 - precision: 0.8107 - recall: 0.7351 - auc: 0.8757 - val_loss: 0.4783 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8949\n",
      "Epoch 298/500\n",
      "712/712 [==============================] - 0s 190us/sample - loss: 0.4812 - tp: 208.0000 - fp: 66.0000 - tn: 378.0000 - fn: 60.0000 - accuracy: 0.8230 - precision: 0.7591 - recall: 0.7761 - auc: 0.8720 - val_loss: 0.4747 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8956\n",
      "Epoch 299/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.4820 - tp: 202.0000 - fp: 52.0000 - tn: 392.0000 - fn: 66.0000 - accuracy: 0.8343 - precision: 0.7953 - recall: 0.7537 - auc: 0.8676 - val_loss: 0.4738 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8983\n",
      "Epoch 300/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.4847 - tp: 202.0000 - fp: 46.0000 - tn: 398.0000 - fn: 66.0000 - accuracy: 0.8427 - precision: 0.8145 - recall: 0.7537 - auc: 0.8664 - val_loss: 0.4792 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8944\n",
      "Epoch 301/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4915 - tp: 207.0000 - fp: 60.0000 - tn: 384.0000 - fn: 61.0000 - accuracy: 0.8301 - precision: 0.7753 - recall: 0.7724 - auc: 0.8677 - val_loss: 0.4745 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8934\n",
      "Epoch 302/500\n",
      "712/712 [==============================] - 0s 183us/sample - loss: 0.4813 - tp: 201.0000 - fp: 42.0000 - tn: 402.0000 - fn: 67.0000 - accuracy: 0.8469 - precision: 0.8272 - recall: 0.7500 - auc: 0.8736 - val_loss: 0.4776 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8954\n",
      "Epoch 303/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.4846 - tp: 193.0000 - fp: 47.0000 - tn: 397.0000 - fn: 75.0000 - accuracy: 0.8287 - precision: 0.8042 - recall: 0.7201 - auc: 0.8725 - val_loss: 0.4694 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8978\n",
      "Epoch 304/500\n",
      "712/712 [==============================] - 0s 188us/sample - loss: 0.4882 - tp: 198.0000 - fp: 53.0000 - tn: 391.0000 - fn: 70.0000 - accuracy: 0.8272 - precision: 0.7888 - recall: 0.7388 - auc: 0.8726 - val_loss: 0.4748 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8986\n",
      "Epoch 305/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.4837 - tp: 205.0000 - fp: 47.0000 - tn: 397.0000 - fn: 63.0000 - accuracy: 0.8455 - precision: 0.8135 - recall: 0.7649 - auc: 0.8684 - val_loss: 0.4728 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.8973\n",
      "Epoch 306/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.4895 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8713 - val_loss: 0.4698 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8972\n",
      "Epoch 307/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4881 - tp: 197.0000 - fp: 52.0000 - tn: 392.0000 - fn: 71.0000 - accuracy: 0.8272 - precision: 0.7912 - recall: 0.7351 - auc: 0.8666 - val_loss: 0.4821 - val_tp: 62.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 12.0000 - val_accuracy: 0.8101 - val_precision: 0.7381 - val_recall: 0.8378 - val_auc: 0.8949\n",
      "Epoch 308/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.4860 - tp: 204.0000 - fp: 61.0000 - tn: 383.0000 - fn: 64.0000 - accuracy: 0.8244 - precision: 0.7698 - recall: 0.7612 - auc: 0.8630 - val_loss: 0.4729 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8995\n",
      "Epoch 309/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4777 - tp: 207.0000 - fp: 60.0000 - tn: 384.0000 - fn: 61.0000 - accuracy: 0.8301 - precision: 0.7753 - recall: 0.7724 - auc: 0.8810 - val_loss: 0.4834 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8974\n",
      "Epoch 310/500\n",
      "712/712 [==============================] - 0s 191us/sample - loss: 0.4956 - tp: 198.0000 - fp: 46.0000 - tn: 398.0000 - fn: 70.0000 - accuracy: 0.8371 - precision: 0.8115 - recall: 0.7388 - auc: 0.8642 - val_loss: 0.4689 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8951\n",
      "Epoch 311/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.4810 - tp: 207.0000 - fp: 62.0000 - tn: 382.0000 - fn: 61.0000 - accuracy: 0.8272 - precision: 0.7695 - recall: 0.7724 - auc: 0.8690 - val_loss: 0.4722 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8955\n",
      "Epoch 312/500\n",
      "712/712 [==============================] - 0s 183us/sample - loss: 0.4899 - tp: 197.0000 - fp: 52.0000 - tn: 392.0000 - fn: 71.0000 - accuracy: 0.8272 - precision: 0.7912 - recall: 0.7351 - auc: 0.8675 - val_loss: 0.4744 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8943\n",
      "Epoch 313/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.5005 - tp: 193.0000 - fp: 50.0000 - tn: 394.0000 - fn: 75.0000 - accuracy: 0.8244 - precision: 0.7942 - recall: 0.7201 - auc: 0.8535 - val_loss: 0.4809 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8938\n",
      "Epoch 314/500\n",
      "712/712 [==============================] - 0s 190us/sample - loss: 0.4828 - tp: 202.0000 - fp: 49.0000 - tn: 395.0000 - fn: 66.0000 - accuracy: 0.8385 - precision: 0.8048 - recall: 0.7537 - auc: 0.8685 - val_loss: 0.4770 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8930\n",
      "Epoch 315/500\n",
      "712/712 [==============================] - 0s 189us/sample - loss: 0.4907 - tp: 202.0000 - fp: 57.0000 - tn: 387.0000 - fn: 66.0000 - accuracy: 0.8272 - precision: 0.7799 - recall: 0.7537 - auc: 0.8638 - val_loss: 0.4898 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8970\n",
      "Epoch 316/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4980 - tp: 199.0000 - fp: 47.0000 - tn: 397.0000 - fn: 69.0000 - accuracy: 0.8371 - precision: 0.8089 - recall: 0.7425 - auc: 0.8579 - val_loss: 0.4738 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8976\n",
      "Epoch 317/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.4803 - tp: 203.0000 - fp: 49.0000 - tn: 395.0000 - fn: 65.0000 - accuracy: 0.8399 - precision: 0.8056 - recall: 0.7575 - auc: 0.8755 - val_loss: 0.4753 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8959\n",
      "Epoch 318/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4997 - tp: 205.0000 - fp: 59.0000 - tn: 385.0000 - fn: 63.0000 - accuracy: 0.8287 - precision: 0.7765 - recall: 0.7649 - auc: 0.8530 - val_loss: 0.4804 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8962\n",
      "Epoch 319/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.5031 - tp: 186.0000 - fp: 47.0000 - tn: 397.0000 - fn: 82.0000 - accuracy: 0.8188 - precision: 0.7983 - recall: 0.6940 - auc: 0.8681 - val_loss: 0.4720 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8970\n",
      "Epoch 320/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4825 - tp: 207.0000 - fp: 63.0000 - tn: 381.0000 - fn: 61.0000 - accuracy: 0.8258 - precision: 0.7667 - recall: 0.7724 - auc: 0.8680 - val_loss: 0.4690 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8969\n",
      "Epoch 321/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4810 - tp: 200.0000 - fp: 59.0000 - tn: 385.0000 - fn: 68.0000 - accuracy: 0.8216 - precision: 0.7722 - recall: 0.7463 - auc: 0.8719 - val_loss: 0.4723 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.9001\n",
      "Epoch 322/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.4862 - tp: 195.0000 - fp: 47.0000 - tn: 397.0000 - fn: 73.0000 - accuracy: 0.8315 - precision: 0.8058 - recall: 0.7276 - auc: 0.8672 - val_loss: 0.4761 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8981\n",
      "Epoch 323/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.4812 - tp: 205.0000 - fp: 60.0000 - tn: 384.0000 - fn: 63.0000 - accuracy: 0.8272 - precision: 0.7736 - recall: 0.7649 - auc: 0.8731 - val_loss: 0.4707 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8948\n",
      "Epoch 324/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4868 - tp: 204.0000 - fp: 51.0000 - tn: 393.0000 - fn: 64.0000 - accuracy: 0.8385 - precision: 0.8000 - recall: 0.7612 - auc: 0.8694 - val_loss: 0.4719 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8981\n",
      "Epoch 325/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4868 - tp: 203.0000 - fp: 51.0000 - tn: 393.0000 - fn: 65.0000 - accuracy: 0.8371 - precision: 0.7992 - recall: 0.7575 - auc: 0.8697 - val_loss: 0.4758 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8950\n",
      "Epoch 326/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4715 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8779 - val_loss: 0.4699 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8961\n",
      "Epoch 327/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.4741 - tp: 204.0000 - fp: 46.0000 - tn: 398.0000 - fn: 64.0000 - accuracy: 0.8455 - precision: 0.8160 - recall: 0.7612 - auc: 0.8720 - val_loss: 0.4736 - val_tp: 62.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 12.0000 - val_accuracy: 0.8045 - val_precision: 0.7294 - val_recall: 0.8378 - val_auc: 0.8958\n",
      "Epoch 328/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4866 - tp: 205.0000 - fp: 57.0000 - tn: 387.0000 - fn: 63.0000 - accuracy: 0.8315 - precision: 0.7824 - recall: 0.7649 - auc: 0.8704 - val_loss: 0.4624 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8973\n",
      "Epoch 329/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.5043 - tp: 198.0000 - fp: 49.0000 - tn: 395.0000 - fn: 70.0000 - accuracy: 0.8329 - precision: 0.8016 - recall: 0.7388 - auc: 0.8546 - val_loss: 0.4674 - val_tp: 56.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 18.0000 - val_accuracy: 0.8156 - val_precision: 0.7887 - val_recall: 0.7568 - val_auc: 0.9001\n",
      "Epoch 330/500\n",
      "712/712 [==============================] - 0s 156us/sample - loss: 0.4679 - tp: 205.0000 - fp: 57.0000 - tn: 387.0000 - fn: 63.0000 - accuracy: 0.8315 - precision: 0.7824 - recall: 0.7649 - auc: 0.8783 - val_loss: 0.4708 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8961\n",
      "Epoch 331/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.4902 - tp: 196.0000 - fp: 56.0000 - tn: 388.0000 - fn: 72.0000 - accuracy: 0.8202 - precision: 0.7778 - recall: 0.7313 - auc: 0.8606 - val_loss: 0.4681 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8974\n",
      "Epoch 332/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 0.4874 - tp: 209.0000 - fp: 55.0000 - tn: 389.0000 - fn: 59.0000 - accuracy: 0.8399 - precision: 0.7917 - recall: 0.7799 - auc: 0.8657 - val_loss: 0.4791 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8960\n",
      "Epoch 333/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4937 - tp: 200.0000 - fp: 58.0000 - tn: 386.0000 - fn: 68.0000 - accuracy: 0.8230 - precision: 0.7752 - recall: 0.7463 - auc: 0.8600 - val_loss: 0.4743 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8983\n",
      "Epoch 334/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.4768 - tp: 203.0000 - fp: 50.0000 - tn: 394.0000 - fn: 65.0000 - accuracy: 0.8385 - precision: 0.8024 - recall: 0.7575 - auc: 0.8776 - val_loss: 0.4679 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8987\n",
      "Epoch 335/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4837 - tp: 202.0000 - fp: 49.0000 - tn: 395.0000 - fn: 66.0000 - accuracy: 0.8385 - precision: 0.8048 - recall: 0.7537 - auc: 0.8613 - val_loss: 0.4716 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8961\n",
      "Epoch 336/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.4879 - tp: 199.0000 - fp: 51.0000 - tn: 393.0000 - fn: 69.0000 - accuracy: 0.8315 - precision: 0.7960 - recall: 0.7425 - auc: 0.8653 - val_loss: 0.4695 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8981\n",
      "Epoch 337/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4915 - tp: 204.0000 - fp: 55.0000 - tn: 389.0000 - fn: 64.0000 - accuracy: 0.8329 - precision: 0.7876 - recall: 0.7612 - auc: 0.8596 - val_loss: 0.4734 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8958\n",
      "Epoch 338/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4853 - tp: 202.0000 - fp: 56.0000 - tn: 388.0000 - fn: 66.0000 - accuracy: 0.8287 - precision: 0.7829 - recall: 0.7537 - auc: 0.8642 - val_loss: 0.4790 - val_tp: 61.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 13.0000 - val_accuracy: 0.8156 - val_precision: 0.7531 - val_recall: 0.8243 - val_auc: 0.8954\n",
      "Epoch 339/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4869 - tp: 197.0000 - fp: 57.0000 - tn: 387.0000 - fn: 71.0000 - accuracy: 0.8202 - precision: 0.7756 - recall: 0.7351 - auc: 0.8734 - val_loss: 0.4713 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.9044\n",
      "Epoch 340/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.5064 - tp: 192.0000 - fp: 51.0000 - tn: 393.0000 - fn: 76.0000 - accuracy: 0.8216 - precision: 0.7901 - recall: 0.7164 - auc: 0.8518 - val_loss: 0.4715 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8950\n",
      "Epoch 341/500\n",
      "712/712 [==============================] - 0s 198us/sample - loss: 0.4846 - tp: 204.0000 - fp: 47.0000 - tn: 397.0000 - fn: 64.0000 - accuracy: 0.8441 - precision: 0.8127 - recall: 0.7612 - auc: 0.8658 - val_loss: 0.4831 - val_tp: 68.0000 - val_fp: 27.0000 - val_tn: 78.0000 - val_fn: 6.0000 - val_accuracy: 0.8156 - val_precision: 0.7158 - val_recall: 0.9189 - val_auc: 0.8958\n",
      "Epoch 342/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4874 - tp: 196.0000 - fp: 51.0000 - tn: 393.0000 - fn: 72.0000 - accuracy: 0.8272 - precision: 0.7935 - recall: 0.7313 - auc: 0.8658 - val_loss: 0.4766 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8954\n",
      "Epoch 343/500\n",
      "712/712 [==============================] - 0s 187us/sample - loss: 0.4835 - tp: 204.0000 - fp: 61.0000 - tn: 383.0000 - fn: 64.0000 - accuracy: 0.8244 - precision: 0.7698 - recall: 0.7612 - auc: 0.8707 - val_loss: 0.4795 - val_tp: 55.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 19.0000 - val_accuracy: 0.8212 - val_precision: 0.8088 - val_recall: 0.7432 - val_auc: 0.8936\n",
      "Epoch 344/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.4841 - tp: 198.0000 - fp: 48.0000 - tn: 396.0000 - fn: 70.0000 - accuracy: 0.8343 - precision: 0.8049 - recall: 0.7388 - auc: 0.8740 - val_loss: 0.4787 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8968\n",
      "Epoch 345/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4864 - tp: 207.0000 - fp: 53.0000 - tn: 391.0000 - fn: 61.0000 - accuracy: 0.8399 - precision: 0.7962 - recall: 0.7724 - auc: 0.8627 - val_loss: 0.4658 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.9003\n",
      "Epoch 346/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4898 - tp: 196.0000 - fp: 47.0000 - tn: 397.0000 - fn: 72.0000 - accuracy: 0.8329 - precision: 0.8066 - recall: 0.7313 - auc: 0.8759 - val_loss: 0.4794 - val_tp: 60.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 14.0000 - val_accuracy: 0.7989 - val_precision: 0.7317 - val_recall: 0.8108 - val_auc: 0.8955\n",
      "Epoch 347/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.4825 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8652 - val_loss: 0.4750 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8964\n",
      "Epoch 348/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4914 - tp: 204.0000 - fp: 54.0000 - tn: 390.0000 - fn: 64.0000 - accuracy: 0.8343 - precision: 0.7907 - recall: 0.7612 - auc: 0.8650 - val_loss: 0.4661 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9000\n",
      "Epoch 349/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4799 - tp: 189.0000 - fp: 36.0000 - tn: 408.0000 - fn: 79.0000 - accuracy: 0.8385 - precision: 0.8400 - recall: 0.7052 - auc: 0.8861 - val_loss: 0.4723 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8949\n",
      "Epoch 350/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.4983 - tp: 205.0000 - fp: 64.0000 - tn: 380.0000 - fn: 63.0000 - accuracy: 0.8216 - precision: 0.7621 - recall: 0.7649 - auc: 0.8630 - val_loss: 0.4662 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8974\n",
      "Epoch 351/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4794 - tp: 197.0000 - fp: 41.0000 - tn: 403.0000 - fn: 71.0000 - accuracy: 0.8427 - precision: 0.8277 - recall: 0.7351 - auc: 0.8768 - val_loss: 0.4668 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8974\n",
      "Epoch 352/500\n",
      "712/712 [==============================] - 0s 189us/sample - loss: 0.4863 - tp: 210.0000 - fp: 71.0000 - tn: 373.0000 - fn: 58.0000 - accuracy: 0.8188 - precision: 0.7473 - recall: 0.7836 - auc: 0.8678 - val_loss: 0.4761 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8964\n",
      "Epoch 353/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.4973 - tp: 206.0000 - fp: 67.0000 - tn: 377.0000 - fn: 62.0000 - accuracy: 0.8188 - precision: 0.7546 - recall: 0.7687 - auc: 0.8665 - val_loss: 0.4753 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8958\n",
      "Epoch 354/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4710 - tp: 205.0000 - fp: 53.0000 - tn: 391.0000 - fn: 63.0000 - accuracy: 0.8371 - precision: 0.7946 - recall: 0.7649 - auc: 0.8836 - val_loss: 0.4753 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8935\n",
      "Epoch 355/500\n",
      "712/712 [==============================] - 0s 181us/sample - loss: 0.4971 - tp: 200.0000 - fp: 51.0000 - tn: 393.0000 - fn: 68.0000 - accuracy: 0.8329 - precision: 0.7968 - recall: 0.7463 - auc: 0.8604 - val_loss: 0.4838 - val_tp: 61.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 13.0000 - val_accuracy: 0.8156 - val_precision: 0.7531 - val_recall: 0.8243 - val_auc: 0.8936\n",
      "Epoch 356/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4785 - tp: 198.0000 - fp: 63.0000 - tn: 381.0000 - fn: 70.0000 - accuracy: 0.8132 - precision: 0.7586 - recall: 0.7388 - auc: 0.8751 - val_loss: 0.4758 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8949\n",
      "Epoch 357/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.4796 - tp: 203.0000 - fp: 50.0000 - tn: 394.0000 - fn: 65.0000 - accuracy: 0.8385 - precision: 0.8024 - recall: 0.7575 - auc: 0.8689 - val_loss: 0.4753 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8954\n",
      "Epoch 358/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.4933 - tp: 198.0000 - fp: 48.0000 - tn: 396.0000 - fn: 70.0000 - accuracy: 0.8343 - precision: 0.8049 - recall: 0.7388 - auc: 0.8694 - val_loss: 0.5046 - val_tp: 68.0000 - val_fp: 32.0000 - val_tn: 73.0000 - val_fn: 6.0000 - val_accuracy: 0.7877 - val_precision: 0.6800 - val_recall: 0.9189 - val_auc: 0.8932\n",
      "Epoch 359/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4887 - tp: 195.0000 - fp: 55.0000 - tn: 389.0000 - fn: 73.0000 - accuracy: 0.8202 - precision: 0.7800 - recall: 0.7276 - auc: 0.8688 - val_loss: 0.4714 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8957\n",
      "Epoch 360/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4874 - tp: 205.0000 - fp: 59.0000 - tn: 385.0000 - fn: 63.0000 - accuracy: 0.8287 - precision: 0.7765 - recall: 0.7649 - auc: 0.8725 - val_loss: 0.4640 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8995\n",
      "Epoch 361/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4922 - tp: 200.0000 - fp: 49.0000 - tn: 395.0000 - fn: 68.0000 - accuracy: 0.8357 - precision: 0.8032 - recall: 0.7463 - auc: 0.8637 - val_loss: 0.4662 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8976\n",
      "Epoch 362/500\n",
      "712/712 [==============================] - 0s 149us/sample - loss: 0.4947 - tp: 191.0000 - fp: 54.0000 - tn: 390.0000 - fn: 77.0000 - accuracy: 0.8160 - precision: 0.7796 - recall: 0.7127 - auc: 0.8588 - val_loss: 0.4695 - val_tp: 60.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 14.0000 - val_accuracy: 0.8101 - val_precision: 0.7500 - val_recall: 0.8108 - val_auc: 0.8944\n",
      "Epoch 363/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4841 - tp: 198.0000 - fp: 57.0000 - tn: 387.0000 - fn: 70.0000 - accuracy: 0.8216 - precision: 0.7765 - recall: 0.7388 - auc: 0.8716 - val_loss: 0.4760 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8923\n",
      "Epoch 364/500\n",
      "712/712 [==============================] - 0s 156us/sample - loss: 0.4829 - tp: 197.0000 - fp: 51.0000 - tn: 393.0000 - fn: 71.0000 - accuracy: 0.8287 - precision: 0.7944 - recall: 0.7351 - auc: 0.8688 - val_loss: 0.4768 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8962\n",
      "Epoch 365/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4771 - tp: 203.0000 - fp: 53.0000 - tn: 391.0000 - fn: 65.0000 - accuracy: 0.8343 - precision: 0.7930 - recall: 0.7575 - auc: 0.8711 - val_loss: 0.4792 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8931\n",
      "Epoch 366/500\n",
      "712/712 [==============================] - 0s 188us/sample - loss: 0.4777 - tp: 207.0000 - fp: 48.0000 - tn: 396.0000 - fn: 61.0000 - accuracy: 0.8469 - precision: 0.8118 - recall: 0.7724 - auc: 0.8698 - val_loss: 0.4862 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8932\n",
      "Epoch 367/500\n",
      "712/712 [==============================] - 0s 186us/sample - loss: 0.4684 - tp: 206.0000 - fp: 57.0000 - tn: 387.0000 - fn: 62.0000 - accuracy: 0.8329 - precision: 0.7833 - recall: 0.7687 - auc: 0.8788 - val_loss: 0.4821 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8956\n",
      "Epoch 368/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4924 - tp: 197.0000 - fp: 43.0000 - tn: 401.0000 - fn: 71.0000 - accuracy: 0.8399 - precision: 0.8208 - recall: 0.7351 - auc: 0.8602 - val_loss: 0.4836 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8947\n",
      "Epoch 369/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.4794 - tp: 195.0000 - fp: 54.0000 - tn: 390.0000 - fn: 73.0000 - accuracy: 0.8216 - precision: 0.7831 - recall: 0.7276 - auc: 0.8740 - val_loss: 0.4661 - val_tp: 62.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 12.0000 - val_accuracy: 0.8156 - val_precision: 0.7470 - val_recall: 0.8378 - val_auc: 0.8921\n",
      "Epoch 370/500\n",
      "712/712 [==============================] - 0s 154us/sample - loss: 0.4828 - tp: 210.0000 - fp: 67.0000 - tn: 377.0000 - fn: 58.0000 - accuracy: 0.8244 - precision: 0.7581 - recall: 0.7836 - auc: 0.8676 - val_loss: 0.4669 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8929\n",
      "Epoch 371/500\n",
      "712/712 [==============================] - 0s 148us/sample - loss: 0.4901 - tp: 198.0000 - fp: 44.0000 - tn: 400.0000 - fn: 70.0000 - accuracy: 0.8399 - precision: 0.8182 - recall: 0.7388 - auc: 0.8635 - val_loss: 0.4945 - val_tp: 65.0000 - val_fp: 29.0000 - val_tn: 76.0000 - val_fn: 9.0000 - val_accuracy: 0.7877 - val_precision: 0.6915 - val_recall: 0.8784 - val_auc: 0.8918\n",
      "Epoch 372/500\n",
      "712/712 [==============================] - 0s 153us/sample - loss: 0.4991 - tp: 205.0000 - fp: 64.0000 - tn: 380.0000 - fn: 63.0000 - accuracy: 0.8216 - precision: 0.7621 - recall: 0.7649 - auc: 0.8709 - val_loss: 0.4636 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8909\n",
      "Epoch 373/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.4785 - tp: 202.0000 - fp: 56.0000 - tn: 388.0000 - fn: 66.0000 - accuracy: 0.8287 - precision: 0.7829 - recall: 0.7537 - auc: 0.8705 - val_loss: 0.4710 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8976\n",
      "Epoch 374/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.4910 - tp: 204.0000 - fp: 54.0000 - tn: 390.0000 - fn: 64.0000 - accuracy: 0.8343 - precision: 0.7907 - recall: 0.7612 - auc: 0.8577 - val_loss: 0.4755 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8976\n",
      "Epoch 375/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.4851 - tp: 199.0000 - fp: 45.0000 - tn: 399.0000 - fn: 69.0000 - accuracy: 0.8399 - precision: 0.8156 - recall: 0.7425 - auc: 0.8685 - val_loss: 0.4737 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8956\n",
      "Epoch 376/500\n",
      "712/712 [==============================] - 0s 188us/sample - loss: 0.4791 - tp: 202.0000 - fp: 54.0000 - tn: 390.0000 - fn: 66.0000 - accuracy: 0.8315 - precision: 0.7891 - recall: 0.7537 - auc: 0.8701 - val_loss: 0.4765 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8977\n",
      "Epoch 377/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4970 - tp: 194.0000 - fp: 50.0000 - tn: 394.0000 - fn: 74.0000 - accuracy: 0.8258 - precision: 0.7951 - recall: 0.7239 - auc: 0.8633 - val_loss: 0.4845 - val_tp: 54.0000 - val_fp: 11.0000 - val_tn: 94.0000 - val_fn: 20.0000 - val_accuracy: 0.8268 - val_precision: 0.8308 - val_recall: 0.7297 - val_auc: 0.8999\n",
      "Epoch 378/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.4972 - tp: 201.0000 - fp: 70.0000 - tn: 374.0000 - fn: 67.0000 - accuracy: 0.8076 - precision: 0.7417 - recall: 0.7500 - auc: 0.8617 - val_loss: 0.4683 - val_tp: 59.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 15.0000 - val_accuracy: 0.8436 - val_precision: 0.8194 - val_recall: 0.7973 - val_auc: 0.8986\n",
      "Epoch 379/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.4840 - tp: 208.0000 - fp: 55.0000 - tn: 389.0000 - fn: 60.0000 - accuracy: 0.8385 - precision: 0.7909 - recall: 0.7761 - auc: 0.8717 - val_loss: 0.4678 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8953\n",
      "Epoch 380/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.4775 - tp: 202.0000 - fp: 46.0000 - tn: 398.0000 - fn: 66.0000 - accuracy: 0.8427 - precision: 0.8145 - recall: 0.7537 - auc: 0.8732 - val_loss: 0.4855 - val_tp: 64.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 10.0000 - val_accuracy: 0.7877 - val_precision: 0.6957 - val_recall: 0.8649 - val_auc: 0.8939\n",
      "Epoch 381/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.4956 - tp: 196.0000 - fp: 52.0000 - tn: 392.0000 - fn: 72.0000 - accuracy: 0.8258 - precision: 0.7903 - recall: 0.7313 - auc: 0.8640 - val_loss: 0.4829 - val_tp: 64.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 10.0000 - val_accuracy: 0.8045 - val_precision: 0.7191 - val_recall: 0.8649 - val_auc: 0.8966\n",
      "Epoch 382/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4951 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8628 - val_loss: 0.4730 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8976\n",
      "Epoch 383/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.5025 - tp: 206.0000 - fp: 53.0000 - tn: 391.0000 - fn: 62.0000 - accuracy: 0.8385 - precision: 0.7954 - recall: 0.7687 - auc: 0.8575 - val_loss: 0.4720 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8968\n",
      "Epoch 384/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.4858 - tp: 204.0000 - fp: 57.0000 - tn: 387.0000 - fn: 64.0000 - accuracy: 0.8301 - precision: 0.7816 - recall: 0.7612 - auc: 0.8671 - val_loss: 0.4736 - val_tp: 60.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 14.0000 - val_accuracy: 0.8380 - val_precision: 0.8000 - val_recall: 0.8108 - val_auc: 0.8953\n",
      "Epoch 385/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.4835 - tp: 206.0000 - fp: 50.0000 - tn: 394.0000 - fn: 62.0000 - accuracy: 0.8427 - precision: 0.8047 - recall: 0.7687 - auc: 0.8675 - val_loss: 0.4723 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8983\n",
      "Epoch 386/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.4762 - tp: 204.0000 - fp: 51.0000 - tn: 393.0000 - fn: 64.0000 - accuracy: 0.8385 - precision: 0.8000 - recall: 0.7612 - auc: 0.8689 - val_loss: 0.4746 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8952\n",
      "Epoch 387/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.4664 - tp: 209.0000 - fp: 67.0000 - tn: 377.0000 - fn: 59.0000 - accuracy: 0.8230 - precision: 0.7572 - recall: 0.7799 - auc: 0.8847 - val_loss: 0.4788 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8922\n",
      "Epoch 388/500\n",
      "712/712 [==============================] - 0s 195us/sample - loss: 0.4701 - tp: 204.0000 - fp: 51.0000 - tn: 393.0000 - fn: 64.0000 - accuracy: 0.8385 - precision: 0.8000 - recall: 0.7612 - auc: 0.8866 - val_loss: 0.4683 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.8979\n",
      "Epoch 389/500\n",
      "712/712 [==============================] - 0s 188us/sample - loss: 0.4868 - tp: 191.0000 - fp: 52.0000 - tn: 392.0000 - fn: 77.0000 - accuracy: 0.8188 - precision: 0.7860 - recall: 0.7127 - auc: 0.8656 - val_loss: 0.4627 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8974\n",
      "Epoch 390/500\n",
      "712/712 [==============================] - 0s 196us/sample - loss: 0.4800 - tp: 202.0000 - fp: 51.0000 - tn: 393.0000 - fn: 66.0000 - accuracy: 0.8357 - precision: 0.7984 - recall: 0.7537 - auc: 0.8700 - val_loss: 0.4704 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8955\n",
      "Epoch 391/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 0.4839 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8710 - val_loss: 0.4729 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8959\n",
      "Epoch 392/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4856 - tp: 199.0000 - fp: 56.0000 - tn: 388.0000 - fn: 69.0000 - accuracy: 0.8244 - precision: 0.7804 - recall: 0.7425 - auc: 0.8730 - val_loss: 0.4677 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8926\n",
      "Epoch 393/500\n",
      "712/712 [==============================] - 0s 195us/sample - loss: 0.4927 - tp: 198.0000 - fp: 55.0000 - tn: 389.0000 - fn: 70.0000 - accuracy: 0.8244 - precision: 0.7826 - recall: 0.7388 - auc: 0.8633 - val_loss: 0.4702 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8982\n",
      "Epoch 394/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4762 - tp: 214.0000 - fp: 74.0000 - tn: 370.0000 - fn: 54.0000 - accuracy: 0.8202 - precision: 0.7431 - recall: 0.7985 - auc: 0.8742 - val_loss: 0.4759 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8939\n",
      "Epoch 395/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.4875 - tp: 199.0000 - fp: 50.0000 - tn: 394.0000 - fn: 69.0000 - accuracy: 0.8329 - precision: 0.7992 - recall: 0.7425 - auc: 0.8703 - val_loss: 0.4751 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8961\n",
      "Epoch 396/500\n",
      "712/712 [==============================] - 0s 179us/sample - loss: 0.4875 - tp: 203.0000 - fp: 50.0000 - tn: 394.0000 - fn: 65.0000 - accuracy: 0.8385 - precision: 0.8024 - recall: 0.7575 - auc: 0.8679 - val_loss: 0.4696 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8942\n",
      "Epoch 397/500\n",
      "712/712 [==============================] - 0s 186us/sample - loss: 0.4805 - tp: 202.0000 - fp: 63.0000 - tn: 381.0000 - fn: 66.0000 - accuracy: 0.8188 - precision: 0.7623 - recall: 0.7537 - auc: 0.8669 - val_loss: 0.4688 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8956\n",
      "Epoch 398/500\n",
      "712/712 [==============================] - 0s 195us/sample - loss: 0.4812 - tp: 200.0000 - fp: 57.0000 - tn: 387.0000 - fn: 68.0000 - accuracy: 0.8244 - precision: 0.7782 - recall: 0.7463 - auc: 0.8733 - val_loss: 0.4643 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8982\n",
      "Epoch 399/500\n",
      "712/712 [==============================] - 0s 192us/sample - loss: 0.4810 - tp: 194.0000 - fp: 48.0000 - tn: 396.0000 - fn: 74.0000 - accuracy: 0.8287 - precision: 0.8017 - recall: 0.7239 - auc: 0.8657 - val_loss: 0.4755 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8956\n",
      "Epoch 400/500\n",
      "712/712 [==============================] - 0s 190us/sample - loss: 0.4935 - tp: 203.0000 - fp: 64.0000 - tn: 380.0000 - fn: 65.0000 - accuracy: 0.8188 - precision: 0.7603 - recall: 0.7575 - auc: 0.8642 - val_loss: 0.4749 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8947\n",
      "Epoch 401/500\n",
      "712/712 [==============================] - 0s 192us/sample - loss: 0.4858 - tp: 193.0000 - fp: 50.0000 - tn: 394.0000 - fn: 75.0000 - accuracy: 0.8244 - precision: 0.7942 - recall: 0.7201 - auc: 0.8690 - val_loss: 0.4703 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8996\n",
      "Epoch 402/500\n",
      "712/712 [==============================] - 0s 186us/sample - loss: 0.4665 - tp: 206.0000 - fp: 51.0000 - tn: 393.0000 - fn: 62.0000 - accuracy: 0.8413 - precision: 0.8016 - recall: 0.7687 - auc: 0.8827 - val_loss: 0.4725 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8932\n",
      "Epoch 403/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.4918 - tp: 199.0000 - fp: 42.0000 - tn: 402.0000 - fn: 69.0000 - accuracy: 0.8441 - precision: 0.8257 - recall: 0.7425 - auc: 0.8613 - val_loss: 0.4864 - val_tp: 64.0000 - val_fp: 26.0000 - val_tn: 79.0000 - val_fn: 10.0000 - val_accuracy: 0.7989 - val_precision: 0.7111 - val_recall: 0.8649 - val_auc: 0.8954\n",
      "Epoch 404/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4947 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8632 - val_loss: 0.4661 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8957\n",
      "Epoch 405/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.4918 - tp: 200.0000 - fp: 49.0000 - tn: 395.0000 - fn: 68.0000 - accuracy: 0.8357 - precision: 0.8032 - recall: 0.7463 - auc: 0.8574 - val_loss: 0.4745 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8977\n",
      "Epoch 406/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.4785 - tp: 212.0000 - fp: 51.0000 - tn: 393.0000 - fn: 56.0000 - accuracy: 0.8497 - precision: 0.8061 - recall: 0.7910 - auc: 0.8707 - val_loss: 0.4876 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 20.0000 - val_accuracy: 0.8212 - val_precision: 0.8182 - val_recall: 0.7297 - val_auc: 0.8909\n",
      "Epoch 407/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4852 - tp: 198.0000 - fp: 46.0000 - tn: 398.0000 - fn: 70.0000 - accuracy: 0.8371 - precision: 0.8115 - recall: 0.7388 - auc: 0.8692 - val_loss: 0.4749 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8940\n",
      "Epoch 408/500\n",
      "712/712 [==============================] - 0s 202us/sample - loss: 0.4716 - tp: 200.0000 - fp: 54.0000 - tn: 390.0000 - fn: 68.0000 - accuracy: 0.8287 - precision: 0.7874 - recall: 0.7463 - auc: 0.8818 - val_loss: 0.4681 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8993\n",
      "Epoch 409/500\n",
      "712/712 [==============================] - 0s 212us/sample - loss: 0.4800 - tp: 206.0000 - fp: 52.0000 - tn: 392.0000 - fn: 62.0000 - accuracy: 0.8399 - precision: 0.7984 - recall: 0.7687 - auc: 0.8735 - val_loss: 0.4594 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8961\n",
      "Epoch 410/500\n",
      "712/712 [==============================] - 0s 207us/sample - loss: 0.4847 - tp: 202.0000 - fp: 53.0000 - tn: 391.0000 - fn: 66.0000 - accuracy: 0.8329 - precision: 0.7922 - recall: 0.7537 - auc: 0.8707 - val_loss: 0.4709 - val_tp: 65.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 9.0000 - val_accuracy: 0.8101 - val_precision: 0.7222 - val_recall: 0.8784 - val_auc: 0.8976\n",
      "Epoch 411/500\n",
      "712/712 [==============================] - 0s 242us/sample - loss: 0.4888 - tp: 211.0000 - fp: 69.0000 - tn: 375.0000 - fn: 57.0000 - accuracy: 0.8230 - precision: 0.7536 - recall: 0.7873 - auc: 0.8709 - val_loss: 0.4655 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8961\n",
      "Epoch 412/500\n",
      "712/712 [==============================] - 0s 203us/sample - loss: 0.4951 - tp: 205.0000 - fp: 67.0000 - tn: 377.0000 - fn: 63.0000 - accuracy: 0.8174 - precision: 0.7537 - recall: 0.7649 - auc: 0.8600 - val_loss: 0.4748 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8945\n",
      "Epoch 413/500\n",
      "712/712 [==============================] - 0s 201us/sample - loss: 0.4937 - tp: 194.0000 - fp: 46.0000 - tn: 398.0000 - fn: 74.0000 - accuracy: 0.8315 - precision: 0.8083 - recall: 0.7239 - auc: 0.8687 - val_loss: 0.4735 - val_tp: 60.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 14.0000 - val_accuracy: 0.8268 - val_precision: 0.7792 - val_recall: 0.8108 - val_auc: 0.8986\n",
      "Epoch 414/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.4754 - tp: 197.0000 - fp: 45.0000 - tn: 399.0000 - fn: 71.0000 - accuracy: 0.8371 - precision: 0.8140 - recall: 0.7351 - auc: 0.8765 - val_loss: 0.4778 - val_tp: 60.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 14.0000 - val_accuracy: 0.8156 - val_precision: 0.7595 - val_recall: 0.8108 - val_auc: 0.8956\n",
      "Epoch 415/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.4898 - tp: 201.0000 - fp: 62.0000 - tn: 382.0000 - fn: 67.0000 - accuracy: 0.8188 - precision: 0.7643 - recall: 0.7500 - auc: 0.8630 - val_loss: 0.4683 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8977\n",
      "Epoch 416/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4781 - tp: 198.0000 - fp: 55.0000 - tn: 389.0000 - fn: 70.0000 - accuracy: 0.8244 - precision: 0.7826 - recall: 0.7388 - auc: 0.8746 - val_loss: 0.4778 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8918\n",
      "Epoch 417/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4900 - tp: 201.0000 - fp: 47.0000 - tn: 397.0000 - fn: 67.0000 - accuracy: 0.8399 - precision: 0.8105 - recall: 0.7500 - auc: 0.8698 - val_loss: 0.4821 - val_tp: 64.0000 - val_fp: 24.0000 - val_tn: 81.0000 - val_fn: 10.0000 - val_accuracy: 0.8101 - val_precision: 0.7273 - val_recall: 0.8649 - val_auc: 0.8941\n",
      "Epoch 418/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4875 - tp: 194.0000 - fp: 49.0000 - tn: 395.0000 - fn: 74.0000 - accuracy: 0.8272 - precision: 0.7984 - recall: 0.7239 - auc: 0.8710 - val_loss: 0.4705 - val_tp: 64.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 10.0000 - val_accuracy: 0.8045 - val_precision: 0.7191 - val_recall: 0.8649 - val_auc: 0.8988\n",
      "Epoch 419/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4984 - tp: 197.0000 - fp: 50.0000 - tn: 394.0000 - fn: 71.0000 - accuracy: 0.8301 - precision: 0.7976 - recall: 0.7351 - auc: 0.8608 - val_loss: 0.4690 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8974\n",
      "Epoch 420/500\n",
      "712/712 [==============================] - 0s 186us/sample - loss: 0.4741 - tp: 204.0000 - fp: 56.0000 - tn: 388.0000 - fn: 64.0000 - accuracy: 0.8315 - precision: 0.7846 - recall: 0.7612 - auc: 0.8744 - val_loss: 0.4835 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.8947\n",
      "Epoch 421/500\n",
      "712/712 [==============================] - 0s 191us/sample - loss: 0.4901 - tp: 195.0000 - fp: 51.0000 - tn: 393.0000 - fn: 73.0000 - accuracy: 0.8258 - precision: 0.7927 - recall: 0.7276 - auc: 0.8644 - val_loss: 0.4716 - val_tp: 61.0000 - val_fp: 22.0000 - val_tn: 83.0000 - val_fn: 13.0000 - val_accuracy: 0.8045 - val_precision: 0.7349 - val_recall: 0.8243 - val_auc: 0.8981\n",
      "Epoch 422/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.4961 - tp: 197.0000 - fp: 55.0000 - tn: 389.0000 - fn: 71.0000 - accuracy: 0.8230 - precision: 0.7817 - recall: 0.7351 - auc: 0.8606 - val_loss: 0.4756 - val_tp: 61.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 13.0000 - val_accuracy: 0.7989 - val_precision: 0.7262 - val_recall: 0.8243 - val_auc: 0.8967\n",
      "Epoch 423/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.4807 - tp: 202.0000 - fp: 47.0000 - tn: 397.0000 - fn: 66.0000 - accuracy: 0.8413 - precision: 0.8112 - recall: 0.7537 - auc: 0.8723 - val_loss: 0.4827 - val_tp: 63.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 11.0000 - val_accuracy: 0.7989 - val_precision: 0.7159 - val_recall: 0.8514 - val_auc: 0.8931\n",
      "Epoch 424/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.4851 - tp: 210.0000 - fp: 56.0000 - tn: 388.0000 - fn: 58.0000 - accuracy: 0.8399 - precision: 0.7895 - recall: 0.7836 - auc: 0.8688 - val_loss: 0.4658 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8932\n",
      "Epoch 425/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4777 - tp: 207.0000 - fp: 59.0000 - tn: 385.0000 - fn: 61.0000 - accuracy: 0.8315 - precision: 0.7782 - recall: 0.7724 - auc: 0.8773 - val_loss: 0.4617 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8959\n",
      "Epoch 426/500\n",
      "712/712 [==============================] - 0s 205us/sample - loss: 0.4748 - tp: 203.0000 - fp: 44.0000 - tn: 400.0000 - fn: 65.0000 - accuracy: 0.8469 - precision: 0.8219 - recall: 0.7575 - auc: 0.8703 - val_loss: 0.4804 - val_tp: 68.0000 - val_fp: 28.0000 - val_tn: 77.0000 - val_fn: 6.0000 - val_accuracy: 0.8101 - val_precision: 0.7083 - val_recall: 0.9189 - val_auc: 0.8969\n",
      "Epoch 427/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4916 - tp: 205.0000 - fp: 75.0000 - tn: 369.0000 - fn: 63.0000 - accuracy: 0.8062 - precision: 0.7321 - recall: 0.7649 - auc: 0.8669 - val_loss: 0.4650 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8988\n",
      "Epoch 428/500\n",
      "712/712 [==============================] - 0s 230us/sample - loss: 0.4746 - tp: 204.0000 - fp: 55.0000 - tn: 389.0000 - fn: 64.0000 - accuracy: 0.8329 - precision: 0.7876 - recall: 0.7612 - auc: 0.8765 - val_loss: 0.4612 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.9015\n",
      "Epoch 429/500\n",
      "712/712 [==============================] - 0s 186us/sample - loss: 0.4938 - tp: 203.0000 - fp: 62.0000 - tn: 382.0000 - fn: 65.0000 - accuracy: 0.8216 - precision: 0.7660 - recall: 0.7575 - auc: 0.8613 - val_loss: 0.4606 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8979\n",
      "Epoch 430/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4740 - tp: 200.0000 - fp: 49.0000 - tn: 395.0000 - fn: 68.0000 - accuracy: 0.8357 - precision: 0.8032 - recall: 0.7463 - auc: 0.8711 - val_loss: 0.4650 - val_tp: 63.0000 - val_fp: 21.0000 - val_tn: 84.0000 - val_fn: 11.0000 - val_accuracy: 0.8212 - val_precision: 0.7500 - val_recall: 0.8514 - val_auc: 0.9005\n",
      "Epoch 431/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4736 - tp: 205.0000 - fp: 56.0000 - tn: 388.0000 - fn: 63.0000 - accuracy: 0.8329 - precision: 0.7854 - recall: 0.7649 - auc: 0.8721 - val_loss: 0.4678 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8976\n",
      "Epoch 432/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.4696 - tp: 201.0000 - fp: 61.0000 - tn: 383.0000 - fn: 67.0000 - accuracy: 0.8202 - precision: 0.7672 - recall: 0.7500 - auc: 0.8777 - val_loss: 0.4630 - val_tp: 62.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 12.0000 - val_accuracy: 0.8380 - val_precision: 0.7848 - val_recall: 0.8378 - val_auc: 0.8974\n",
      "Epoch 433/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4821 - tp: 209.0000 - fp: 69.0000 - tn: 375.0000 - fn: 59.0000 - accuracy: 0.8202 - precision: 0.7518 - recall: 0.7799 - auc: 0.8703 - val_loss: 0.4607 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8950\n",
      "Epoch 434/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4701 - tp: 201.0000 - fp: 46.0000 - tn: 398.0000 - fn: 67.0000 - accuracy: 0.8413 - precision: 0.8138 - recall: 0.7500 - auc: 0.8794 - val_loss: 0.4620 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.9005\n",
      "Epoch 435/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.4739 - tp: 201.0000 - fp: 62.0000 - tn: 382.0000 - fn: 67.0000 - accuracy: 0.8188 - precision: 0.7643 - recall: 0.7500 - auc: 0.8830 - val_loss: 0.4586 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8941\n",
      "Epoch 436/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4934 - tp: 198.0000 - fp: 50.0000 - tn: 394.0000 - fn: 70.0000 - accuracy: 0.8315 - precision: 0.7984 - recall: 0.7388 - auc: 0.8676 - val_loss: 0.4642 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8983\n",
      "Epoch 437/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.4774 - tp: 205.0000 - fp: 56.0000 - tn: 388.0000 - fn: 63.0000 - accuracy: 0.8329 - precision: 0.7854 - recall: 0.7649 - auc: 0.8697 - val_loss: 0.4721 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8992\n",
      "Epoch 438/500\n",
      "712/712 [==============================] - 0s 157us/sample - loss: 0.4879 - tp: 196.0000 - fp: 44.0000 - tn: 400.0000 - fn: 72.0000 - accuracy: 0.8371 - precision: 0.8167 - recall: 0.7313 - auc: 0.8682 - val_loss: 0.4668 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8976\n",
      "Epoch 439/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.4814 - tp: 202.0000 - fp: 56.0000 - tn: 388.0000 - fn: 66.0000 - accuracy: 0.8287 - precision: 0.7829 - recall: 0.7537 - auc: 0.8721 - val_loss: 0.4673 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8965\n",
      "Epoch 440/500\n",
      "712/712 [==============================] - 0s 149us/sample - loss: 0.4658 - tp: 209.0000 - fp: 56.0000 - tn: 388.0000 - fn: 59.0000 - accuracy: 0.8385 - precision: 0.7887 - recall: 0.7799 - auc: 0.8814 - val_loss: 0.4701 - val_tp: 56.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 18.0000 - val_accuracy: 0.8324 - val_precision: 0.8235 - val_recall: 0.7568 - val_auc: 0.8974\n",
      "Epoch 441/500\n",
      "712/712 [==============================] - 0s 154us/sample - loss: 0.4777 - tp: 197.0000 - fp: 44.0000 - tn: 400.0000 - fn: 71.0000 - accuracy: 0.8385 - precision: 0.8174 - recall: 0.7351 - auc: 0.8668 - val_loss: 0.4755 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.9030\n",
      "Epoch 442/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.4752 - tp: 204.0000 - fp: 55.0000 - tn: 389.0000 - fn: 64.0000 - accuracy: 0.8329 - precision: 0.7876 - recall: 0.7612 - auc: 0.8718 - val_loss: 0.4658 - val_tp: 60.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 14.0000 - val_accuracy: 0.8324 - val_precision: 0.7895 - val_recall: 0.8108 - val_auc: 0.8978\n",
      "Epoch 443/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.4899 - tp: 193.0000 - fp: 44.0000 - tn: 400.0000 - fn: 75.0000 - accuracy: 0.8329 - precision: 0.8143 - recall: 0.7201 - auc: 0.8642 - val_loss: 0.4653 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9003\n",
      "Epoch 444/500\n",
      "712/712 [==============================] - 0s 163us/sample - loss: 0.4891 - tp: 201.0000 - fp: 61.0000 - tn: 383.0000 - fn: 67.0000 - accuracy: 0.8202 - precision: 0.7672 - recall: 0.7500 - auc: 0.8634 - val_loss: 0.4628 - val_tp: 57.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 17.0000 - val_accuracy: 0.8324 - val_precision: 0.8143 - val_recall: 0.7703 - val_auc: 0.8983\n",
      "Epoch 445/500\n",
      "712/712 [==============================] - 0s 156us/sample - loss: 0.4824 - tp: 197.0000 - fp: 54.0000 - tn: 390.0000 - fn: 71.0000 - accuracy: 0.8244 - precision: 0.7849 - recall: 0.7351 - auc: 0.8634 - val_loss: 0.4658 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8977\n",
      "Epoch 446/500\n",
      "712/712 [==============================] - 0s 183us/sample - loss: 0.4663 - tp: 204.0000 - fp: 53.0000 - tn: 391.0000 - fn: 64.0000 - accuracy: 0.8357 - precision: 0.7938 - recall: 0.7612 - auc: 0.8821 - val_loss: 0.4660 - val_tp: 58.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 16.0000 - val_accuracy: 0.8380 - val_precision: 0.8169 - val_recall: 0.7838 - val_auc: 0.8983\n",
      "Epoch 447/500\n",
      "712/712 [==============================] - 0s 189us/sample - loss: 0.4819 - tp: 199.0000 - fp: 49.0000 - tn: 395.0000 - fn: 69.0000 - accuracy: 0.8343 - precision: 0.8024 - recall: 0.7425 - auc: 0.8709 - val_loss: 0.4659 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.9037\n",
      "Epoch 448/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4753 - tp: 206.0000 - fp: 50.0000 - tn: 394.0000 - fn: 62.0000 - accuracy: 0.8427 - precision: 0.8047 - recall: 0.7687 - auc: 0.8756 - val_loss: 0.4631 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8978\n",
      "Epoch 449/500\n",
      "712/712 [==============================] - 0s 184us/sample - loss: 0.4895 - tp: 199.0000 - fp: 52.0000 - tn: 392.0000 - fn: 69.0000 - accuracy: 0.8301 - precision: 0.7928 - recall: 0.7425 - auc: 0.8619 - val_loss: 0.4639 - val_tp: 67.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 7.0000 - val_accuracy: 0.8212 - val_precision: 0.7283 - val_recall: 0.9054 - val_auc: 0.9025\n",
      "Epoch 450/500\n",
      "712/712 [==============================] - 0s 180us/sample - loss: 0.4910 - tp: 205.0000 - fp: 59.0000 - tn: 385.0000 - fn: 63.0000 - accuracy: 0.8287 - precision: 0.7765 - recall: 0.7649 - auc: 0.8641 - val_loss: 0.4633 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.9015\n",
      "Epoch 451/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.4727 - tp: 200.0000 - fp: 56.0000 - tn: 388.0000 - fn: 68.0000 - accuracy: 0.8258 - precision: 0.7812 - recall: 0.7463 - auc: 0.8771 - val_loss: 0.4675 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8989\n",
      "Epoch 452/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.4921 - tp: 203.0000 - fp: 59.0000 - tn: 385.0000 - fn: 65.0000 - accuracy: 0.8258 - precision: 0.7748 - recall: 0.7575 - auc: 0.8632 - val_loss: 0.4712 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9014\n",
      "Epoch 453/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4728 - tp: 205.0000 - fp: 44.0000 - tn: 400.0000 - fn: 63.0000 - accuracy: 0.8497 - precision: 0.8233 - recall: 0.7649 - auc: 0.8724 - val_loss: 0.4647 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.9001\n",
      "Epoch 454/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4815 - tp: 198.0000 - fp: 47.0000 - tn: 397.0000 - fn: 70.0000 - accuracy: 0.8357 - precision: 0.8082 - recall: 0.7388 - auc: 0.8677 - val_loss: 0.4625 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9040\n",
      "Epoch 455/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4734 - tp: 207.0000 - fp: 58.0000 - tn: 386.0000 - fn: 61.0000 - accuracy: 0.8329 - precision: 0.7811 - recall: 0.7724 - auc: 0.8776 - val_loss: 0.4658 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.9015\n",
      "Epoch 456/500\n",
      "712/712 [==============================] - 0s 170us/sample - loss: 0.4955 - tp: 198.0000 - fp: 58.0000 - tn: 386.0000 - fn: 70.0000 - accuracy: 0.8202 - precision: 0.7734 - recall: 0.7388 - auc: 0.8649 - val_loss: 0.4613 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.9031\n",
      "Epoch 457/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.4689 - tp: 201.0000 - fp: 48.0000 - tn: 396.0000 - fn: 67.0000 - accuracy: 0.8385 - precision: 0.8072 - recall: 0.7500 - auc: 0.8826 - val_loss: 0.4687 - val_tp: 61.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 13.0000 - val_accuracy: 0.8212 - val_precision: 0.7625 - val_recall: 0.8243 - val_auc: 0.8995\n",
      "Epoch 458/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4797 - tp: 196.0000 - fp: 53.0000 - tn: 391.0000 - fn: 72.0000 - accuracy: 0.8244 - precision: 0.7871 - recall: 0.7313 - auc: 0.8673 - val_loss: 0.4683 - val_tp: 67.0000 - val_fp: 25.0000 - val_tn: 80.0000 - val_fn: 7.0000 - val_accuracy: 0.8212 - val_precision: 0.7283 - val_recall: 0.9054 - val_auc: 0.8985\n",
      "Epoch 459/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4852 - tp: 202.0000 - fp: 61.0000 - tn: 383.0000 - fn: 66.0000 - accuracy: 0.8216 - precision: 0.7681 - recall: 0.7537 - auc: 0.8694 - val_loss: 0.4664 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9012\n",
      "Epoch 460/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4793 - tp: 203.0000 - fp: 51.0000 - tn: 393.0000 - fn: 65.0000 - accuracy: 0.8371 - precision: 0.7992 - recall: 0.7575 - auc: 0.8721 - val_loss: 0.4676 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.9014\n",
      "Epoch 461/500\n",
      "712/712 [==============================] - 0s 157us/sample - loss: 0.4779 - tp: 205.0000 - fp: 61.0000 - tn: 383.0000 - fn: 63.0000 - accuracy: 0.8258 - precision: 0.7707 - recall: 0.7649 - auc: 0.8692 - val_loss: 0.4705 - val_tp: 58.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 16.0000 - val_accuracy: 0.8324 - val_precision: 0.8056 - val_recall: 0.7838 - val_auc: 0.8946\n",
      "Epoch 462/500\n",
      "712/712 [==============================] - 0s 176us/sample - loss: 0.4739 - tp: 200.0000 - fp: 55.0000 - tn: 389.0000 - fn: 68.0000 - accuracy: 0.8272 - precision: 0.7843 - recall: 0.7463 - auc: 0.8772 - val_loss: 0.4665 - val_tp: 58.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 16.0000 - val_accuracy: 0.8268 - val_precision: 0.7945 - val_recall: 0.7838 - val_auc: 0.8959\n",
      "Epoch 463/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.4896 - tp: 196.0000 - fp: 62.0000 - tn: 382.0000 - fn: 72.0000 - accuracy: 0.8118 - precision: 0.7597 - recall: 0.7313 - auc: 0.8725 - val_loss: 0.4665 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8998\n",
      "Epoch 464/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.4843 - tp: 206.0000 - fp: 65.0000 - tn: 379.0000 - fn: 62.0000 - accuracy: 0.8216 - precision: 0.7601 - recall: 0.7687 - auc: 0.8657 - val_loss: 0.4655 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8994\n",
      "Epoch 465/500\n",
      "712/712 [==============================] - 0s 200us/sample - loss: 0.4733 - tp: 207.0000 - fp: 46.0000 - tn: 398.0000 - fn: 61.0000 - accuracy: 0.8497 - precision: 0.8182 - recall: 0.7724 - auc: 0.8724 - val_loss: 0.4659 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8938\n",
      "Epoch 466/500\n",
      "712/712 [==============================] - 0s 185us/sample - loss: 0.4731 - tp: 204.0000 - fp: 57.0000 - tn: 387.0000 - fn: 64.0000 - accuracy: 0.8301 - precision: 0.7816 - recall: 0.7612 - auc: 0.8722 - val_loss: 0.4719 - val_tp: 64.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 10.0000 - val_accuracy: 0.8324 - val_precision: 0.7619 - val_recall: 0.8649 - val_auc: 0.9017\n",
      "Epoch 467/500\n",
      "712/712 [==============================] - 0s 175us/sample - loss: 0.4868 - tp: 208.0000 - fp: 61.0000 - tn: 383.0000 - fn: 60.0000 - accuracy: 0.8301 - precision: 0.7732 - recall: 0.7761 - auc: 0.8717 - val_loss: 0.4632 - val_tp: 63.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 11.0000 - val_accuracy: 0.8268 - val_precision: 0.7590 - val_recall: 0.8514 - val_auc: 0.8969\n",
      "Epoch 468/500\n",
      "712/712 [==============================] - 0s 172us/sample - loss: 0.4614 - tp: 208.0000 - fp: 51.0000 - tn: 393.0000 - fn: 60.0000 - accuracy: 0.8441 - precision: 0.8031 - recall: 0.7761 - auc: 0.8826 - val_loss: 0.4657 - val_tp: 59.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 15.0000 - val_accuracy: 0.8101 - val_precision: 0.7564 - val_recall: 0.7973 - val_auc: 0.9001\n",
      "Epoch 469/500\n",
      "712/712 [==============================] - 0s 166us/sample - loss: 0.4878 - tp: 205.0000 - fp: 67.0000 - tn: 377.0000 - fn: 63.0000 - accuracy: 0.8174 - precision: 0.7537 - recall: 0.7649 - auc: 0.8724 - val_loss: 0.4663 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8969\n",
      "Epoch 470/500\n",
      "712/712 [==============================] - 0s 178us/sample - loss: 0.4698 - tp: 208.0000 - fp: 60.0000 - tn: 384.0000 - fn: 60.0000 - accuracy: 0.8315 - precision: 0.7761 - recall: 0.7761 - auc: 0.8808 - val_loss: 0.4637 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8961\n",
      "Epoch 471/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.4763 - tp: 198.0000 - fp: 42.0000 - tn: 402.0000 - fn: 70.0000 - accuracy: 0.8427 - precision: 0.8250 - recall: 0.7388 - auc: 0.8745 - val_loss: 0.4677 - val_tp: 59.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 15.0000 - val_accuracy: 0.8101 - val_precision: 0.7564 - val_recall: 0.7973 - val_auc: 0.8983\n",
      "Epoch 472/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4851 - tp: 205.0000 - fp: 53.0000 - tn: 391.0000 - fn: 63.0000 - accuracy: 0.8371 - precision: 0.7946 - recall: 0.7649 - auc: 0.8696 - val_loss: 0.4697 - val_tp: 60.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 14.0000 - val_accuracy: 0.8212 - val_precision: 0.7692 - val_recall: 0.8108 - val_auc: 0.8967\n",
      "Epoch 473/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.4887 - tp: 205.0000 - fp: 58.0000 - tn: 386.0000 - fn: 63.0000 - accuracy: 0.8301 - precision: 0.7795 - recall: 0.7649 - auc: 0.8663 - val_loss: 0.4716 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8971\n",
      "Epoch 474/500\n",
      "712/712 [==============================] - 0s 156us/sample - loss: 0.4699 - tp: 195.0000 - fp: 46.0000 - tn: 398.0000 - fn: 73.0000 - accuracy: 0.8329 - precision: 0.8091 - recall: 0.7276 - auc: 0.8823 - val_loss: 0.4690 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.8997\n",
      "Epoch 475/500\n",
      "712/712 [==============================] - 0s 174us/sample - loss: 0.4756 - tp: 201.0000 - fp: 52.0000 - tn: 392.0000 - fn: 67.0000 - accuracy: 0.8329 - precision: 0.7945 - recall: 0.7500 - auc: 0.8743 - val_loss: 0.4699 - val_tp: 59.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 15.0000 - val_accuracy: 0.8324 - val_precision: 0.7973 - val_recall: 0.7973 - val_auc: 0.8943\n",
      "Epoch 476/500\n",
      "712/712 [==============================] - 0s 157us/sample - loss: 0.4777 - tp: 197.0000 - fp: 49.0000 - tn: 395.0000 - fn: 71.0000 - accuracy: 0.8315 - precision: 0.8008 - recall: 0.7351 - auc: 0.8739 - val_loss: 0.4690 - val_tp: 61.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 13.0000 - val_accuracy: 0.8268 - val_precision: 0.7722 - val_recall: 0.8243 - val_auc: 0.8987\n",
      "Epoch 477/500\n",
      "712/712 [==============================] - 0s 171us/sample - loss: 0.4761 - tp: 207.0000 - fp: 60.0000 - tn: 384.0000 - fn: 61.0000 - accuracy: 0.8301 - precision: 0.7753 - recall: 0.7724 - auc: 0.8761 - val_loss: 0.4763 - val_tp: 55.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 19.0000 - val_accuracy: 0.8268 - val_precision: 0.8209 - val_recall: 0.7432 - val_auc: 0.9060\n",
      "Epoch 478/500\n",
      "712/712 [==============================] - 0s 155us/sample - loss: 0.4707 - tp: 196.0000 - fp: 43.0000 - tn: 401.0000 - fn: 72.0000 - accuracy: 0.8385 - precision: 0.8201 - recall: 0.7313 - auc: 0.8748 - val_loss: 0.4682 - val_tp: 62.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 12.0000 - val_accuracy: 0.8268 - val_precision: 0.7654 - val_recall: 0.8378 - val_auc: 0.8979\n",
      "Epoch 479/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4740 - tp: 202.0000 - fp: 45.0000 - tn: 399.0000 - fn: 66.0000 - accuracy: 0.8441 - precision: 0.8178 - recall: 0.7537 - auc: 0.8722 - val_loss: 0.4644 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.9084\n",
      "Epoch 480/500\n",
      "712/712 [==============================] - 0s 156us/sample - loss: 0.4858 - tp: 207.0000 - fp: 56.0000 - tn: 388.0000 - fn: 61.0000 - accuracy: 0.8357 - precision: 0.7871 - recall: 0.7724 - auc: 0.8662 - val_loss: 0.4613 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.9028\n",
      "Epoch 481/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.4817 - tp: 198.0000 - fp: 55.0000 - tn: 389.0000 - fn: 70.0000 - accuracy: 0.8244 - precision: 0.7826 - recall: 0.7388 - auc: 0.8693 - val_loss: 0.4638 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8996\n",
      "Epoch 482/500\n",
      "712/712 [==============================] - 0s 159us/sample - loss: 0.4746 - tp: 199.0000 - fp: 37.0000 - tn: 407.0000 - fn: 69.0000 - accuracy: 0.8511 - precision: 0.8432 - recall: 0.7425 - auc: 0.8669 - val_loss: 0.4674 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.8990\n",
      "Epoch 483/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4757 - tp: 193.0000 - fp: 42.0000 - tn: 402.0000 - fn: 75.0000 - accuracy: 0.8357 - precision: 0.8213 - recall: 0.7201 - auc: 0.8721 - val_loss: 0.4645 - val_tp: 62.0000 - val_fp: 19.0000 - val_tn: 86.0000 - val_fn: 12.0000 - val_accuracy: 0.8268 - val_precision: 0.7654 - val_recall: 0.8378 - val_auc: 0.8981\n",
      "Epoch 484/500\n",
      "712/712 [==============================] - 0s 162us/sample - loss: 0.4860 - tp: 210.0000 - fp: 69.0000 - tn: 375.0000 - fn: 58.0000 - accuracy: 0.8216 - precision: 0.7527 - recall: 0.7836 - auc: 0.8710 - val_loss: 0.4772 - val_tp: 55.0000 - val_fp: 10.0000 - val_tn: 95.0000 - val_fn: 19.0000 - val_accuracy: 0.8380 - val_precision: 0.8462 - val_recall: 0.7432 - val_auc: 0.8994\n",
      "Epoch 485/500\n",
      "712/712 [==============================] - 0s 156us/sample - loss: 0.4912 - tp: 201.0000 - fp: 55.0000 - tn: 389.0000 - fn: 67.0000 - accuracy: 0.8287 - precision: 0.7852 - recall: 0.7500 - auc: 0.8610 - val_loss: 0.4703 - val_tp: 67.0000 - val_fp: 23.0000 - val_tn: 82.0000 - val_fn: 7.0000 - val_accuracy: 0.8324 - val_precision: 0.7444 - val_recall: 0.9054 - val_auc: 0.9017\n",
      "Epoch 486/500\n",
      "712/712 [==============================] - 0s 167us/sample - loss: 0.4760 - tp: 197.0000 - fp: 41.0000 - tn: 403.0000 - fn: 71.0000 - accuracy: 0.8427 - precision: 0.8277 - recall: 0.7351 - auc: 0.8692 - val_loss: 0.4656 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.9029\n",
      "Epoch 487/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4693 - tp: 203.0000 - fp: 52.0000 - tn: 392.0000 - fn: 65.0000 - accuracy: 0.8357 - precision: 0.7961 - recall: 0.7575 - auc: 0.8848 - val_loss: 0.4694 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.8981\n",
      "Epoch 488/500\n",
      "712/712 [==============================] - 0s 173us/sample - loss: 0.4843 - tp: 206.0000 - fp: 62.0000 - tn: 382.0000 - fn: 62.0000 - accuracy: 0.8258 - precision: 0.7687 - recall: 0.7687 - auc: 0.8632 - val_loss: 0.4643 - val_tp: 57.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 17.0000 - val_accuracy: 0.8156 - val_precision: 0.7808 - val_recall: 0.7703 - val_auc: 0.9006\n",
      "Epoch 489/500\n",
      "712/712 [==============================] - 0s 177us/sample - loss: 0.4698 - tp: 207.0000 - fp: 58.0000 - tn: 386.0000 - fn: 61.0000 - accuracy: 0.8329 - precision: 0.7811 - recall: 0.7724 - auc: 0.8778 - val_loss: 0.4658 - val_tp: 57.0000 - val_fp: 12.0000 - val_tn: 93.0000 - val_fn: 17.0000 - val_accuracy: 0.8380 - val_precision: 0.8261 - val_recall: 0.7703 - val_auc: 0.8956\n",
      "Epoch 490/500\n",
      "712/712 [==============================] - 0s 168us/sample - loss: 0.4804 - tp: 194.0000 - fp: 45.0000 - tn: 399.0000 - fn: 74.0000 - accuracy: 0.8329 - precision: 0.8117 - recall: 0.7239 - auc: 0.8714 - val_loss: 0.4722 - val_tp: 59.0000 - val_fp: 18.0000 - val_tn: 87.0000 - val_fn: 15.0000 - val_accuracy: 0.8156 - val_precision: 0.7662 - val_recall: 0.7973 - val_auc: 0.9021\n",
      "Epoch 491/500\n",
      "712/712 [==============================] - 0s 161us/sample - loss: 0.4698 - tp: 199.0000 - fp: 39.0000 - tn: 405.0000 - fn: 69.0000 - accuracy: 0.8483 - precision: 0.8361 - recall: 0.7425 - auc: 0.8779 - val_loss: 0.4672 - val_tp: 58.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 16.0000 - val_accuracy: 0.8156 - val_precision: 0.7733 - val_recall: 0.7838 - val_auc: 0.8977\n",
      "Epoch 492/500\n",
      "712/712 [==============================] - 0s 160us/sample - loss: 0.4734 - tp: 209.0000 - fp: 59.0000 - tn: 385.0000 - fn: 59.0000 - accuracy: 0.8343 - precision: 0.7799 - recall: 0.7799 - auc: 0.8724 - val_loss: 0.4686 - val_tp: 56.0000 - val_fp: 13.0000 - val_tn: 92.0000 - val_fn: 18.0000 - val_accuracy: 0.8268 - val_precision: 0.8116 - val_recall: 0.7568 - val_auc: 0.8979\n",
      "Epoch 493/500\n",
      "712/712 [==============================] - 0s 165us/sample - loss: 0.4747 - tp: 209.0000 - fp: 61.0000 - tn: 383.0000 - fn: 59.0000 - accuracy: 0.8315 - precision: 0.7741 - recall: 0.7799 - auc: 0.8696 - val_loss: 0.4633 - val_tp: 59.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 15.0000 - val_accuracy: 0.8268 - val_precision: 0.7867 - val_recall: 0.7973 - val_auc: 0.8962\n",
      "Epoch 494/500\n",
      "712/712 [==============================] - 0s 164us/sample - loss: 0.4748 - tp: 203.0000 - fp: 51.0000 - tn: 393.0000 - fn: 65.0000 - accuracy: 0.8371 - precision: 0.7992 - recall: 0.7575 - auc: 0.8759 - val_loss: 0.4632 - val_tp: 57.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 17.0000 - val_accuracy: 0.8268 - val_precision: 0.8028 - val_recall: 0.7703 - val_auc: 0.8975\n",
      "Epoch 495/500\n",
      "712/712 [==============================] - 0s 158us/sample - loss: 0.4699 - tp: 195.0000 - fp: 38.0000 - tn: 406.0000 - fn: 73.0000 - accuracy: 0.8441 - precision: 0.8369 - recall: 0.7276 - auc: 0.8792 - val_loss: 0.4734 - val_tp: 64.0000 - val_fp: 20.0000 - val_tn: 85.0000 - val_fn: 10.0000 - val_accuracy: 0.8324 - val_precision: 0.7619 - val_recall: 0.8649 - val_auc: 0.9002\n",
      "Epoch 496/500\n",
      "712/712 [==============================] - 0s 152us/sample - loss: 0.4753 - tp: 208.0000 - fp: 49.0000 - tn: 395.0000 - fn: 60.0000 - accuracy: 0.8469 - precision: 0.8093 - recall: 0.7761 - auc: 0.8725 - val_loss: 0.4622 - val_tp: 58.0000 - val_fp: 16.0000 - val_tn: 89.0000 - val_fn: 16.0000 - val_accuracy: 0.8212 - val_precision: 0.7838 - val_recall: 0.7838 - val_auc: 0.8961\n",
      "Epoch 497/500\n",
      "712/712 [==============================] - 0s 153us/sample - loss: 0.4675 - tp: 201.0000 - fp: 53.0000 - tn: 391.0000 - fn: 67.0000 - accuracy: 0.8315 - precision: 0.7913 - recall: 0.7500 - auc: 0.8805 - val_loss: 0.4701 - val_tp: 56.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 18.0000 - val_accuracy: 0.8212 - val_precision: 0.8000 - val_recall: 0.7568 - val_auc: 0.9035\n",
      "Epoch 498/500\n",
      "712/712 [==============================] - 0s 169us/sample - loss: 0.4714 - tp: 203.0000 - fp: 49.0000 - tn: 395.0000 - fn: 65.0000 - accuracy: 0.8399 - precision: 0.8056 - recall: 0.7575 - auc: 0.8744 - val_loss: 0.4638 - val_tp: 59.0000 - val_fp: 14.0000 - val_tn: 91.0000 - val_fn: 15.0000 - val_accuracy: 0.8380 - val_precision: 0.8082 - val_recall: 0.7973 - val_auc: 0.8940\n",
      "Epoch 499/500\n",
      "712/712 [==============================] - 0s 182us/sample - loss: 0.4963 - tp: 197.0000 - fp: 62.0000 - tn: 382.0000 - fn: 71.0000 - accuracy: 0.8132 - precision: 0.7606 - recall: 0.7351 - auc: 0.8583 - val_loss: 0.4662 - val_tp: 59.0000 - val_fp: 17.0000 - val_tn: 88.0000 - val_fn: 15.0000 - val_accuracy: 0.8212 - val_precision: 0.7763 - val_recall: 0.7973 - val_auc: 0.9010\n",
      "Epoch 500/500\n",
      "712/712 [==============================] - 0s 200us/sample - loss: 0.4854 - tp: 201.0000 - fp: 57.0000 - tn: 387.0000 - fn: 67.0000 - accuracy: 0.8258 - precision: 0.7791 - recall: 0.7500 - auc: 0.8650 - val_loss: 0.4693 - val_tp: 57.0000 - val_fp: 15.0000 - val_tn: 90.0000 - val_fn: 17.0000 - val_accuracy: 0.8212 - val_precision: 0.7917 - val_recall: 0.7703 - val_auc: 0.8947\n"
     ]
    }
   ],
   "source": [
    "# Create a new model each time before running training (otherwise new trainings would just be on already trained model)\n",
    "model = get_model(X_train.shape[1])\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=500, batch_size=32, validation_data=(X_dev, Y_dev), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Results of the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', 'recall', 'auc', 'val_loss', 'val_tp', 'val_fp', 'val_tn', 'val_fn', 'val_accuracy', 'val_precision', 'val_recall', 'val_auc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train evaluation:\n",
      "712/712 - 1s - loss: 0.4405 - tp: 200.0000 - fp: 39.0000 - tn: 405.0000 - fn: 68.0000 - accuracy: 0.8497 - precision: 0.8368 - recall: 0.7463 - auc: 0.8798\n"
     ]
    }
   ],
   "source": [
    "print(\"Train evaluation:\")\n",
    "_ = model.evaluate(X_train, Y_train, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev evaluation:\n",
      "179/179 - 0s - loss: 0.4769 - tp: 57.0000 - fp: 15.0000 - tn: 90.0000 - fn: 17.0000 - accuracy: 0.8212 - precision: 0.7917 - recall: 0.7703 - auc: 0.8947\n"
     ]
    }
   ],
   "source": [
    "print(\"Dev evaluation:\")\n",
    "_ = model.evaluate(X_dev, Y_dev, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tune hyperparameters for the DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "\n",
    "# https://www.sicara.ai/blog/hyperparameter-tuning-keras-tuner\n",
    "# https://www.curiousily.com/posts/hackers-guide-to-hyperparameter-tuning/\n",
    "\n",
    "class TitanicHyperModel(HyperModel):\n",
    "    def __init__(self, input_size):\n",
    "        self.input_shape = (input_size, )\n",
    "\n",
    "    def build(self, hp):\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Dense, Dropout\n",
    "#         from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        for layer_n in range(hp.Int(\"num_layers\", min_value=1, max_value=6, step=1, default=6) + 1):\n",
    "            units = hp.Int(\n",
    "                f\"dense_units_{layer_n}\",\n",
    "                min_value=8,\n",
    "                max_value=64,\n",
    "                step=8,\n",
    "                default=64\n",
    "            )\n",
    "            activation = hp.Choice(\n",
    "                f\"dense_activation_{layer_n}\",\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu'\n",
    "            )\n",
    "#             regularizer_l1 = hp.Float(\n",
    "#                 f'l1_{layer_n}',\n",
    "#                 min_value=1e-5,\n",
    "#                 max_value=0.1,\n",
    "#                 sampling='LOG',\n",
    "#                 default=0\n",
    "#             )\n",
    "#             regularizer_l2 = hp.Float(\n",
    "#                 f'l2_{layer_n}',\n",
    "#                 min_value=1e-5,\n",
    "#                 max_value=0.1,\n",
    "#                 sampling='LOG',\n",
    "#                 default=0.01\n",
    "#             )\n",
    "            \n",
    "            input_shape_params = {\"input_shape\": self.input_shape} if layer_n == 0 else {}\n",
    "            model.add(Dense(units=units, **input_shape_params, activation=activation)) #, kernel_regularizer=L1L2(l1=regularizer_l1, l2=regularizer_l2)))\n",
    "            \n",
    "            droupout_rate = hp.Float(\n",
    "                f'dropout_{layer_n}',\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                default=0.25,\n",
    "                step=0.05,\n",
    "            )\n",
    "            \n",
    "            model.add(Dropout(rate=droupout_rate))\n",
    "        \n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling='LOG',\n",
    "                    default=1e-3\n",
    "                )\n",
    "            ),\n",
    "            metrics=[\"accuracy\"],\n",
    "            loss=\"binary_crossentropy\",\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "hypermodel = TitanicHyperModel(input_size=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev mode\n",
    "\n",
    "if MODE == \"DEV\":\n",
    "    MAX_TRIALS = 20\n",
    "    EXECUTION_PER_TRIAL = 2\n",
    "    N_EPOCH_SEARCH = 20\n",
    "else:\n",
    "    MAX_TRIALS = 200\n",
    "    EXECUTION_PER_TRIAL = 3\n",
    "    N_EPOCH_SEARCH = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Search space summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Default search space size: 23</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">num_layers (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 6</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_units_0 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_activation_0 (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: relu</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: ['relu', 'tanh', 'sigmoid']</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dropout_0 (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 0.05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_units_1 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_activation_1 (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: relu</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: ['relu', 'tanh', 'sigmoid']</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dropout_1 (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 0.05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_units_2 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_activation_2 (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: relu</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: ['relu', 'tanh', 'sigmoid']</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dropout_2 (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 0.05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_units_3 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_activation_3 (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: relu</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: ['relu', 'tanh', 'sigmoid']</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dropout_3 (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 0.05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_units_4 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_activation_4 (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: relu</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: ['relu', 'tanh', 'sigmoid']</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dropout_4 (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 0.05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_units_5 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_activation_5 (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: relu</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: ['relu', 'tanh', 'sigmoid']</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dropout_5 (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 0.05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_units_6 (Int)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 8</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dense_activation_6 (Choice)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: relu</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-ordered: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-values: ['relu', 'tanh', 'sigmoid']</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">dropout_6 (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: 0.05</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">learning_rate (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: log</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "try:\n",
    "    ATTEMPT = ATTEMPT + 1\n",
    "except NameError:\n",
    "    ATTEMPT = 0\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective=\"val_accuracy\",\n",
    "    seed=SEED,\n",
    "    max_trials=MAX_TRIALS,\n",
    "    executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "    directory=f'random_search_{ATTEMPT}',\n",
    "    project_name='titanic'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Results summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Results in random_search_0/titanic</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Showing 10 best trials</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Objective(name='val_accuracy', direction='max')</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tuner.search(X_train, Y_train, epochs=N_EPOCH_SEARCH, validation_data=(X_dev, Y_dev))\n",
    "\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Results of the tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c4dec79a8530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d598baa86603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get the best model tuned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"Best model\")\n",
    "\n",
    "# Get the best model tuned.\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "_, accuracy = best_model.evaluate(X_train, Y_train)\n",
    "print(f\"Tuned train accuracy: {accuracy}\")\n",
    "\n",
    "_, accuracy = best_model.evaluate(X_dev, Y_dev)\n",
    "print(f\"Tuned dev accuracy: {accuracy}\")\n",
    "\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_predictions(model, submission_name):\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    predictions = np.round(predictions).astype(np.uint8).reshape((-1))\n",
    "\n",
    "    print(f\"{submission_name}:\\n{predictions}\")\n",
    "    \n",
    "    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "    output.to_csv(f\"{submission_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_submission:\n",
      "[0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1\n",
      " 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 1 0 1 1 1 0 1 0 0 1]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-de0d16135529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstore_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dl_submission\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstore_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dl_tuned_submission\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "store_predictions(model, \"dl_submission\")\n",
    "store_predictions(best_model, \"dl_tuned_submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
